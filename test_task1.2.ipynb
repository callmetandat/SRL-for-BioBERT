{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "model = AutoModelForMaskedLM.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "    \n",
    "text = 'the a/t-rich mut sequence indicates that normal splicing was abolished by a g-to-a transition at the first [MASK] of intron 2.'\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "print(\"mask_token_index: \", mask_token_index)\n",
    "\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "print(\"mask_token_logits: \", mask_token_logits)\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 10020, dim=1).indices[0].tolist()\n",
    "\n",
    "token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy==3.5.0\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def assign_labels(sentence, arguments):\n",
    "    # Tokenize the sentence into words\n",
    "    sentence = sentence.lower()\n",
    "    words = re.findall(r'\\w+|[;,.]', sentence)\n",
    "\n",
    "    # Create a list to store labels for each word\n",
    "    labels = ['O'] * len(words)\n",
    "\n",
    "    # Iterate through the arguments and assign labels\n",
    "    for arg_id, arg_text in arguments.items():\n",
    "        # Tokenize the argument into words\n",
    "        arg_text = arg_text.lower()\n",
    "        arg_words = re.findall(r'\\w+|[;,.]', arg_text)\n",
    "\n",
    "        # Iterate through the words in the sentence\n",
    "        for i in range(len(words) - len(arg_words) + 1):\n",
    "            if words[i:i+len(arg_words)] == arg_words:\n",
    "                # Assign a label based on the argument key\n",
    "                for j in range(len(arg_words)):\n",
    "                    if j == 0:\n",
    "                        labels[i+j] = f'B-A{arg_id}'\n",
    "                    else:\n",
    "                        labels[i+j] = f'I-A{arg_id}'\n",
    "\n",
    "    return words, labels\n",
    "\n",
    "# Example usage:\n",
    "sentence = \"A G-to-A transition at the first nucleotide of intron 2 of patient 1 abolished normal splicing.\"\n",
    "arguments = {0: \"a G-to-A transition at the first nucleotide of intron 2\", 1: \"normal splicing\"}\n",
    "\n",
    "words, labels = assign_labels(sentence, arguments)\n",
    "\n",
    "for word, label in zip(words, labels):\n",
    "    print(f\"{word}: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a function to detect base verbs\n",
    "def detect_base_verbs(sentence):\n",
    "    # Process the input sentence with spaCy\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Initialize a list to store base verbs\n",
    "    base_verbs = []\n",
    "\n",
    "    # Iterate through the tokens in the sentence\n",
    "    for i, token in enumerate(doc):\n",
    "        \n",
    "        print(\"token:\", token)\n",
    "        print(\"token.pos:\", token.pos_)\n",
    "        print(\"token.dep:\", token.dep_)\n",
    "        print(\"token.lemma:\", token.lemma_)\n",
    "        print('\\n')\n",
    "        # Check if the token is a verb (POS tag starts with 'V') and not a auxiliary verb (aux)\n",
    "        if (token.pos_.startswith('V')) and token.dep_ != 'aux':\n",
    "            if token.lemma_ == 'truncate':\n",
    "                # If the token is a pronoun, use the text of the token\n",
    "                print(\"DAY NE\", token)\n",
    "            base_verbs.append(token.lemma_)\n",
    "\n",
    "    return base_verbs\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"frameshift sox9 mutations, as our data show, have the probability of actually truncating its two activation domains, while all missense mutations reported to date lie in the high mobility group (hmg) dna-binding domain.\" \n",
    "sen = \"Electrophoretic mobility shift assays showed that the full-length BCL6 protein extracted from transfected COS cells and a bacterially expressed protein that contains the BCL6 zinc fingers and may be remarkably truncated can bind specifically to DNA from the U3 promoter/enhancer region of HIV-1.\"\n",
    "\n",
    "sen2 = \"Specifically, the Stat5a molecule in which the C-terminus can be truncated at amino acids 740 or 751 effectively blocked the induction of both CIS and OSM, whereas the C-terminal truncations at amino acids 762 or 773 had no effect on the induction of either gene.\"\n",
    "\n",
    "sen3 = 'a g-to-a transition at the first nucleotide of intron 2 of patient 1 abolishes normal splicing.'\n",
    "# Detect base verbs in the sentence\n",
    "\n",
    "\n",
    "sen4 = 'C-terminally truncated Stat5a proteins likely to be remarkably truncated at amino acids 762 or 773 had no effect on the induction of either gene.'\n",
    "\n",
    "sen5 = 'signal transduction has been initiated by scf by direct dimerization of its receptor, kit, and the two juxtaposed receptors undergo tyrosine autophosphorylation (heldin, 1995; broudy, 1997), which initiated downstream intracellular signaling.'\n",
    "\n",
    "detect_base_verbs(sen5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['a', 'g-to-a', 'transition', 'at', 'the', 'first', 'nucleotide', 'of', 'intron', '2', 'of', 'patient', '1', '#abolish', 'normal', 'splicing', '.']\n",
    "\n",
    "words = ['frameshift', 'sox9', 'mutations', ',', 'as', 'our', 'data', 'show', ',', 'have', 'the', 'probability', 'of', 'actually', 'truncating', 'its', 'two', 'activation', 'domains', ',', 'while', 'all', 'missense', 'mutations', 'reported', 'to', 'date', 'lie', 'in', 'the', 'high', 'mobility', 'group', '(hmg)', 'dna-binding', 'domain', '.']\n",
    "def analyze_word(word, lowercase=True):\n",
    "    token = nlp(word)\n",
    "   \n",
    "   \n",
    "    lemma = token[0].lemma_\n",
    "    if lowercase: lemma = lemma.lower()\n",
    "    return lemma, token[0].pos_\n",
    "\n",
    "print(analyze_word('abolishes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python xml2conll/xml2conll.py --input='./MLM/data/GramVar/abolish_full.xml' --output='tessst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from xml.dom import minidom\n",
    "\n",
    "PATH_DATA = './MLM/'\n",
    "\n",
    "# create class to preprocess data\n",
    "class Preprocessing:\n",
    "    def __init__(self, file_name):\n",
    "        self.file_name = file_name\n",
    "        self.data_arg = None\n",
    "        self.predicate = None\n",
    "        self.roles = []\n",
    "        self.data_role = None\n",
    "        self.min_threshold = 0.1\n",
    "        self.output_data = PATH_DATA + 'test_xml/' + file_name.split('.')[0] + '.csv'\n",
    "      \n",
    "    def read_xml_file(self):\n",
    "        mydoc = minidom.parse(PATH_DATA + 'data/GramVar/' + 'abolish_full.xml')\n",
    "        self.predicate = mydoc.getElementsByTagName('predicate')[0].getAttribute('lemma')\n",
    "        self.roles = dict()\n",
    "        for arg in mydoc.getElementsByTagName('role'):\n",
    "            self.roles.update({arg.getAttribute('n'): arg.getAttribute('descr')})\n",
    "       \n",
    "        examples = mydoc.getElementsByTagName('example')\n",
    "       \n",
    "        ids = [i for i in range(len(examples))]\n",
    "        srcs, texts, args = [], [], []\n",
    "        for example in examples:\n",
    "            text = example.getElementsByTagName('text')[0].firstChild.nodeValue\n",
    "            src = example.getAttribute('src')\n",
    "            \n",
    "            arg_temp = dict()\n",
    "            for arg in example.getElementsByTagName('arg'):\n",
    "                \n",
    "                arg_temp.update({arg.getAttribute('n'): arg.firstChild.nodeValue})\n",
    "                \n",
    "            texts.append(text)\n",
    "            srcs.append(src)\n",
    "            args.append(arg_temp)\n",
    "      \n",
    "        self.data_arg = pd.DataFrame({'id': ids, 'source': srcs, 'text': texts, 'arguments': args})\n",
    "\n",
    "    def __remove_argument__(self, index_role):\n",
    "        if index_role < 0 or index_role >= len(self.roles):\n",
    "            return\n",
    "        for i in range(len(self.data_arg['arguments'])):\n",
    "            if list(self.roles.items())[index_role][0] in self.data_arg['arguments'][i]:\n",
    "                self.data_arg['arguments'][i].pop(list(self.roles.items())[index_role][0])\n",
    "   \n",
    " \n",
    "    def dependency_parsing(self):\n",
    "        def print_dependency_parsing(token):\n",
    "            print(\n",
    "                f\"\"\"\n",
    "                TOKEN: {token.text}\n",
    "                =====\n",
    "                {token.tag_ = }\n",
    "                {token.head.text = }\n",
    "                {token.dep_ = }\n",
    "                {spacy.explain(token.dep_) = }\"\"\")\n",
    "        \n",
    "        max_len_arg = max([len(arg) for arg in self.data_arg['arguments'].values])\n",
    "        print(\"max_len_arg:\", max_len_arg)\n",
    "        count_args = [0 for i in range(max_len_arg)]\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        lst_index_remove = []\n",
    "        for i in tqdm(range(len(self.data_arg))):\n",
    "            doc = nlp(self.data_arg['text'][i])\n",
    "            root = [token for token in doc if token.head == token][0]\n",
    "            for token in doc:\n",
    "                for j in range(len(self.data_arg['arguments'][i])):\n",
    "                    if token.text in list(self.data_arg['arguments'][i].items())[j][1] and token.head.text == root.text:\n",
    "                        count_args[j] += 1\n",
    "        for j in range(len(count_args)):\n",
    "            if count_args[j] < len(self.data_arg) * self.min_threshold:\n",
    "                lst_index_remove.append(j)\n",
    "        # for index in sorted(lst_index_remove, reverse=True):\n",
    "        #     self.__remove_argument__(index)\n",
    "        self.data_arg.to_csv(self.output_data, index=False)\n",
    "        \n",
    "filename = PATH_DATA+ 'data/GramVar/'+ 'abolish.xml'\n",
    "\n",
    "preprocessor = Preprocessing(filename)\n",
    "preprocessor.read_xml_file()\n",
    "preprocessor.dependency_parsing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_checkpoint = \"dmis-lab/biobert-base-cased-v1.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "text = 'disruption of rpos, mlra or csga, and the curli subunit gene being able to abolish ha and curli production by strain chi7122 interrupt this open reading frame.'\n",
    "\n",
    "# distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "# print(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\")\n",
    "# print(f\"'>>> BERT number of parameters: 110M'\")\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "# token_logits = model(**inputs).logits\n",
    "# # Find the location of [MASK] and extract its logits\n",
    "# mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "# mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 23730,  1104,   187,  5674,  1116,   117,   182,  1233,  1611,\n",
       "          1137,   172,  1116,  2571,   117,  1105,  1103, 17331,  1182, 27555,\n",
       "          5565,  1217,  1682,  1106,   170, 15792,  2944,  5871,  1105, 17331,\n",
       "          1182,  1707,  1118, 10512, 22572,  1182,  1559, 11964,  1477, 19717,\n",
       "          1142,  1501,  3455,  4207,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
