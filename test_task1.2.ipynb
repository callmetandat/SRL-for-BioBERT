{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "def create_masked_data(dataDir, wriDir, tokenizer):\n",
    "    '''\n",
    "    create_masked_data('./MLM/mlm_prepared_data', './MLM/masked_data', tokenizer)\n",
    "    '''\n",
    "    def masking_sentence(token_ids, tokenizer):\n",
    "        '''\n",
    "        Function to mask each token in a sentence and return the masked sentence and the corresponding label\n",
    "        '''\n",
    "        except_tokens = [tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id]\n",
    "        \n",
    "        mask = np.random.binomial(1, 0.1, (len(token_ids),))   # Masking each token with 15% probability\n",
    "        print(\"mask:\",  np.where(mask)[0])\n",
    "        token_ids_copy = copy.deepcopy(token_ids)\n",
    "        masked_sentence = pd.DataFrame()\n",
    "        labels = [-100] * len(token_ids)\n",
    "        for idx, token in enumerate(token_ids):\n",
    "            \n",
    "            if (idx in np.where(mask)[0]) and (idx not in except_tokens):\n",
    "                token_ids_copy[idx] = tokenizer.mask_token_id\n",
    "                labels[idx] = token\n",
    "            \n",
    "        masked_sentence['masked_token_id'] = [token_ids_copy]\n",
    "        masked_sentence['label_id'] = [labels]\n",
    "            \n",
    "        return masked_sentence\n",
    "    \n",
    "    train_file = os.path.join(dataDir, 'dev_mlm_abolish_full.json')\n",
    "    train_data = pd.read_json(train_file, lines=True)\n",
    "    \n",
    "    masked_data_df = pd.DataFrame()\n",
    "    for idx, sen in enumerate(train_data['token_id']):\n",
    "        masked_sentence = masking_sentence(sen, tokenizer)\n",
    "        \n",
    "        masked_data_df = pd.concat([masked_data_df, masked_sentence], ignore_index=True)\n",
    "    \n",
    "    masked_data_df.to_json(os.path.join(wriDir, 'train_masked_data.json'), orient='records', lines=True)\n",
    "    \n",
    "    return masked_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"dmis-lab/biobert-base-cased-v1.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask: [ 8 31]\n",
      "mask: [11 31 34]\n",
      "mask: [ 5 10 16 23 28 32 35]\n",
      "mask: [ 3  8 29 45 46]\n",
      "mask: [ 6 12 15 23]\n",
      "mask: [ 9 15 18 27 28 42]\n",
      "mask: [ 4  6 13 16 47]\n",
      "mask: [15 18 21 26 30 38 46]\n",
      "mask: [ 4 11 16 17 21 27 29 35 39 43 45]\n",
      "mask: [35 44]\n",
      "mask: [ 1  2  6 16 38 44 49]\n",
      "mask: [ 9 16 18 38 43]\n",
      "mask: [ 1  5  6 16 38]\n",
      "mask: [11 29 34 45]\n",
      "mask: [ 1  2 14 29 43 47]\n",
      "mask: [27 29 36 38 49]\n",
      "mask: [14 24 45]\n",
      "mask: [ 2 12 18 19 20 23 25 26 35]\n",
      "mask: [28 29 30 41 44]\n",
      "mask: [ 3 11 21 41]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked_token_id</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 1748, 4982, 117, 5190, 4789, 1107, 19255...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 5871, 1105, 17331, 1182, 1707, 1118, 105...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 1748, 4982, 117, 1103, 103, 26883, 1320,...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, 3687, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 27914, 1116, 103, 1144, 8632, 2393, 1643...</td>\n",
       "      <td>[-100, -100, -100, 1134, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 1107, 5014, 117, 1748, 3687, 103, 1320, ...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, 26883, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[101, 2393, 1643, 3246, 1110, 8632, 1118, 2791...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[101, 1103, 3687, 26883, 103, 1104, 103, 3031,...</td>\n",
       "      <td>[-100, -100, -100, -100, 1320, -100, 194, -100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[101, 1103, 172, 1643, 1775, 10424, 1475, 1789...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[101, 181, 4559, 3080, 103, 1105, 2848, 14637,...</td>\n",
       "      <td>[-100, -100, -100, -100, 2007, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[101, 17895, 1104, 1103, 5250, 23984, 176, 657...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[101, 103, 103, 8661, 2629, 1108, 103, 1106, 1...</td>\n",
       "      <td>[-100, 1103, 172, -100, -100, -100, 1682, -100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[101, 17157, 1107, 1103, 175, 8495, 1161, 5565...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[101, 103, 4982, 117, 5190, 103, 103, 19255, 1...</td>\n",
       "      <td>[-100, 1748, -100, -100, -100, 4789, 1107, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[101, 1107, 1901, 117, 3687, 26883, 1320, 1104...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[101, 103, 103, 1186, 7861, 1110, 8632, 1118, ...</td>\n",
       "      <td>[-100, 176, 22454, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[101, 1107, 5014, 117, 1748, 3687, 26883, 1320...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[101, 1103, 3687, 26883, 1320, 1104, 194, 3031...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[101, 188, 103, 17292, 1108, 2423, 8632, 1118,...</td>\n",
       "      <td>[-100, -100, 10941, -100, -100, -100, -100, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[101, 1103, 3687, 26883, 1320, 1104, 194, 3031...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[101, 1649, 117, 103, 2335, 8116, 1104, 175, 6...</td>\n",
       "      <td>[-100, -100, -100, 1142, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      masked_token_id  \\\n",
       "0   [101, 1748, 4982, 117, 5190, 4789, 1107, 19255...   \n",
       "1   [101, 5871, 1105, 17331, 1182, 1707, 1118, 105...   \n",
       "2   [101, 1748, 4982, 117, 1103, 103, 26883, 1320,...   \n",
       "3   [101, 27914, 1116, 103, 1144, 8632, 2393, 1643...   \n",
       "4   [101, 1107, 5014, 117, 1748, 3687, 103, 1320, ...   \n",
       "5   [101, 2393, 1643, 3246, 1110, 8632, 1118, 2791...   \n",
       "6   [101, 1103, 3687, 26883, 103, 1104, 103, 3031,...   \n",
       "7   [101, 1103, 172, 1643, 1775, 10424, 1475, 1789...   \n",
       "8   [101, 181, 4559, 3080, 103, 1105, 2848, 14637,...   \n",
       "9   [101, 17895, 1104, 1103, 5250, 23984, 176, 657...   \n",
       "10  [101, 103, 103, 8661, 2629, 1108, 103, 1106, 1...   \n",
       "11  [101, 17157, 1107, 1103, 175, 8495, 1161, 5565...   \n",
       "12  [101, 103, 4982, 117, 5190, 103, 103, 19255, 1...   \n",
       "13  [101, 1107, 1901, 117, 3687, 26883, 1320, 1104...   \n",
       "14  [101, 103, 103, 1186, 7861, 1110, 8632, 1118, ...   \n",
       "15  [101, 1107, 5014, 117, 1748, 3687, 26883, 1320...   \n",
       "16  [101, 1103, 3687, 26883, 1320, 1104, 194, 3031...   \n",
       "17  [101, 188, 103, 17292, 1108, 2423, 8632, 1118,...   \n",
       "18  [101, 1103, 3687, 26883, 1320, 1104, 194, 3031...   \n",
       "19  [101, 1649, 117, 103, 2335, 8116, 1104, 175, 6...   \n",
       "\n",
       "                                             label_id  \n",
       "0   [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "1   [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "2   [-100, -100, -100, -100, -100, 3687, -100, -10...  \n",
       "3   [-100, -100, -100, 1134, -100, -100, -100, -10...  \n",
       "4   [-100, -100, -100, -100, -100, -100, 26883, -1...  \n",
       "5   [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "6   [-100, -100, -100, -100, 1320, -100, 194, -100...  \n",
       "7   [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "8   [-100, -100, -100, -100, 2007, -100, -100, -10...  \n",
       "9   [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "10  [-100, 1103, 172, -100, -100, -100, 1682, -100...  \n",
       "11  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "12  [-100, 1748, -100, -100, -100, 4789, 1107, -10...  \n",
       "13  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "14  [-100, 176, 22454, -100, -100, -100, -100, -10...  \n",
       "15  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "16  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "17  [-100, -100, 10941, -100, -100, -100, -100, -1...  \n",
       "18  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "19  [-100, -100, -100, 1142, -100, -100, -100, -10...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_masked_data('./MLM/mlm_prepared_data', './MLM/masked_data', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking_sentence(token_ids, tokenizer):\n",
    "    '''\n",
    "    Function to mask each token in a sentence and return the masked sentence and the corresponding label\n",
    "    '''\n",
    "    except_tokens = [tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id]\n",
    "    \n",
    "    mask = np.random.binomial(1, 0.1, (len(token_ids),))   # Masking each token with 15% probability\n",
    "    print(\"mask:\",  np.where(mask)[0])\n",
    "    token_ids_copy = copy.deepcopy(token_ids)\n",
    "    masked_sentence = pd.DataFrame()\n",
    "    labels = [-100] * len(token_ids)\n",
    "    for idx, token in enumerate(token_ids):\n",
    "        \n",
    "        if (idx in np.where(mask)[0]) and (idx not in except_tokens):\n",
    "            token_ids_copy[idx] = tokenizer.mask_token_id\n",
    "            labels[idx] = token\n",
    "        \n",
    "    masked_sentence['masked_token_id'] = [token_ids_copy]\n",
    "    masked_sentence['label_id'] = [labels]\n",
    "         \n",
    "    return masked_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = [101,1107,5014,117,1748,3687,26883,1320,1104,1103,172,118,6020,14715,11179,16617,5777,1107,1103,185,7897,1571,21392,1116,171,1604,1105,171,1580,8632,15416,1348,23842,117,6142,4422,3687,26883,1320,1104,1103,21996,184,5822,11478,27105,17853,113,11769,102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask: [ 1  3 13 25 40]\n"
     ]
    }
   ],
   "source": [
    "a = masking_sentence(token_id, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked_token_id</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 103, 4982, 117, 5190, 4789, 1107, 103, 1...</td>\n",
       "      <td>[-100, 1748, -100, -100, -100, -100, -100, 192...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     masked_token_id  \\\n",
       "0  [101, 103, 4982, 117, 5190, 4789, 1107, 103, 1...   \n",
       "\n",
       "                                            label_id  \n",
       "0  [-100, 1748, -100, -100, -100, -100, -100, 192...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 1748,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 19255,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 1151,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 1103,\n",
       " 3687,\n",
       " 26883,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 117,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 1648,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['label_id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
