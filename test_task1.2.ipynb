{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_token_index:  tensor([29])\n",
      "mask_token_logits:  tensor([[-4.6412, -3.5301, -3.9047,  ..., -2.6799, -4.3767, -3.4781]],\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -7.1181,  -7.3490,  -7.2368,  ...,  -5.9377,  -5.8142,  -5.6505],\n",
       "         [ -8.9271,  -9.0926,  -9.4347,  ...,  -5.4978,  -7.8965,  -7.1630],\n",
       "         [ -9.7153, -10.0035, -10.7065,  ...,  -9.1228,  -9.3101, -10.5179],\n",
       "         ...,\n",
       "         [ -7.5503,  -7.9151,  -7.2499,  ...,  -5.8271,  -6.6470,  -5.8421],\n",
       "         [-11.1996, -11.9432, -11.5165,  ..., -10.1310, -11.2889,  -9.8898],\n",
       "         [ -7.1008,  -8.0511,  -7.6138,  ...,  -6.1989,  -6.6256,  -6.3939]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "model = AutoModelForMaskedLM.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "    \n",
    "text = 'the a/t-rich mut sequence indicates that normal splicing was abolished by a g-to-a transition at the first [MASK] of intron 2.'\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "print(\"mask_token_index: \", mask_token_index)\n",
    "\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "print(\"mask_token_logits: \", mask_token_logits)\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 10020, dim=1).indices[0].tolist()\n",
    "\n",
    "token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy==3.5.0\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: B-A0\n",
      "g: I-A0\n",
      "to: I-A0\n",
      "a: I-A0\n",
      "transition: I-A0\n",
      "at: I-A0\n",
      "the: I-A0\n",
      "first: I-A0\n",
      "nucleotide: I-A0\n",
      "of: I-A0\n",
      "intron: I-A0\n",
      "2: I-A0\n",
      "of: O\n",
      "patient: O\n",
      "1: O\n",
      "abolished: O\n",
      "normal: B-A1\n",
      "splicing: I-A1\n",
      ".: O\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def assign_labels(sentence, arguments):\n",
    "    # Tokenize the sentence into words\n",
    "    sentence = sentence.lower()\n",
    "    words = re.findall(r'\\w+|[;,.]', sentence)\n",
    "\n",
    "    # Create a list to store labels for each word\n",
    "    labels = ['O'] * len(words)\n",
    "\n",
    "    # Iterate through the arguments and assign labels\n",
    "    for arg_id, arg_text in arguments.items():\n",
    "        # Tokenize the argument into words\n",
    "        arg_text = arg_text.lower()\n",
    "        arg_words = re.findall(r'\\w+|[;,.]', arg_text)\n",
    "\n",
    "        # Iterate through the words in the sentence\n",
    "        for i in range(len(words) - len(arg_words) + 1):\n",
    "            if words[i:i+len(arg_words)] == arg_words:\n",
    "                # Assign a label based on the argument key\n",
    "                for j in range(len(arg_words)):\n",
    "                    if j == 0:\n",
    "                        labels[i+j] = f'B-A{arg_id}'\n",
    "                    else:\n",
    "                        labels[i+j] = f'I-A{arg_id}'\n",
    "\n",
    "    return words, labels\n",
    "\n",
    "# Example usage:\n",
    "sentence = \"A G-to-A transition at the first nucleotide of intron 2 of patient 1 abolished normal splicing.\"\n",
    "arguments = {0: \"a G-to-A transition at the first nucleotide of intron 2\", 1: \"normal splicing\"}\n",
    "\n",
    "words, labels = assign_labels(sentence, arguments)\n",
    "\n",
    "for word, label in zip(words, labels):\n",
    "    print(f\"{word}: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a function to detect base verbs\n",
    "def detect_base_verbs(sentence):\n",
    "    # Process the input sentence with spaCy\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Initialize a list to store base verbs\n",
    "    base_verbs = []\n",
    "\n",
    "    # Iterate through the tokens in the sentence\n",
    "    for i, token in enumerate(doc):\n",
    "        \n",
    "        print(\"token:\", token)\n",
    "        print(\"token.pos:\", token.pos_)\n",
    "        print(\"token.dep:\", token.dep_)\n",
    "        print(\"token.lemma:\", token.lemma_)\n",
    "        print('\\n')\n",
    "        # Check if the token is a verb (POS tag starts with 'V') and not a auxiliary verb (aux)\n",
    "        if (token.pos_.startswith('V')) and token.dep_ != 'aux':\n",
    "            if token.lemma_ == 'truncate':\n",
    "                # If the token is a pronoun, use the text of the token\n",
    "                print(\"DAY NE\", token)\n",
    "            base_verbs.append(token.lemma_)\n",
    "\n",
    "    return base_verbs\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"frameshift sox9 mutations, as our data show, have the probability of actually truncating its two activation domains, while all missense mutations reported to date lie in the high mobility group (hmg) dna-binding domain.\" \n",
    "sen = \"electrophoretic truncation shift assays truncated that the full-length bcl6 protein extracted from transfected cos cells and a bacterially expressed protein that contains the bcl6 zinc fingers and may be remarkably truncated can bind specifically to dna from the u3 promoter/enhancer region of hiv-1.\"\n",
    "\n",
    "sen2 = \"Specifically, the Stat5a molecule in which the C-terminus can be truncated at amino acids 740 or 751 effectively blocked the induction of both CIS and OSM, whereas the C-terminal truncations at amino acids 762 or 773 had no effect on the induction of either gene.\"\n",
    "# Detect base verbs in the sentence\n",
    "detect_base_verbs(sen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['a', 'g-to-a', 'transition', 'at', 'the', 'first', 'nucleotide', 'of', 'intron', '2', 'of', 'patient', '1', '#abolish', 'normal', 'splicing', '.']\n",
    "\n",
    "words = ['frameshift', 'sox9', 'mutations', ',', 'as', 'our', 'data', 'show', ',', 'have', 'the', 'probability', 'of', 'actually', 'truncating', 'its', 'two', 'activation', 'domains', ',', 'while', 'all', 'missense', 'mutations', 'reported', 'to', 'date', 'lie', 'in', 'the', 'high', 'mobility', 'group', '(hmg)', 'dna-binding', 'domain', '.']\n",
    "def analyze_word(word, lowercase=True):\n",
    "    token = nlp(word)\n",
    "   \n",
    "    if token.pos_.startswith('V') and token.dep_ != 'aux':\n",
    "       \n",
    "        lemma = token[0].lemma_\n",
    "        if lowercase: lemma = lemma.lower()\n",
    "    return lemma, token[0].pos_\n",
    "\n",
    "print(analyze_word('truncated'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 0 documents\n",
      "[nltk_data] Downloading package punkt to /home/phatpham/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "0 lines written\n"
     ]
    }
   ],
   "source": [
    "!python xml2conll/xml2conll.py --input='./MLM/data/GramVar/abolish_full.xml' --output='tessst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
