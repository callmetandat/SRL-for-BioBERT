{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: type: <class 'list'>\n",
      "prediction: ['O', 'O', 'O', 'O', 'O', 'O', 'X', 'O', 'O', '[CLS]', 'O', 'O', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "label: ['B-A0', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'B-A2', 'I-A2', 'O', 'B-A1', 'I-A1', 'I-A1', 'O', 'O', 'O', 'O']\n",
      "lineTok: ['B-A0', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'B-A2', 'I-A2', 'O', 'B-A1', 'I-A1', 'I-A1', 'O', 'O', 'O', 'O']\n",
      "lineLab: ['O', 'O', 'O', 'O', 'O', 'O', 'X', 'O', 'O', '[CLS]', 'O', 'O', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "prediction_new: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "label_new: ['B-A0', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-A2', 'I-A2', 'O', 'O', 'I-A1', 'I-A1', 'O', 'O', 'O', 'O']\n",
      "new1: [['B-A0', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-A2', 'I-A2', 'O', 'O', 'I-A1', 'I-A1', 'O', 'O', 'O', 'O']]\n",
      "result_f1: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import ast\n",
    "\n",
    "\n",
    "data=[]\n",
    "prediction_new = []\n",
    "label_new = []\n",
    "\n",
    "# Read the file and extract the prediction and label columns\n",
    "line = \"17625\t['O', 'O', 'O', 'O', 'O', 'O', 'X', 'O', 'O', '[CLS]', 'O', 'O', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O']\t['B-A0', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'B-A2', 'I-A2', 'O', 'B-A1', 'I-A1', 'I-A1', 'O', 'O', 'O', 'O']\"\n",
    "\n",
    "\n",
    "uid, prediction, label = line.strip().split(\"\\t\")\n",
    "data.append((prediction.strip(\"[]\"), label.strip(\"[]\")))\n",
    "\n",
    "print(f\"data: type: {type(data)}\")\n",
    "print(f\"prediction: {prediction}\")\n",
    "print(f\"label: {label}\")\n",
    "# convert list to string\n",
    "predictions_data = [d[0] for d in data]\n",
    "labels_data = [d[1] for d in data]\n",
    "\n",
    "lineTok = ast.literal_eval(label)\n",
    "lineLab = ast.literal_eval(prediction)\n",
    "\n",
    "print(f\"lineTok: {lineTok}\")\n",
    "print(f\"lineLab: {lineLab}\")\n",
    "for (Tok, Lab) in zip(lineTok, lineLab):\n",
    "    \n",
    "    if Lab in ['[CLS]','[SEP]', 'X']: # replace non-text tokens with O. These will not be evaluated.\n",
    "        prediction_new.append('O')\n",
    "        label_new.append('O')\n",
    "        continue\n",
    "    if(Lab == \"B-V\"):\n",
    "        prediction_new.append(\"V\")\n",
    "    else:\n",
    "        prediction_new.append(Lab)\n",
    "        label_new.append(Tok)\n",
    "        \n",
    "print(f\"prediction_new: {prediction_new}\")\n",
    "print(f\"label_new: {label_new}\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import recall_score as class_recall_score\n",
    "\n",
    "new1 = [label_new]\n",
    "print(f\"new1: {new1}\")\n",
    "new2 = [prediction_new]\n",
    "\n",
    "result_f1 = f1_score(label_new, prediction_new,average=\"micro\")\n",
    "\n",
    "print(f\"result_f1: {result_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "predictions = []\n",
    "\n",
    "filePath = \"./output/conllsrl_test_predictions_29.tsv\"\n",
    "with open(filePath, \"r\") as f:\n",
    "    for line in f:\n",
    "        # delete the first line\n",
    "        if \"uid\" in line:\n",
    "            continue\n",
    "        uid, prediction, label = line.strip().split(\"\\t\")\n",
    "        labels.append(label)\n",
    "        predictions.append(prediction)\n",
    "        data.append((prediction.strip(\"[]\"), label.strip(\"[]\")))\n",
    "\n",
    "print(\"prediction: \", type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  <class 'list'>\n",
      "result_f1: 0.761921602045164\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "predictions = []\n",
    "\n",
    "filePath = \"./output/conllsrl_test_predictions_29.tsv\"\n",
    "with open(filePath, \"r\") as f:\n",
    "    for line in f:\n",
    "        # delete the first line\n",
    "        if \"uid\" in line:\n",
    "            continue\n",
    "        uid, prediction, label = line.strip().split(\"\\t\")\n",
    "        labels.append(label)\n",
    "        predictions.append(prediction)\n",
    "        data.append((prediction.strip(\"[]\"), label.strip(\"[]\")))\n",
    "\n",
    "print(\"prediction: \", type(data))\n",
    "# apply this conversion to all the elements in the list\n",
    "lineToks = [ast.literal_eval(label) for label in labels]\n",
    "lineLabs = [ast.literal_eval(prediction) for prediction in predictions]\n",
    "\n",
    "trueLabels = [] # labels\n",
    "predictLabels = [] # predictions\n",
    "\n",
    "for lineTok, lineLab in zip(lineToks, lineLabs):\n",
    "    if lineLab in ['[CLS]','[SEP]', 'X']: # replace non-text tokens with O. These will not be evaluated.\n",
    "        predictLabels.append('O')\n",
    "        trueLabels.append('O')\n",
    "        continue\n",
    "    if(lineLab == \"B-V\"):\n",
    "        predictLabels.append(\"V\")\n",
    "    else:\n",
    "        predictLabels.append(lineLab)\n",
    "        trueLabels.append(lineTok) \n",
    " \n",
    "# flatten the list of lists into a single list\n",
    "trueLabels = [item for sublist in trueLabels for item in sublist]\n",
    "predictLabels = [item for sublist in predictLabels for item in sublist]\n",
    "       \n",
    "result_f1 = f1_score(trueLabels, predictLabels,average=\"micro\")\n",
    "\n",
    "print(f\"result_f1: {result_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dmis-lab/biobert-base-cased-v1.2\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True,truncation=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"the a/t-rich mut sequence indicates that normal splicing was abolished by a g-to-a transition at the first nucleotide of intron 2.\"\n",
    "text1 = \"torc induction was abolished by deletion of the distal tor box (box1), which interrupt this open reading frame.\"\n",
    "text2 = \"torc induction was abolished by deletion of the distal tor box (box3), when demolish this writing frame.\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(\"[CLS] \" + text1 + \" [SEP]\")\n",
    "tokenized_text2 = tokenizer.tokenize(\"[CLS] \" + text2 + \" [SEP]\")\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "indexed_tokens2 = tokenizer.convert_tokens_to_ids(tokenized_text2)\n",
    "\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "segments_ids2 = [1] * len(tokenized_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(segments_ids))\n",
    "print(len(segments_ids2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "tokens_tensor2 = torch.tensor([indexed_tokens2])\n",
    "segments_tensors2 = torch.tensor([segments_ids2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    outputs2 = model(tokens_tensor2, segments_tensors2)\n",
    "    hidden_states = outputs[2]\n",
    "    hidden_states2 = outputs2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "token_embeddings2 = torch.stack(hidden_states2, dim=0)\n",
    "\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings2 = torch.squeeze(token_embeddings2, dim=1)\n",
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "token_embeddings2 = token_embeddings2.permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "token_vecs_cat2 = []\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "    \n",
    "for token in token_embeddings2:\n",
    "    cat_vec2 = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    token_vecs_cat2.append(cat_vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test cosine similarity\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_2_tensors(tensor1, tensor2):\n",
    "    \n",
    "    # Compute the dot product\n",
    "    dot_product = torch.dot(tensor1, tensor2)\n",
    "\n",
    "    # Compute the L2 (Euclidean) norms\n",
    "    norm_tensor1 = torch.norm(tensor1)\n",
    "    norm_tensor2 = torch.norm(tensor2)\n",
    "\n",
    "    # Calculate the cosine similarity\n",
    "    similarity = dot_product / (norm_tensor1 * norm_tensor2)\n",
    "\n",
    "    return similarity\n",
    "\n",
    "def cosine_similarity(sen1, sen2):\n",
    "    assert len(sen1) == len(sen2), \"Sentence lengths are not equal\"\n",
    "        \n",
    "    res_vec = []\n",
    "    for word in range(len(sen1)):\n",
    "        res_vec.append(cosine_similarity_2_tensors(sen1[word], sen2[word]))\n",
    "    return res_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_res = cosine_similarity(token_vecs_cat, token_vecs_cat2)\n",
    "cosine_similarity_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test cosine module similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_module_2_tensors(tensor1, tensor2):\n",
    "    cosine = cosine_similarity_2_tensors(tensor1, tensor2)\n",
    "    module = 1 - torch.abs(torch.norm(tensor1) - torch.norm(tensor2))/(torch.norm(tensor1) + torch.norm(tensor2))\n",
    "    \n",
    "    cosine_module = 1/2 * (cosine + module)\n",
    "\n",
    "    return cosine_module\n",
    "\n",
    "def cosine_module(sen1, sen2):\n",
    "    assert len(sen1) == len(sen2), \"Sentence lengths are not equal\"\n",
    "        \n",
    "    res_vec = []\n",
    "    for word in range(len(sen1)):\n",
    "        res_vec.append(cosine_module_2_tensors(sen1[word], sen2[word]))\n",
    "    return res_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_module_res = cosine_module(token_vecs_cat, token_vecs_cat2)\n",
    "cosine_module_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "test = []\n",
    "with open('word_vecs_output/vecs_ner_coNLL_testa_abolish.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 2, 'vec': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'new_key': 'new_value'},\n",
       " {'id': 3,\n",
       "  'vec': [11, 12, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "  'new_key': 'new_value'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = [{'id': 2,\n",
    "        'vec': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]},\n",
    "{'id': 3,'vec': [11, 12, 12, 13, 14, 15, 16, 17, 18, 19]}]\n",
    "        \n",
    "\n",
    "# add new key to the dict\n",
    "for d in dict:\n",
    "        d['new_key'] = 'new_value'\n",
    "        \n",
    "dict\n",
    "\n",
    "# add new dict to the list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/Phat Pham/Documents/THESIS/SRL-for-BioBERT/test.ipynb Cell 21\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Phat%20Pham/Documents/THESIS/SRL-for-BioBERT/test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m TaskType, ModelType, NLP_MODELS\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Phat%20Pham/Documents/THESIS/SRL-for-BioBERT/test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Load pre-trained model (weights)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Phat%20Pham/Documents/THESIS/SRL-for-BioBERT/test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m loadedDict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m./output/multi_task_model_9_13050.pt\u001b[39;49m\u001b[39m'\u001b[39;49m, map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Phat%20Pham/Documents/THESIS/SRL-for-BioBERT/test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m taskParams \u001b[39m=\u001b[39m loadedDict[\u001b[39m'\u001b[39m\u001b[39mtask_params\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Phat%20Pham/Documents/THESIS/SRL-for-BioBERT/test.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m modelName \u001b[39m=\u001b[39mtaskParams\u001b[39m.\u001b[39mmodelType\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39mlower()\n",
      "File \u001b[0;32m~/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1112\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1116\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1117\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m     _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m typed_storage\u001b[39m.\u001b[39m_data_ptr() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1121\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/serialization.py:1086\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrestore_location\u001b[39m(storage, location):\n\u001b[0;32m-> 1086\u001b[0m     \u001b[39mreturn\u001b[39;00m default_restore_location(storage, \u001b[39mstr\u001b[39;49m(map_location))\n",
      "File \u001b[0;32m~/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[1;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "#making and loading model\n",
    "import torch\n",
    "from models.model import multiTaskModel\n",
    "from utils.data_utils import TaskType, ModelType, NLP_MODELS\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "loadedDict = torch.load('./output/multi_task_model_9_13050.pt', map_location=torch.device('cuda'))\n",
    "\n",
    "taskParams = loadedDict['task_params']\n",
    "\n",
    "modelName =taskParams.modelType.name.lower()\n",
    "_, _ , tokenizerClass, defaultName = NLP_MODELS[modelName]\n",
    "configName = taskParams.modelConfig\n",
    "\n",
    "#making tokenizer for model\n",
    "tokenizer = tokenizerClass.from_pretrained(configName)\n",
    "\n",
    "allParams = {}\n",
    "allParams['task_params'] = taskParams\n",
    "allParams['gpu'] = torch.cuda.is_available()\n",
    "# dummy values\n",
    "allParams['num_train_steps'] = 10\n",
    "allParams['warmup_steps'] = 0\n",
    "allParams['learning_rate'] = 2e-5\n",
    "allParams['epsilon'] = 1e-8\n",
    "\n",
    "model = multiTaskModel(allParams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attension_masks = torch.tensor([[1] * len(tokenized_text)])\n",
    "output = model.network(tokens_tensor, segments_tensors, attension_masks, 0, 'conllsrl')\n",
    "output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()\n",
    "\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n",
    "print(token_vecs_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taskName :  odict_values(['conllsrl'])\n",
      "allTasksList :  [{'data_task_id': 0, 'data_': [{'uid': 0, 'label': [12, 2, 13, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'token_id': [101, 1045, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'type_id': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}], 'data_task_type': <TaskType.NER: 3>, 'data_task_name': 'conllsrl'}]\n",
      "inferDataLoader :  <torch.utils.data.dataloader.DataLoader object at 0x7fa01171aeb0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 1/1 [00:00<00:00,  5.95it/s]\n"
     ]
    }
   ],
   "source": [
    "text = [\"in addition, deletion of the distal tor box (box1) abolished torc induction whereas the presence of a dna fragment starting three bases upstream from box1 suffices for normal torc expression.\"]\n",
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "from infer_pipeline import inferPipeline\n",
    "from utils.data_utils import TaskType\n",
    "pipe = inferPipeline(modelPath='./output/multi_task_model_8_367.pt', maxSeqLen=50)\n",
    "\n",
    "data = pipe.infer(text, ['conllsrl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "def read_json_file(dirPath, wrtDir):\n",
    "    files = []\n",
    "    for path in os.listdir(dirPath):\n",
    "        if os.path.isfile(os.path.join(dirPath, path)):\n",
    "            files.append(path)\n",
    "            \n",
    "    \n",
    "    for file in files:\n",
    "        with open(os.path.join(dirPath, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "        vec_origin = get_embedding(data)\n",
    "        vec_finetuned = get_embedding_finetuned(data)\n",
    "        cosine = cosine_similarity(vec_origin, vec_finetuned)\n",
    "        cosine_module = cosine_module(vec_origin, vec_finetuned)\n",
    "        \n",
    "        features = {'uid': data['uid'], 'cosine': cosine, 'cosine_module': cosine_module}\n",
    "        with open(os.path.join(wrtDir, 'vecs_{}.pkl'.format(file.split('.')[0])), 'wb') as vecs_wri:\n",
    "            pickle.dump(features, vecs_wri)\n",
    "    return data\n",
    "\n",
    "data = read_json_file('./word_vecs_output/')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data('../data/coNLL_tsv/bert-base-uncased_prepared_data/ner_conll_testa_abolish.json')\n",
    "    \n",
    "print(\"data shape: \", len(data))  # 280\n",
    "tokens_id = data[0]['token_id']\n",
    "segments_id = data[0]['type_id']\n",
    "\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([tokens_id])\n",
    "segments_tensors = torch.tensor([segments_id])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('dmis-lab/biobert-base-cased-v1.2',\n",
    "                                output_hidden_states = True # Whether the model returns all hidden-states.\n",
    "                                )\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    print(\"outputs: \", len(outputs)) # 2\n",
    "    hidden_states = outputs[2]\n",
    "    \n",
    "    \n",
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")  #13\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i])) #1\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i])) #50\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))     # 768\n",
    "\n",
    "\n",
    "# `hidden_states` is a Python list.\n",
    "print('      Type of hidden_states: ', type(hidden_states)) # <class 'tuple'>\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', hidden_states[0].size()) # torch.Size([1, 50, 768])\n",
    "\n",
    "\n",
    "## Sentence Vectors\n",
    "# `hidden_states` has shape [13 x 1 x 50 x 768]\n",
    "\n",
    "# `token_vecs` is a tensor with shape [50 x 768]\n",
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size()) # torch.Size([768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_vecs_cat:  2\n",
      "['22110', '22111', '22112', '22113', '22114', '22115', '22116', '22117', '22118', '22119', '22120', '22121', '22122', '22123', '22124', '22125', '22126', '22127', '22128', '22129', '22130', '22131', '22132', '22133', '22134', '22135', '22136', '22137', '22138', '22139', '22140', '22141']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('models/outputs.pkl', 'rb') as f:\n",
    "    token_vecs_cat = pickle.load(f)\n",
    "\n",
    "print(\"token_vecs_cat: \", len(token_vecs_cat)) # 50\n",
    "print(token_vecs_cat['uid']) # (3072,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
