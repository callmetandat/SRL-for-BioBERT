{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: type: <class 'list'>\n",
      "prediction: ['O', 'O', 'O', 'O', 'O', 'O', 'X', 'O', 'O', '[CLS]', 'O', 'O', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "label: ['B-A0', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'B-A2', 'I-A2', 'O', 'B-A1', 'I-A1', 'I-A1', 'O', 'O', 'O', 'O']\n",
      "lineTok: ['B-A0', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'B-A2', 'I-A2', 'O', 'B-A1', 'I-A1', 'I-A1', 'O', 'O', 'O', 'O']\n",
      "lineLab: ['O', 'O', 'O', 'O', 'O', 'O', 'X', 'O', 'O', '[CLS]', 'O', 'O', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "prediction_new: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "label_new: ['B-A0', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-A2', 'I-A2', 'O', 'O', 'I-A1', 'I-A1', 'O', 'O', 'O', 'O']\n",
      "new1: [['B-A0', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-A2', 'I-A2', 'O', 'O', 'I-A1', 'I-A1', 'O', 'O', 'O', 'O']]\n",
      "result_f1: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import ast\n",
    "\n",
    "\n",
    "data=[]\n",
    "prediction_new = []\n",
    "label_new = []\n",
    "\n",
    "# Read the file and extract the prediction and label columns\n",
    "line = \"17625\t['O', 'O', 'O', 'O', 'O', 'O', 'X', 'O', 'O', '[CLS]', 'O', 'O', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O']\t['B-A0', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'B-A2', 'I-A2', 'O', 'B-A1', 'I-A1', 'I-A1', 'O', 'O', 'O', 'O']\"\n",
    "\n",
    "\n",
    "uid, prediction, label = line.strip().split(\"\\t\")\n",
    "data.append((prediction.strip(\"[]\"), label.strip(\"[]\")))\n",
    "\n",
    "print(f\"data: type: {type(data)}\")\n",
    "print(f\"prediction: {prediction}\")\n",
    "print(f\"label: {label}\")\n",
    "# convert list to string\n",
    "predictions_data = [d[0] for d in data]\n",
    "labels_data = [d[1] for d in data]\n",
    "\n",
    "lineTok = ast.literal_eval(label)\n",
    "lineLab = ast.literal_eval(prediction)\n",
    "\n",
    "print(f\"lineTok: {lineTok}\")\n",
    "print(f\"lineLab: {lineLab}\")\n",
    "for (Tok, Lab) in zip(lineTok, lineLab):\n",
    "    \n",
    "    if Lab in ['[CLS]','[SEP]', 'X']: # replace non-text tokens with O. These will not be evaluated.\n",
    "        prediction_new.append('O')\n",
    "        label_new.append('O')\n",
    "        continue\n",
    "    if(Lab == \"B-V\"):\n",
    "        prediction_new.append(\"V\")\n",
    "    else:\n",
    "        prediction_new.append(Lab)\n",
    "        label_new.append(Tok)\n",
    "        \n",
    "print(f\"prediction_new: {prediction_new}\")\n",
    "print(f\"label_new: {label_new}\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import recall_score as class_recall_score\n",
    "\n",
    "new1 = [label_new]\n",
    "print(f\"new1: {new1}\")\n",
    "new2 = [prediction_new]\n",
    "\n",
    "result_f1 = f1_score(label_new, prediction_new,average=\"micro\")\n",
    "\n",
    "print(f\"result_f1: {result_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "predictions = []\n",
    "\n",
    "filePath = \"./output/conllsrl_test_predictions_29.tsv\"\n",
    "with open(filePath, \"r\") as f:\n",
    "    for line in f:\n",
    "        # delete the first line\n",
    "        if \"uid\" in line:\n",
    "            continue\n",
    "        uid, prediction, label = line.strip().split(\"\\t\")\n",
    "        labels.append(label)\n",
    "        predictions.append(prediction)\n",
    "        data.append((prediction.strip(\"[]\"), label.strip(\"[]\")))\n",
    "\n",
    "print(\"prediction: \", type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  <class 'list'>\n",
      "result_f1: 0.761921602045164\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "predictions = []\n",
    "\n",
    "filePath = \"./output/conllsrl_test_predictions_29.tsv\"\n",
    "with open(filePath, \"r\") as f:\n",
    "    for line in f:\n",
    "        # delete the first line\n",
    "        if \"uid\" in line:\n",
    "            continue\n",
    "        uid, prediction, label = line.strip().split(\"\\t\")\n",
    "        labels.append(label)\n",
    "        predictions.append(prediction)\n",
    "        data.append((prediction.strip(\"[]\"), label.strip(\"[]\")))\n",
    "\n",
    "print(\"prediction: \", type(data))\n",
    "# apply this conversion to all the elements in the list\n",
    "lineToks = [ast.literal_eval(label) for label in labels]\n",
    "lineLabs = [ast.literal_eval(prediction) for prediction in predictions]\n",
    "\n",
    "trueLabels = [] # labels\n",
    "predictLabels = [] # predictions\n",
    "\n",
    "for lineTok, lineLab in zip(lineToks, lineLabs):\n",
    "    if lineLab in ['[CLS]','[SEP]', 'X']: # replace non-text tokens with O. These will not be evaluated.\n",
    "        predictLabels.append('O')\n",
    "        trueLabels.append('O')\n",
    "        continue\n",
    "    if(lineLab == \"B-V\"):\n",
    "        predictLabels.append(\"V\")\n",
    "    else:\n",
    "        predictLabels.append(lineLab)\n",
    "        trueLabels.append(lineTok) \n",
    " \n",
    "# flatten the list of lists into a single list\n",
    "trueLabels = [item for sublist in trueLabels for item in sublist]\n",
    "predictLabels = [item for sublist in predictLabels for item in sublist]\n",
    "       \n",
    "result_f1 = f1_score(trueLabels, predictLabels,average=\"micro\")\n",
    "\n",
    "print(f\"result_f1: {result_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary) dmis-lab/biobert-base-cased-v1.2\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True,truncation=True)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2', do_lower_case=True,truncation=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "['[CLS]', 'disruption', 'of', 'r', '##po', '##s', ',', 'm', '##l', '##ra', 'or', 'c', '##s', '##ga', ',', 'and', 'the', 'curl', '##i', 'subunit', 'gene', 'being', 'able', 'to', 'a', '##bol', '##ish', 'ha', 'and', 'curl', '##i', 'production', 'by', 'strain', 'ch', '##i', '##7', '##12', '##2', 'interrupt', 'this', 'open', 'reading', 'frame', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#text = \"the a/t-rich mut sequence indicates that normal splicing was abolished by a g-to-a transition at the first nucleotide of intron 2.\"\n",
    "text2 = \"disruption of rpos, mlra or csga, and the curli subunit gene being able to abolish ha and curli production by strain chi7122 interrupt this open reading frame.\"\n",
    "text1 = \"torc induction was abolished by deletion of the distal tor box (box1), which interrupt this open reading frame.\"\n",
    "\n",
    "text = \"in addition, deletion of the distal tor box (box1) abolished torc induction whereas the presence of a dna fragment starting three bases upstream from box1 suffices for normal torc expression.\"\n",
    "\n",
    "\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "tokenized_text2 = tokenizer.tokenize(\"[CLS] \" + text2 + \" [SEP]\")\n",
    "# Print out the tokens.\n",
    "print (len(tokenized_text))\n",
    "print (tokenized_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "indexed_tokens2 = tokenizer.convert_tokens_to_ids(tokenized_text2)\n",
    "# # Display the words with their indeces.\n",
    "# for tup in zip(tokenized_text, indexed_tokens):\n",
    "#     print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "out = tokenizer.encode_plus(text = indexed_tokens, add_special_tokens=False,\n",
    "                                        truncation_strategy ='only_first',\n",
    "                                        max_length = 50, pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenIds: [101, 1107, 1901, 117, 3687, 26883, 1320, 1104, 1103, 4267, 19760, 1106, 1197, 2884, 113, 2884, 1475, 114, 8632, 1106, 19878, 18293, 6142, 1103, 2915, 1104, 170, 173, 1605, 17906, 2547, 1210, 7616, 15011, 1121, 2884, 1475, 28117, 3101, 18117, 1111, 2999, 1106, 19878, 2838, 119, 102, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "tokenIds = out['input_ids']\n",
    "typeIds = out['token_type_ids']\n",
    "\n",
    "print(f\"tokenIds: {tokenIds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the 29 tokens as belonging to sentence \"1\".\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "segments_ids2 = [1] * len(tokenized_text2)\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making and loading model\n",
    "import torch\n",
    "from models.model import multiTaskModel\n",
    "from utils.data_utils import TaskType, ModelType, NLP_MODELS\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "loadedDict = torch.load('./output/multi_task_model_9_13050.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "taskParams = loadedDict['task_params']\n",
    "\n",
    "modelName =taskParams.modelType.name.lower()\n",
    "_, _ , tokenizerClass, defaultName = NLP_MODELS[modelName]\n",
    "configName = taskParams.modelConfig\n",
    "\n",
    "#making tokenizer for model\n",
    "tokenizer = tokenizerClass.from_pretrained(configName)\n",
    "\n",
    "allParams = {}\n",
    "allParams['task_params'] = taskParams\n",
    "allParams['gpu'] = torch.cuda.is_available()\n",
    "# dummy values\n",
    "allParams['num_train_steps'] = 10\n",
    "allParams['warmup_steps'] = 0\n",
    "allParams['learning_rate'] = 2e-5\n",
    "allParams['epsilon'] = 1e-8\n",
    "\n",
    "model = multiTaskModel(allParams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attension_masks = torch.tensor([[1] * len(tokenized_text)])\n",
    "output = model.network(tokens_tensor, segments_tensors, attension_masks, 0, 'conllsrl')\n",
    "output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()\n",
    "\n",
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n",
    "print(token_vecs_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"in addition, deletion of the distal tor box (box1) abolished torc induction whereas the presence of a dna fragment starting three bases upstream from box1 suffices for normal torc expression.\"\n",
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "from infer_pipeline import inferPipeline\n",
    "from utils.data_utils import TaskType\n",
    "pipe = inferPipeline(modelPath='./output/multi_task_model_8_367.pt', maxSeqLen=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity between two word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cosine_sim(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two 2D tensors using PyTorch.\n",
    "\n",
    "    Args:\n",
    "        tensor1 (torch.Tensor): The first 2D tensor.\n",
    "        tensor2 (torch.Tensor): The second 2D tensor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The cosine similarity between the two tensors.\n",
    "    \"\"\"\n",
    "    # Flatten both tensors to 1D arrays\n",
    "    flat_tensor1 = tensor1.view(-1)\n",
    "    flat_tensor2 = tensor2.view(-1)\n",
    "\n",
    "    # Compute the dot product\n",
    "    dot_product = torch.dot(flat_tensor1, flat_tensor2)\n",
    "\n",
    "    # Compute the L2 (Euclidean) norms\n",
    "    norm_tensor1 = torch.norm(flat_tensor1)\n",
    "    norm_tensor2 = torch.norm(flat_tensor2)\n",
    "\n",
    "    # Calculate the cosine similarity\n",
    "    similarity = dot_product / (norm_tensor1 * norm_tensor2)\n",
    "\n",
    "    return similarity\n",
    "\n",
    "def cosine_similarity(tensor1, tensor2):\n",
    "    # Pad tensor2 to match the size of tensor1\n",
    "    height_diff = tensor1.shape[0] - tensor2.shape[0]\n",
    "    width_diff = tensor1.shape[1] - tensor2.shape[1]\n",
    "\n",
    "    padded_tensor2 = F.pad(tensor2, (0, width_diff, 0, height_diff))\n",
    "\n",
    "    # Now, both tensors have the same size, and you can calculate cosine similarity as before\n",
    "    similarity = cosine_sim(tensor1, padded_tensor2)\n",
    "\n",
    "    return similarity\n",
    "# Example usage\n",
    "\n",
    "similarity = cosine_similarity(token_vecs, token_vecs2)\n",
    "print(\"Cosine Similarity:\", similarity.item())  # Convert to a Python float\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cosine module similarity:\", consine_module_similarity(token_vecs, token_vecs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor. \n",
    "token_embeddings = torch.stack(hidden_states, dim=0) # torch.Size([13, 1, 50, 768])\n",
    "\n",
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1) # torch.Size([13, 50, 768])\n",
    "\n",
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2) # torch.Size([50, 13, 768])\n",
    "\n",
    "    \n",
    "    # ## CONCATENATE the last four layers\n",
    "    # # Stores the token vectors, with shape [22 x 3,072]\n",
    "    # token_vecs_cat = []\n",
    "\n",
    "    # # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # # For each token in the sentence...\n",
    "    # for token in token_embeddings:\n",
    "        \n",
    "    #     # `token` is a [12 x 768] tensor\n",
    "\n",
    "    #     # Concatenate the vectors (that is, append them together) from the last \n",
    "    #     # four layers.\n",
    "    #     # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    #     cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "        \n",
    "    #     # Use `cat_vec` to represent `token`.\n",
    "    #     token_vecs_cat.append(cat_vec)  # Shape is: 50 x 3072\n",
    "\n",
    "    # print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0]))) \n",
    "    \n",
    "    # ## SUMMING LAST FOUR LAYERS\n",
    "    # # Stores the token vectors, with shape [22 x 768]\n",
    "    # token_vecs_sum = []\n",
    "\n",
    "    # # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # # For each token in the sentence...\n",
    "    # for token in token_embeddings:\n",
    "\n",
    "    #     # `token` is a [50 x 768] tensor\n",
    "\n",
    "    #     # Sum the vectors from the last four layers.\n",
    "    #     sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        \n",
    "    #     # Use `sum_vec` to represent `token`.\n",
    "    #     token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    # print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0]))) # Shape is: 50 x 768\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data('../data/coNLL_tsv/bert-base-uncased_prepared_data/ner_conll_testa_abolish.json')\n",
    "    \n",
    "print(\"data shape: \", len(data))  # 280\n",
    "tokens_id = data[0]['token_id']\n",
    "segments_id = data[0]['type_id']\n",
    "\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([tokens_id])\n",
    "segments_tensors = torch.tensor([segments_id])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('dmis-lab/biobert-base-cased-v1.2',\n",
    "                                output_hidden_states = True # Whether the model returns all hidden-states.\n",
    "                                )\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    print(\"outputs: \", len(outputs)) # 2\n",
    "    hidden_states = outputs[2]\n",
    "    \n",
    "    \n",
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")  #13\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i])) #1\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i])) #50\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))     # 768\n",
    "\n",
    "\n",
    "# `hidden_states` is a Python list.\n",
    "print('      Type of hidden_states: ', type(hidden_states)) # <class 'tuple'>\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', hidden_states[0].size()) # torch.Size([1, 50, 768])\n",
    "\n",
    "\n",
    "## Sentence Vectors\n",
    "# `hidden_states` has shape [13 x 1 x 50 x 768]\n",
    "\n",
    "# `token_vecs` is a tensor with shape [50 x 768]\n",
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size()) # torch.Size([768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_vecs_cat:  2\n",
      "['22110', '22111', '22112', '22113', '22114', '22115', '22116', '22117', '22118', '22119', '22120', '22121', '22122', '22123', '22124', '22125', '22126', '22127', '22128', '22129', '22130', '22131', '22132', '22133', '22134', '22135', '22136', '22137', '22138', '22139', '22140', '22141']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('models/outputs.pkl', 'rb') as f:\n",
    "    token_vecs_cat = pickle.load(f)\n",
    "\n",
    "print(\"token_vecs_cat: \", len(token_vecs_cat)) # 50\n",
    "print(token_vecs_cat['uid']) # (3072,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
