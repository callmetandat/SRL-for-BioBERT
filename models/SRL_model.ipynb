{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../data_transformations.py --transform_file 'transform_file_conll.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-07 11:03:28.315531: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-07 11:03:28.980003: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-07 11:03:30.934813: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Current Directory:  /mnt/c/Users/Phat Pham/Documents/THESIS/SRL-for-BioBERT/models../data/coNLL_tsv\n",
      "task object created from task file...\n",
      "bert model tokenizer loaded for config dmis-lab/biobert-base-cased-v1.2\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_train.tsv\n",
      "Processing Started...\n",
      "Data Size:  41740\n",
      "number of threads:  7\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  1%|▎                                       | 44/5962 [00:00<00:13, 428.27it/s]/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  1%|▏                                       | 34/5962 [00:00<00:18, 321.59it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  0%|▏                                       | 21/5962 [00:00<00:28, 206.97it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                        | 17/5962 [00:00<00:35, 168.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/5962 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                          | 9/5962 [00:00<01:06, 89.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                        | 14/5962 [00:00<00:44, 134.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  1%|▎                                       | 42/5962 [00:00<00:33, 174.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|▎                                       | 40/5962 [00:00<00:29, 204.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|▏                                       | 21/5962 [00:00<00:28, 207.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  1%|▌                                       | 87/5962 [00:00<00:22, 256.50it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|▏                                       | 24/5962 [00:00<00:48, 123.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▏                                       | 36/5962 [00:00<00:33, 176.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  1%|▍                                       | 71/5962 [00:00<00:26, 220.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|▍                                       | 73/5962 [00:00<00:22, 261.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▎                                       | 52/5962 [00:00<00:22, 265.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  2%|▋                                       | 95/5962 [00:00<00:24, 239.27it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|▎                                       | 43/5962 [00:00<00:39, 151.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▊                                      | 117/5962 [00:00<00:25, 229.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  2%|▋                                       | 98/5962 [00:00<00:24, 237.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|▋                                      | 100/5962 [00:00<00:22, 259.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▌                                       | 79/5962 [00:00<00:23, 255.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|▍                                       | 59/5962 [00:00<00:38, 152.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  2%|▊                                      | 121/5962 [00:00<00:25, 228.14it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▋                                      | 101/5962 [00:00<00:23, 246.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|▉                                      | 142/5962 [00:00<00:27, 209.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|▌                                       | 79/5962 [00:00<00:35, 164.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  2%|▊                                      | 123/5962 [00:00<00:34, 166.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▊                                      | 126/5962 [00:00<00:27, 213.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▋                                      | 105/5962 [00:00<00:34, 168.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▋                                       | 96/5962 [00:00<00:40, 145.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|█                                      | 155/5962 [00:00<00:29, 198.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  3%|█                                      | 164/5962 [00:00<00:35, 165.10it/s]\u001b[A\u001b[A\n",
      "  2%|▉                                      | 145/5962 [00:00<00:40, 143.78it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█                                      | 153/5962 [00:00<00:26, 222.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▋                                      | 114/5962 [00:00<00:37, 154.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|█▏                                     | 179/5962 [00:00<00:27, 208.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▏                                     | 182/5962 [00:00<00:35, 163.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  3%|█                                      | 166/5962 [00:00<00:32, 175.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▏                                     | 176/5962 [00:00<00:25, 223.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  3%|█                                      | 164/5962 [00:00<00:39, 147.16it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▉                                      | 135/5962 [00:00<00:34, 170.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|█▎                                     | 208/5962 [00:00<00:25, 229.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▎                                     | 202/5962 [00:01<00:33, 172.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  3%|█▏                                     | 186/5962 [00:01<00:32, 177.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▎                                     | 199/5962 [00:00<00:26, 217.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  3%|█▏                                     | 182/5962 [00:01<00:39, 146.58it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|█                                      | 153/5962 [00:00<00:33, 171.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▌                                     | 237/5962 [00:01<00:23, 241.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▍                                     | 225/5962 [00:01<00:31, 182.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  3%|█▎                                     | 207/5962 [00:01<00:30, 185.69it/s]\u001b[A\u001b[A\n",
      "  4%|█▍                                     | 214/5962 [00:01<00:31, 185.00it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▍                                     | 221/5962 [00:01<00:28, 198.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▏                                     | 172/5962 [00:01<00:32, 175.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|█▊                                     | 269/5962 [00:01<00:21, 263.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▌                                     | 245/5962 [00:01<00:30, 186.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  4%|█▌                                     | 231/5962 [00:01<00:28, 199.45it/s]\u001b[A\u001b[A\n",
      "  4%|█▌                                     | 246/5962 [00:01<00:26, 217.03it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▎                                     | 193/5962 [00:01<00:31, 183.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▌                                     | 242/5962 [00:01<00:29, 195.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|█▉                                     | 301/5962 [00:01<00:20, 277.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▊                                     | 270/5962 [00:01<00:28, 202.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  4%|█▋                                     | 261/5962 [00:01<00:25, 226.00it/s]\u001b[A\u001b[A\n",
      "  5%|█▊                                     | 282/5962 [00:01<00:22, 254.01it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▋                                     | 262/5962 [00:01<00:29, 193.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▍                                     | 212/5962 [00:01<00:33, 173.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▍                                     | 228/5962 [00:01<00:30, 187.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|██▏                                    | 330/5962 [00:01<00:21, 267.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  5%|█▉                                     | 291/5962 [00:01<00:30, 188.38it/s]\u001b[A\u001b[A\n",
      "  5%|██                                     | 310/5962 [00:01<00:21, 258.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▉                                     | 296/5962 [00:01<00:24, 233.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▌                                     | 232/5962 [00:01<00:32, 179.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|██▎                                    | 358/5962 [00:01<00:20, 268.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▌                                     | 248/5962 [00:01<00:31, 183.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  5%|██▏                                    | 325/5962 [00:01<00:24, 228.17it/s]\u001b[A\u001b[A\n",
      "  6%|██▏                                    | 338/5962 [00:01<00:21, 259.23it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|██                                     | 322/5962 [00:01<00:23, 239.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▋                                     | 259/5962 [00:01<00:27, 203.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  6%|██▌                                    | 387/5962 [00:01<00:20, 272.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▋                                     | 267/5962 [00:01<00:31, 182.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  6%|██▍                                    | 366/5962 [00:01<00:20, 278.09it/s]\u001b[A\u001b[A\n",
      "  6%|██▍                                    | 368/5962 [00:01<00:20, 268.26it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▎                                    | 351/5962 [00:01<00:22, 252.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▉                                     | 288/5962 [00:01<00:24, 228.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|██▊                                    | 425/5962 [00:01<00:18, 301.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|██                                     | 311/5962 [00:01<00:22, 253.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  7%|██▌                                    | 399/5962 [00:01<00:19, 291.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▌                                    | 385/5962 [00:01<00:20, 276.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|██                                     | 319/5962 [00:01<00:22, 252.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███                                    | 466/5962 [00:01<00:16, 332.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▎                                    | 352/5962 [00:01<00:18, 298.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  7%|██▌                                    | 396/5962 [00:01<00:23, 238.54it/s]\u001b[A\n",
      "\n",
      "  7%|██▉                                    | 445/5962 [00:01<00:16, 337.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▊                                    | 424/5962 [00:01<00:18, 305.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▎                                    | 349/5962 [00:01<00:21, 264.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  9%|███▍                                   | 521/5962 [00:01<00:13, 395.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "  7%|██▊                                    | 435/5962 [00:01<00:19, 277.41it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▏                                   | 480/5962 [00:02<00:16, 327.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|██▉                                    | 458/5962 [00:01<00:17, 315.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  8%|███                                    | 465/5962 [00:01<00:19, 286.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  9%|███▋                                   | 561/5962 [00:01<00:13, 390.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▍                                    | 376/5962 [00:01<00:21, 254.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  8%|███                                    | 469/5962 [00:02<00:19, 286.19it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▎                                   | 514/5962 [00:02<00:18, 302.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▋                                    | 402/5962 [00:02<00:24, 225.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|███▉                                   | 601/5962 [00:02<00:16, 328.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▏                                   | 490/5962 [00:02<00:22, 244.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▋                                   | 555/5962 [00:02<00:16, 329.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  8%|███▎                                   | 499/5962 [00:02<00:22, 238.56it/s]\u001b[A\n",
      "\n",
      "  8%|███▏                                   | 495/5962 [00:02<00:25, 216.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|████▏                                  | 636/5962 [00:02<00:16, 332.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▍                                   | 522/5962 [00:02<00:20, 261.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▊                                    | 426/5962 [00:02<00:26, 211.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▊                                   | 589/5962 [00:02<00:16, 322.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  9%|███▍                                   | 525/5962 [00:02<00:24, 218.06it/s]\u001b[A\n",
      "\n",
      "  9%|███▍                                   | 520/5962 [00:02<00:25, 209.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|████▍                                  | 671/5962 [00:02<00:16, 329.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▎                                   | 497/5962 [00:02<00:21, 258.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|████                                   | 622/5962 [00:02<00:18, 296.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|██▉                                    | 448/5962 [00:02<00:31, 176.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|████▋                                  | 710/5962 [00:02<00:15, 345.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▍                                   | 527/5962 [00:02<00:20, 268.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▊                                   | 583/5962 [00:02<00:20, 261.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  9%|███▌                                   | 549/5962 [00:02<00:28, 187.42it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▏                                   | 489/5962 [00:02<00:23, 229.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 10%|███▊                                   | 583/5962 [00:02<00:21, 250.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|████▉                                  | 754/5962 [00:02<00:14, 371.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▋                                   | 558/5962 [00:02<00:19, 279.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▉                                   | 611/5962 [00:02<00:21, 244.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 12%|████▌                                  | 698/5962 [00:02<00:16, 323.10it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▎                                   | 515/5962 [00:02<00:23, 235.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|█████▏                                 | 792/5962 [00:02<00:13, 373.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▊                                   | 590/5962 [00:02<00:18, 289.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▊                                  | 742/5962 [00:02<00:14, 354.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 10%|███▉                                   | 611/5962 [00:02<00:23, 229.52it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▌                                   | 541/5962 [00:02<00:22, 240.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 11%|████▎                                  | 663/5962 [00:02<00:17, 307.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|█████▍                                 | 830/5962 [00:02<00:14, 353.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|████                                   | 620/5962 [00:02<00:18, 290.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▍                                  | 678/5962 [00:02<00:18, 279.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 11%|████▏                                  | 649/5962 [00:02<00:20, 265.46it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████                                  | 779/5962 [00:02<00:15, 344.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 12%|████▌                                  | 696/5962 [00:02<00:17, 295.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█████▋                                 | 866/5962 [00:02<00:15, 335.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▎                                  | 650/5962 [00:02<00:20, 262.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 11%|████▍                                  | 684/5962 [00:02<00:18, 286.69it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▌                                  | 707/5962 [00:02<00:19, 274.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▎                                 | 815/5962 [00:03<00:15, 326.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█████▉                                 | 902/5962 [00:02<00:14, 341.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▍                                  | 678/5962 [00:02<00:19, 266.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 12%|████▋                                  | 718/5962 [00:03<00:17, 295.77it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▌                                 | 849/5962 [00:03<00:16, 315.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▌                                  | 707/5962 [00:02<00:19, 269.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 13%|████▉                                  | 755/5962 [00:03<00:20, 253.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▏                                  | 633/5962 [00:03<00:24, 216.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|██████▏                                | 937/5962 [00:03<00:17, 291.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████                                  | 765/5962 [00:03<00:18, 274.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 15%|█████▊                                 | 882/5962 [00:03<00:16, 312.66it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▊                                  | 737/5962 [00:03<00:18, 275.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 13%|█████▏                                 | 786/5962 [00:03<00:19, 267.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████▏                                 | 793/5962 [00:03<00:19, 267.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▎                                  | 657/5962 [00:03<00:25, 208.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|██████                                 | 922/5962 [00:03<00:15, 334.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████                                  | 768/5962 [00:03<00:18, 284.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 13%|█████                                  | 778/5962 [00:03<00:21, 241.53it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▌                                  | 691/5962 [00:03<00:22, 239.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▎                               | 1000/5962 [00:03<00:17, 291.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▎                                | 956/5962 [00:03<00:14, 335.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▎                                 | 821/5962 [00:03<00:14, 351.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 14%|█████▎                                 | 810/5962 [00:03<00:19, 260.34it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▋                                  | 721/5962 [00:03<00:20, 254.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▌                                 | 847/5962 [00:03<00:19, 256.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|██████▍                                | 990/5962 [00:03<00:15, 314.99it/s]\u001b[A\u001b[A\u001b[A\n",
      " 14%|█████▍                                 | 838/5962 [00:03<00:19, 264.74it/s]\u001b[A\n",
      "\n",
      " 14%|█████▍                                 | 840/5962 [00:03<00:23, 214.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▌                                 | 857/5962 [00:03<00:16, 313.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 13%|████▉                                  | 748/5962 [00:03<00:21, 244.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|██████▊                               | 1060/5962 [00:03<00:17, 278.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▌                               | 1022/5962 [00:03<00:16, 304.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 14%|█████▋                                 | 863/5962 [00:03<00:25, 202.77it/s]\u001b[A\u001b[A\n",
      " 15%|█████▋                                 | 866/5962 [00:03<00:20, 244.31it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█████▊                                 | 890/5962 [00:03<00:16, 302.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█████▊                                 | 898/5962 [00:03<00:20, 242.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████                                  | 774/5962 [00:03<00:22, 234.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|██████▋                               | 1057/5962 [00:03<00:15, 309.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 15%|█████▊                                 | 885/5962 [00:03<00:24, 204.51it/s]\u001b[A\u001b[A\n",
      " 15%|█████▊                                 | 893/5962 [00:03<00:20, 250.88it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|██████                                 | 921/5962 [00:03<00:17, 282.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████▏                                 | 799/5962 [00:03<00:23, 224.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|██████                                 | 923/5962 [00:03<00:22, 225.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|██████▉                               | 1089/5962 [00:03<00:17, 285.28it/s]\u001b[A\u001b[A\u001b[A\n",
      " 16%|██████                                 | 927/5962 [00:03<00:18, 266.36it/s]\u001b[A\n",
      "\n",
      " 15%|█████▉                                 | 907/5962 [00:03<00:25, 195.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▏                              | 1118/5962 [00:04<00:17, 281.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 16%|██████                                 | 927/5962 [00:04<00:27, 182.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|███████▎                              | 1140/5962 [00:04<00:23, 206.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▏                                | 950/5962 [00:03<00:23, 209.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 16%|██████▏                                | 955/5962 [00:04<00:22, 224.99it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▎                              | 1152/5962 [00:04<00:16, 295.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 16%|██████▏                                | 947/5962 [00:04<00:26, 185.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|███████▍                              | 1166/5962 [00:04<00:22, 215.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▎                                | 974/5962 [00:04<00:24, 206.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 16%|██████▍                                | 979/5962 [00:04<00:23, 213.06it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▌                                 | 844/5962 [00:04<00:32, 158.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▌                              | 1182/5962 [00:04<00:17, 273.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 16%|██████▎                                | 972/5962 [00:04<00:24, 201.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|███████▌                              | 1192/5962 [00:04<00:21, 225.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▌                                | 997/5962 [00:04<00:23, 207.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 17%|██████▍                               | 1008/5962 [00:04<00:21, 229.60it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|█████▋                                 | 866/5962 [00:04<00:29, 170.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▋                              | 1213/5962 [00:04<00:17, 278.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 17%|██████▌                                | 998/5962 [00:04<00:23, 215.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|███████▊                              | 1221/5962 [00:04<00:19, 241.47it/s]\u001b[A\u001b[A\u001b[A\n",
      " 17%|██████▌                               | 1037/5962 [00:04<00:20, 244.33it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▌                               | 1020/5962 [00:04<00:23, 208.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|███████▉                              | 1245/5962 [00:04<00:16, 283.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 17%|██████▌                               | 1029/5962 [00:04<00:20, 240.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|███████▉                              | 1246/5962 [00:04<00:19, 240.47it/s]\u001b[A\u001b[A\u001b[A\n",
      " 18%|██████▊                               | 1066/5962 [00:04<00:19, 256.16it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████▋                               | 1046/5962 [00:04<00:22, 218.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████                              | 1274/5962 [00:04<00:17, 271.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▋                               | 1041/5962 [00:04<00:27, 181.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 18%|██████▋                               | 1054/5962 [00:04<00:21, 226.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████▊                               | 1071/5962 [00:04<00:22, 220.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 18%|██████▉                               | 1093/5962 [00:04<00:22, 220.83it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▎                             | 1302/5962 [00:04<00:19, 242.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████▊                               | 1060/5962 [00:04<00:29, 163.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|████████▎                             | 1296/5962 [00:04<00:22, 208.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████▉                               | 1095/5962 [00:04<00:21, 225.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|███████                               | 1117/5962 [00:04<00:22, 215.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▍                             | 1327/5962 [00:04<00:19, 243.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████▉                               | 1088/5962 [00:04<00:25, 189.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|████████▍                             | 1322/5962 [00:04<00:20, 221.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▎                              | 1140/5962 [00:04<00:16, 284.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 19%|███████▎                              | 1142/5962 [00:04<00:21, 223.76it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|████████▌                             | 1353/5962 [00:04<00:18, 247.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 18%|██████▉                               | 1098/5962 [00:04<00:28, 168.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████                               | 1109/5962 [00:04<00:25, 193.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▍                              | 1170/5962 [00:04<00:16, 286.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|████████▌                             | 1345/5962 [00:04<00:23, 197.13it/s]\u001b[A\u001b[A\u001b[A\n",
      " 20%|███████▍                              | 1166/5962 [00:05<00:21, 226.65it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|████████▊                             | 1379/5962 [00:05<00:18, 243.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▎                              | 1139/5962 [00:04<00:21, 221.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 19%|███████                               | 1117/5962 [00:05<00:28, 170.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▋                              | 1201/5962 [00:04<00:16, 290.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|████████▋                             | 1366/5962 [00:05<00:23, 193.23it/s]\u001b[A\u001b[A\u001b[A\n",
      " 20%|███████▌                              | 1190/5962 [00:05<00:20, 229.20it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|████████▉                             | 1409/5962 [00:05<00:17, 259.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▍                              | 1170/5962 [00:04<00:19, 243.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 19%|███████▎                              | 1146/5962 [00:05<00:24, 199.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|███████▊                              | 1231/5962 [00:05<00:16, 284.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 21%|███████▊                              | 1226/5962 [00:05<00:17, 265.27it/s]\u001b[A\n",
      "\n",
      "\n",
      " 23%|████████▊                             | 1386/5962 [00:05<00:24, 190.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▍                               | 1018/5962 [00:05<00:28, 174.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 24%|█████████▏                            | 1436/5962 [00:05<00:17, 255.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▌                              | 1196/5962 [00:05<00:20, 232.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████                              | 1260/5962 [00:05<00:17, 268.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 21%|████████                              | 1258/5962 [00:05<00:16, 279.90it/s]\u001b[A\n",
      "\n",
      "\n",
      " 24%|████████▉                             | 1408/5962 [00:05<00:23, 197.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▋                               | 1043/5962 [00:05<00:25, 193.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 25%|█████████▎                            | 1462/5962 [00:05<00:17, 256.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|███████▊                              | 1224/5962 [00:05<00:19, 243.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▎                             | 1304/5962 [00:05<00:14, 311.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 22%|████████▎                             | 1299/5962 [00:05<00:14, 314.34it/s]\u001b[A\n",
      "\n",
      "\n",
      " 24%|█████████▏                            | 1434/5962 [00:05<00:21, 213.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▍                            | 1488/5962 [00:05<00:18, 244.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 21%|███████▊                              | 1229/5962 [00:05<00:20, 228.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████                              | 1260/5962 [00:05<00:17, 275.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|█████████▎                            | 1457/5962 [00:05<00:20, 214.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▌                             | 1336/5962 [00:05<00:16, 288.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████▉                               | 1088/5962 [00:05<00:24, 198.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 22%|████████▍                             | 1331/5962 [00:05<00:16, 275.01it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▏                             | 1290/5962 [00:05<00:16, 281.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 25%|█████████▋                            | 1513/5962 [00:05<00:19, 230.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|█████████▍                            | 1479/5962 [00:05<00:21, 208.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|████████▋                             | 1366/5962 [00:05<00:16, 284.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 23%|████████▋                             | 1363/5962 [00:05<00:16, 284.48it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████                               | 1109/5962 [00:05<00:24, 194.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 21%|████████▏                             | 1278/5962 [00:05<00:20, 233.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|█████████▊                            | 1539/5962 [00:05<00:18, 237.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|█████████▌                            | 1501/5962 [00:05<00:21, 204.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▏                              | 1129/5962 [00:05<00:25, 193.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 22%|████████▎                             | 1305/5962 [00:05<00:19, 242.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████                            | 1572/5962 [00:05<00:16, 262.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 23%|████████▉                             | 1393/5962 [00:05<00:16, 269.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|████████▌                             | 1347/5962 [00:05<00:18, 256.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|█████████▋                            | 1525/5962 [00:05<00:20, 211.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▎                              | 1151/5962 [00:05<00:24, 199.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████                             | 1424/5962 [00:05<00:17, 256.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 27%|██████████▏                           | 1599/5962 [00:05<00:17, 255.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|████████▊                             | 1374/5962 [00:05<00:18, 243.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 24%|█████████                             | 1421/5962 [00:05<00:18, 243.79it/s]\u001b[A\n",
      "\n",
      "\n",
      " 26%|█████████▊                            | 1548/5962 [00:05<00:20, 216.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▍                              | 1172/5962 [00:05<00:24, 191.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 23%|████████▋                             | 1354/5962 [00:06<00:19, 232.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▏                            | 1451/5962 [00:05<00:18, 239.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▎                           | 1625/5962 [00:06<00:18, 234.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 24%|█████████▏                            | 1447/5962 [00:06<00:18, 242.24it/s]\u001b[A\n",
      "\n",
      "\n",
      " 26%|██████████                            | 1573/5962 [00:06<00:19, 224.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▌                              | 1196/5962 [00:06<00:23, 204.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 28%|██████████▌                           | 1649/5962 [00:06<00:18, 234.99it/s]\u001b[A\u001b[A\n",
      " 25%|█████████▍                            | 1474/5962 [00:06<00:18, 249.18it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▍                            | 1476/5962 [00:06<00:19, 227.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|██████████▏                           | 1601/5962 [00:06<00:18, 240.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▏                            | 1441/5962 [00:06<00:18, 241.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▊                              | 1217/5962 [00:06<00:23, 199.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 28%|██████████▋                           | 1674/5962 [00:06<00:17, 238.52it/s]\u001b[A\u001b[A\n",
      " 25%|█████████▌                            | 1505/5962 [00:06<00:16, 264.33it/s]\u001b[A\n",
      "\n",
      "\n",
      " 27%|██████████▍                           | 1628/5962 [00:06<00:17, 246.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▌                            | 1500/5962 [00:06<00:20, 219.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▎                            | 1468/5962 [00:06<00:18, 245.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|███████▉                              | 1238/5962 [00:06<00:24, 196.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 24%|█████████▏                            | 1434/5962 [00:06<00:19, 227.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██████████▊                           | 1699/5962 [00:06<00:18, 228.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|█████████▊                            | 1532/5962 [00:06<00:17, 250.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▌                            | 1505/5962 [00:06<00:16, 277.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████                              | 1262/5962 [00:06<00:22, 206.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 24%|█████████▎                            | 1458/5962 [00:06<00:19, 226.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|██████████▉                           | 1723/5962 [00:06<00:18, 226.74it/s]\u001b[A\u001b[A\u001b[A\n",
      " 26%|█████████▉                            | 1567/5962 [00:06<00:16, 272.86it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|█████████▊                            | 1547/5962 [00:06<00:20, 216.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|█████████▊                            | 1539/5962 [00:06<00:15, 293.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▏                             | 1283/5962 [00:06<00:23, 194.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 29%|███████████▏                          | 1757/5962 [00:06<00:16, 257.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|███████████                           | 1729/5962 [00:06<00:14, 294.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████                            | 1574/5962 [00:06<00:19, 230.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 27%|██████████▏                           | 1595/5962 [00:06<00:16, 262.26it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████                            | 1570/5962 [00:06<00:14, 295.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▍                             | 1316/5962 [00:06<00:20, 231.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 25%|█████████▌                            | 1504/5962 [00:06<00:20, 217.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███████████▍                          | 1789/5962 [00:06<00:15, 274.41it/s]\u001b[A\u001b[A\u001b[A\n",
      " 27%|██████████▍                           | 1632/5962 [00:06<00:14, 290.11it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▏                           | 1601/5962 [00:06<00:14, 295.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▌                          | 1817/5962 [00:06<00:15, 271.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▌                             | 1340/5962 [00:06<00:21, 213.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 26%|█████████▋                            | 1526/5962 [00:06<00:22, 200.20it/s]\u001b[A\u001b[A\n",
      " 28%|██████████▌                           | 1667/5962 [00:06<00:14, 303.09it/s]\u001b[A\n",
      "\n",
      "\n",
      " 30%|███████████▍                          | 1802/5962 [00:06<00:14, 284.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▍                           | 1631/5962 [00:06<00:15, 286.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▎                           | 1619/5962 [00:06<00:22, 192.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▊                          | 1845/5962 [00:06<00:15, 263.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 26%|█████████▊                            | 1547/5962 [00:06<00:22, 200.08it/s]\u001b[A\u001b[A\n",
      " 28%|██████████▊                           | 1699/5962 [00:06<00:13, 305.51it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▌                           | 1661/5962 [00:06<00:15, 272.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|███████████▋                          | 1832/5962 [00:06<00:15, 263.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▍                           | 1642/5962 [00:06<00:21, 198.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▉                          | 1875/5962 [00:07<00:15, 271.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 27%|██████████                            | 1580/5962 [00:07<00:18, 234.37it/s]\u001b[A\u001b[A\n",
      " 29%|███████████                           | 1732/5962 [00:07<00:13, 310.87it/s]\u001b[A\n",
      "\n",
      "\n",
      " 31%|███████████▊                          | 1860/5962 [00:07<00:15, 257.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▋                           | 1668/5962 [00:06<00:20, 213.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▏                         | 1904/5962 [00:07<00:14, 276.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|████████▉                             | 1410/5962 [00:07<00:21, 213.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 30%|███████████▏                          | 1764/5962 [00:07<00:13, 307.53it/s]\u001b[A\n",
      "\n",
      " 27%|██████████▏                           | 1604/5962 [00:07<00:19, 219.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▊                           | 1694/5962 [00:07<00:18, 225.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|████████████▎                         | 1940/5962 [00:07<00:13, 299.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▏                            | 1436/5962 [00:07<00:20, 225.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██████████▉                           | 1715/5962 [00:07<00:18, 228.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 30%|███████████▍                          | 1795/5962 [00:07<00:14, 295.87it/s]\u001b[A\n",
      "\n",
      " 27%|██████████▎                           | 1627/5962 [00:07<00:19, 219.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|████████████▎                         | 1924/5962 [00:07<00:14, 282.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▌                         | 1973/5962 [00:07<00:12, 308.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▎                            | 1459/5962 [00:07<00:20, 220.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████▏                          | 1747/5962 [00:07<00:16, 250.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 31%|███████████▋                          | 1826/5962 [00:07<00:13, 299.27it/s]\u001b[A\n",
      "\n",
      " 28%|██████████▌                           | 1650/5962 [00:07<00:20, 215.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|████████████▌                         | 1967/5962 [00:07<00:12, 318.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|████████████▊                         | 2006/5962 [00:07<00:12, 312.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▍                          | 1785/5962 [00:07<00:14, 284.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 31%|███████████▊                          | 1857/5962 [00:07<00:14, 284.74it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▍                            | 1482/5962 [00:07<00:22, 199.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 28%|██████████▋                           | 1672/5962 [00:07<00:21, 201.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|████████████▉                         | 2038/5962 [00:07<00:13, 301.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▌                          | 1823/5962 [00:07<00:13, 308.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 32%|████████████                          | 1886/5962 [00:07<00:14, 285.73it/s]\u001b[A\n",
      "\n",
      " 28%|██████████▊                           | 1694/5962 [00:07<00:20, 206.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▍                          | 1803/5962 [00:07<00:17, 241.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|████████████▋                         | 2000/5962 [00:07<00:18, 215.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▏                        | 2069/5962 [00:07<00:16, 240.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 29%|██████████▉                           | 1715/5962 [00:07<00:21, 194.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▊                          | 1855/5962 [00:07<00:17, 234.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 32%|████████████▏                         | 1915/5962 [00:07<00:20, 200.97it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▋                          | 1828/5962 [00:07<00:21, 194.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 35%|█████████████▎                        | 2096/5962 [00:07<00:15, 242.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▋                            | 1520/5962 [00:07<00:31, 141.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|████████████▉                         | 2027/5962 [00:07<00:21, 181.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|███████████▉                          | 1882/5962 [00:07<00:18, 221.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▊                          | 1854/5962 [00:07<00:19, 209.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 36%|█████████████▌                        | 2122/5962 [00:07<00:15, 246.98it/s]\u001b[A\u001b[A\n",
      " 33%|████████████▎                         | 1939/5962 [00:07<00:20, 197.86it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|█████████▊                            | 1536/5962 [00:07<00:30, 145.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▏                         | 1909/5962 [00:07<00:17, 232.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|█████████████                         | 2050/5962 [00:07<00:21, 181.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 36%|█████████████▊                        | 2158/5962 [00:08<00:13, 275.24it/s]\u001b[A\u001b[A\n",
      " 33%|████████████▌                         | 1977/5962 [00:08<00:16, 239.68it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████                            | 1578/5962 [00:07<00:20, 210.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▉                          | 1877/5962 [00:07<00:21, 191.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▎                         | 1941/5962 [00:07<00:15, 252.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|█████████████▏                        | 2071/5962 [00:08<00:20, 187.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 37%|█████████████▉                        | 2187/5962 [00:08<00:13, 269.81it/s]\u001b[A\u001b[A\n",
      " 34%|████████████▊                         | 2013/5962 [00:08<00:14, 267.22it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▏                           | 1602/5962 [00:08<00:20, 213.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████                          | 1898/5962 [00:08<00:22, 179.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▌                         | 1970/5962 [00:08<00:15, 262.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|█████████████▎                        | 2092/5962 [00:08<00:20, 191.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 31%|███████████▊                          | 1848/5962 [00:08<00:16, 253.26it/s]\u001b[A\u001b[A\n",
      " 37%|██████████████                        | 2215/5962 [00:08<00:14, 261.41it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▎                         | 1931/5962 [00:08<00:18, 213.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|████████████▊                         | 2005/5962 [00:08<00:13, 283.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▎                           | 1625/5962 [00:08<00:23, 188.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|█████████████▍                        | 2117/5962 [00:08<00:18, 205.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 31%|███████████▉                          | 1874/5962 [00:08<00:16, 247.85it/s]\u001b[A\u001b[A\n",
      " 38%|██████████████▎                       | 2242/5962 [00:08<00:14, 248.40it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▍                         | 1955/5962 [00:08<00:18, 219.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|████████████▉                         | 2035/5962 [00:08<00:13, 283.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|█████████████▋                        | 2149/5962 [00:08<00:16, 233.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 32%|████████████                          | 1899/5962 [00:08<00:17, 230.20it/s]\u001b[A\u001b[A\n",
      " 35%|█████████████▍                        | 2113/5962 [00:08<00:13, 287.11it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▍                       | 2268/5962 [00:08<00:15, 236.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▌                         | 1978/5962 [00:08<00:18, 219.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|█████████████▉                        | 2178/5962 [00:08<00:15, 248.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 32%|████████████▎                         | 1924/5962 [00:08<00:17, 233.68it/s]\u001b[A\u001b[A\n",
      " 38%|██████████████▌                       | 2293/5962 [00:08<00:15, 239.54it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|████████████▊                         | 2010/5962 [00:08<00:16, 246.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▏                        | 2065/5962 [00:08<00:17, 221.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|██████████████                        | 2206/5962 [00:08<00:14, 255.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▌                           | 1664/5962 [00:08<00:28, 153.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 36%|█████████████▊                        | 2175/5962 [00:08<00:13, 287.70it/s]\u001b[A\n",
      "\n",
      " 39%|██████████████▊                       | 2318/5962 [00:08<00:15, 233.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|██████████████▎                       | 2238/5962 [00:08<00:13, 271.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|████████████▉                         | 2036/5962 [00:08<00:16, 232.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▋                           | 1681/5962 [00:08<00:30, 140.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▎                        | 2090/5962 [00:08<00:20, 188.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 33%|████████████▌                         | 1971/5962 [00:08<00:19, 205.12it/s]\u001b[A\u001b[A\n",
      " 37%|██████████████                        | 2205/5962 [00:08<00:14, 258.84it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▏                        | 2076/5962 [00:08<00:14, 277.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|██████████████▉                       | 2342/5962 [00:08<00:17, 207.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▊                           | 1696/5962 [00:08<00:30, 137.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 34%|████████████▊                         | 2001/5962 [00:08<00:17, 229.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▍                        | 2112/5962 [00:08<00:21, 182.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 37%|██████████████▏                       | 2232/5962 [00:08<00:15, 241.24it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▍                        | 2105/5962 [00:08<00:14, 264.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|███████████████                       | 2364/5962 [00:09<00:19, 186.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|██████████▉                           | 1711/5962 [00:08<00:34, 124.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████▌                        | 2132/5962 [00:08<00:22, 169.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|████████████▉                         | 2025/5962 [00:09<00:20, 190.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████▌                        | 2133/5962 [00:09<00:16, 236.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|██████████████▊                       | 2316/5962 [00:09<00:17, 212.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▏                      | 2384/5962 [00:09<00:21, 168.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████▋                        | 2156/5962 [00:09<00:20, 184.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 38%|██████████████▌                       | 2282/5962 [00:09<00:16, 226.87it/s]\u001b[A\n",
      "\n",
      " 35%|█████████████▏                        | 2060/5962 [00:09<00:17, 227.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████▊                        | 2160/5962 [00:09<00:15, 241.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|██████████████▉                       | 2338/5962 [00:09<00:17, 211.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████                           | 1738/5962 [00:09<00:33, 124.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▎                      | 2402/5962 [00:09<00:22, 159.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 39%|██████████████▋                       | 2309/5962 [00:09<00:15, 236.80it/s]\u001b[A\n",
      "\n",
      " 35%|█████████████▎                        | 2085/5962 [00:09<00:16, 230.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|█████████████▉                        | 2196/5962 [00:09<00:16, 230.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████▏                          | 1751/5962 [00:09<00:34, 120.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|███████████████                       | 2364/5962 [00:09<00:20, 179.75it/s]\u001b[A\u001b[A\u001b[A\n",
      " 39%|██████████████▉                       | 2334/5962 [00:09<00:18, 196.43it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▍                      | 2419/5962 [00:09<00:28, 126.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████████▏                       | 2220/5962 [00:09<00:17, 208.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 35%|█████████████▍                        | 2110/5962 [00:09<00:22, 173.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|███████████████▏                      | 2384/5962 [00:09<00:20, 172.73it/s]\u001b[A\u001b[A\u001b[A\n",
      " 40%|███████████████                       | 2359/5962 [00:09<00:17, 208.60it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▌                      | 2433/5962 [00:09<00:27, 128.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▍                          | 1787/5962 [00:09<00:29, 139.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▎                       | 2252/5962 [00:09<00:15, 234.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 36%|█████████████▌                        | 2137/5962 [00:09<00:19, 193.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|███████████████▎                      | 2404/5962 [00:09<00:19, 178.78it/s]\u001b[A\u001b[A\u001b[A\n",
      " 41%|███████████████▌                      | 2450/5962 [00:09<00:25, 137.10it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▎                       | 2254/5962 [00:09<00:20, 181.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▌                          | 1805/5962 [00:09<00:27, 149.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 36%|█████████████▊                        | 2160/5962 [00:09<00:19, 197.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|███████████████▌                      | 2436/5962 [00:09<00:16, 213.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▌                       | 2277/5962 [00:09<00:17, 214.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 41%|███████████████▋                      | 2465/5962 [00:09<00:24, 140.07it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▌                       | 2281/5962 [00:09<00:18, 202.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▋                          | 1825/5962 [00:09<00:25, 162.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 37%|█████████████▉                        | 2187/5962 [00:09<00:17, 215.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|███████████████▋                      | 2461/5962 [00:09<00:15, 222.79it/s]\u001b[A\u001b[A\u001b[A\n",
      " 42%|███████████████▉                      | 2496/5962 [00:09<00:19, 181.10it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▋                       | 2305/5962 [00:09<00:17, 210.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▊                          | 1846/5962 [00:09<00:23, 174.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 37%|██████████████                        | 2216/5962 [00:09<00:16, 231.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|███████████████▊                      | 2485/5962 [00:09<00:15, 219.43it/s]\u001b[A\u001b[A\u001b[A\n",
      " 42%|████████████████                      | 2519/5962 [00:10<00:17, 192.74it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▊                       | 2320/5962 [00:09<00:19, 183.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 38%|██████████████▎                       | 2242/5962 [00:10<00:15, 239.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▊                       | 2328/5962 [00:09<00:18, 198.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▉                          | 1864/5962 [00:09<00:25, 161.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|███████████████▉                      | 2508/5962 [00:10<00:18, 188.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|███████████▉                          | 1882/5962 [00:10<00:25, 161.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 42%|███████████████▉                      | 2510/5962 [00:10<00:15, 223.12it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▉                       | 2339/5962 [00:10<00:21, 166.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▉                       | 2350/5962 [00:10<00:20, 178.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 43%|████████████████▍                     | 2574/5962 [00:10<00:17, 194.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████                          | 1899/5962 [00:10<00:25, 157.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 43%|████████████████▏                     | 2537/5962 [00:10<00:14, 233.37it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████                       | 2357/5962 [00:10<00:21, 168.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████████████████                      | 2528/5962 [00:10<00:21, 162.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▌                     | 2605/5962 [00:10<00:15, 223.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▎                         | 1924/5962 [00:10<00:22, 182.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 43%|████████████████▎                     | 2564/5962 [00:10<00:14, 242.67it/s]\u001b[A\n",
      "\n",
      "\n",
      " 43%|████████████████▎                     | 2556/5962 [00:10<00:18, 188.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▏                      | 2377/5962 [00:10<00:20, 172.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 38%|██████████████▌                       | 2288/5962 [00:10<00:21, 170.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▊                     | 2637/5962 [00:10<00:13, 249.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▍                         | 1945/5962 [00:10<00:21, 187.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 44%|████████████████▌                     | 2599/5962 [00:10<00:12, 268.54it/s]\u001b[A\n",
      "\n",
      "\n",
      " 43%|████████████████▍                     | 2581/5962 [00:10<00:16, 202.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▎                      | 2397/5962 [00:10<00:19, 178.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 39%|██████████████▋                       | 2313/5962 [00:10<00:19, 189.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████                     | 2668/5962 [00:10<00:12, 264.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▌                         | 1965/5962 [00:10<00:21, 189.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 44%|████████████████▊                     | 2630/5962 [00:10<00:12, 276.04it/s]\u001b[A\n",
      "\n",
      "\n",
      " 44%|████████████████▌                     | 2608/5962 [00:10<00:15, 219.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▍                      | 2417/5962 [00:10<00:19, 184.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 39%|██████████████▉                       | 2338/5962 [00:10<00:17, 203.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████▏                    | 2697/5962 [00:10<00:12, 269.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▋                         | 1991/5962 [00:10<00:19, 204.82it/s]\n",
      " 45%|████████████████▉                     | 2661/5962 [00:10<00:11, 282.67it/s]\u001b[A\n",
      "\n",
      " 40%|███████████████▏                      | 2375/5962 [00:10<00:14, 246.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|███████████████▉                      | 2505/5962 [00:10<00:11, 292.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▌                      | 2437/5962 [00:10<00:19, 180.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|█████████████████▎                    | 2725/5962 [00:10<00:12, 260.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|████████████▊                         | 2012/5962 [00:10<00:19, 204.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 40%|███████████████▎                      | 2402/5962 [00:10<00:14, 252.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▏                     | 2538/5962 [00:10<00:11, 301.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▋                      | 2456/5962 [00:10<00:19, 176.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 45%|█████████████████▏                    | 2690/5962 [00:10<00:12, 257.36it/s]\u001b[A\n",
      "\n",
      "\n",
      " 45%|████████████████▉                     | 2654/5962 [00:10<00:17, 194.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▌                    | 2752/5962 [00:10<00:12, 247.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▊                      | 2474/5962 [00:10<00:20, 173.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▎                     | 2569/5962 [00:10<00:11, 283.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|█████████████████                     | 2676/5962 [00:10<00:16, 199.34it/s]\u001b[A\u001b[A\u001b[A\n",
      " 46%|█████████████████▎                    | 2717/5962 [00:11<00:14, 229.79it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████                         | 2054/5962 [00:10<00:19, 197.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 47%|█████████████████▋                    | 2781/5962 [00:11<00:12, 259.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|███████████████▉                      | 2493/5962 [00:10<00:19, 176.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|█████████████████▏                    | 2697/5962 [00:11<00:16, 196.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▌                     | 2598/5962 [00:10<00:13, 242.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▏                        | 2077/5962 [00:11<00:18, 206.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▋                      | 2452/5962 [00:11<00:17, 195.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 47%|█████████████████▉                    | 2808/5962 [00:11<00:13, 237.96it/s]\u001b[A\n",
      "\n",
      "\n",
      " 46%|█████████████████▎                    | 2718/5962 [00:11<00:18, 176.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▎                        | 2098/5962 [00:11<00:19, 196.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▋                     | 2624/5962 [00:11<00:14, 229.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 41%|███████████████▊                      | 2474/5962 [00:11<00:17, 200.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▏                     | 2537/5962 [00:11<00:17, 194.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 48%|██████████████████                    | 2833/5962 [00:11<00:14, 218.94it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████▌                        | 2127/5962 [00:11<00:17, 221.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 42%|███████████████▉                      | 2497/5962 [00:11<00:16, 206.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▉                     | 2648/5962 [00:11<00:14, 227.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▍                     | 2572/5962 [00:11<00:14, 238.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|█████████████████▍                    | 2737/5962 [00:11<00:19, 169.09it/s]\u001b[A\u001b[A\u001b[A\n",
      " 48%|██████████████████▏                   | 2856/5962 [00:11<00:14, 217.96it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▌                     | 2600/5962 [00:11<00:13, 250.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 42%|████████████████                      | 2519/5962 [00:11<00:16, 206.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████▋                        | 2150/5962 [00:11<00:18, 211.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|█████████████████▌                    | 2757/5962 [00:11<00:18, 175.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████                     | 2672/5962 [00:11<00:14, 221.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 48%|██████████████████▎                   | 2879/5962 [00:11<00:14, 215.86it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▋                     | 2627/5962 [00:11<00:13, 256.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 43%|████████████████▏                     | 2541/5962 [00:11<00:16, 208.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|█████████████▉                        | 2177/5962 [00:11<00:16, 227.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|█████████████████▋                    | 2784/5962 [00:11<00:15, 199.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████▏                    | 2702/5962 [00:11<00:13, 241.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 49%|██████████████████▌                   | 2908/5962 [00:11<00:12, 235.69it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████████                        | 2204/5962 [00:11<00:15, 239.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 43%|████████████████▍                     | 2576/5962 [00:11<00:13, 242.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|█████████████████▉                    | 2819/5962 [00:11<00:13, 236.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▍                    | 2731/5962 [00:11<00:12, 251.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▉                     | 2653/5962 [00:11<00:13, 237.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 49%|██████████████████▋                   | 2935/5962 [00:11<00:12, 244.62it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▌                     | 2607/5962 [00:11<00:12, 261.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▌                    | 2757/5962 [00:11<00:12, 248.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|██████████████████▏                   | 2844/5962 [00:11<00:13, 227.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████████▊                   | 2961/5962 [00:11<00:12, 247.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 48%|██████████████████▍                   | 2890/5962 [00:11<00:13, 220.40it/s]\u001b[A\n",
      "\n",
      " 44%|████████████████▊                     | 2642/5962 [00:11<00:11, 283.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▋                    | 2783/5962 [00:11<00:13, 235.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▎                       | 2254/5962 [00:11<00:17, 214.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|██████████████████▎                   | 2868/5962 [00:11<00:16, 190.26it/s]\u001b[A\u001b[A\u001b[A\n",
      " 50%|███████████████████                   | 2986/5962 [00:12<00:15, 197.65it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▉                    | 2807/5962 [00:11<00:14, 218.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████▏                    | 2702/5962 [00:11<00:20, 161.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▌                       | 2277/5962 [00:12<00:19, 189.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 45%|█████████████████                     | 2671/5962 [00:12<00:14, 222.48it/s]\u001b[A\u001b[A\n",
      " 49%|██████████████████▋                   | 2933/5962 [00:12<00:16, 188.98it/s]\u001b[A\n",
      "\n",
      "\n",
      " 50%|███████████████████▏                  | 3008/5962 [00:12<00:16, 183.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▎                    | 2722/5962 [00:12<00:19, 169.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 45%|█████████████████▏                    | 2696/5962 [00:12<00:14, 228.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▋                       | 2304/5962 [00:12<00:17, 208.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 50%|██████████████████▉                   | 2962/5962 [00:12<00:14, 213.17it/s]\u001b[A\n",
      "\n",
      "\n",
      " 49%|██████████████████▌                   | 2908/5962 [00:12<00:18, 169.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▏                   | 2856/5962 [00:12<00:13, 223.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▎                  | 3028/5962 [00:12<00:16, 182.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 46%|█████████████████▍                    | 2740/5962 [00:12<00:11, 282.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▊                       | 2327/5962 [00:12<00:17, 213.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 50%|███████████████████                   | 2996/5962 [00:12<00:12, 244.02it/s]\u001b[A\n",
      "\n",
      "\n",
      " 49%|██████████████████▊                   | 2943/5962 [00:12<00:14, 212.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▍                  | 3048/5962 [00:12<00:15, 186.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▋                    | 2775/5962 [00:12<00:15, 210.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 47%|█████████████████▋                    | 2775/5962 [00:12<00:10, 299.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|███████████████                       | 2354/5962 [00:12<00:15, 226.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 51%|███████████████████▎                  | 3037/5962 [00:12<00:10, 289.20it/s]\u001b[A\n",
      "\n",
      "\n",
      " 50%|██████████████████▉                   | 2974/5962 [00:12<00:12, 237.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▌                   | 2914/5962 [00:12<00:12, 249.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▌                  | 3070/5962 [00:12<00:15, 190.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▎                      | 2396/5962 [00:12<00:12, 280.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 52%|███████████████████▌                  | 3071/5962 [00:12<00:09, 301.17it/s]\u001b[A\n",
      "\n",
      " 47%|█████████████████▉                    | 2807/5962 [00:12<00:11, 265.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|███████████████████▏                  | 3004/5962 [00:12<00:11, 249.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▊                   | 2942/5962 [00:12<00:11, 257.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▋                  | 3093/5962 [00:12<00:14, 201.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▍                      | 2425/5962 [00:12<00:13, 271.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 52%|███████████████████▊                  | 3104/5962 [00:12<00:09, 309.10it/s]\u001b[A\n",
      "\n",
      " 48%|██████████████████                    | 2842/5962 [00:12<00:10, 284.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|███████████████████▍                  | 3041/5962 [00:12<00:10, 282.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▏                   | 2852/5962 [00:12<00:13, 237.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▊                  | 3114/5962 [00:12<00:14, 191.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▋                      | 2453/5962 [00:12<00:13, 264.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 53%|████████████████████                  | 3145/5962 [00:12<00:08, 336.37it/s]\u001b[A\n",
      "\n",
      " 48%|██████████████████▎                   | 2872/5962 [00:12<00:11, 276.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▎                   | 2880/5962 [00:12<00:12, 249.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|███████████████████▌                  | 3071/5962 [00:12<00:10, 269.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████▏                  | 3001/5962 [00:12<00:11, 262.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|███████████████████▉                  | 3134/5962 [00:12<00:16, 175.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 49%|██████████████████▌                   | 2906/5962 [00:12<00:10, 291.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▎                 | 3180/5962 [00:12<00:09, 281.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▎                  | 3028/5962 [00:12<00:11, 254.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|███████████████████▊                  | 3099/5962 [00:12<00:11, 247.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████                  | 3153/5962 [00:13<00:16, 169.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▋                   | 2941/5962 [00:12<00:11, 271.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▌                  | 3063/5962 [00:12<00:10, 280.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 49%|██████████████████▋                   | 2937/5962 [00:13<00:11, 256.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|███████████████████▉                  | 3125/5962 [00:12<00:11, 243.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▏                     | 2537/5962 [00:12<00:12, 268.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 53%|████████████████████▏                 | 3171/5962 [00:13<00:16, 165.04it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████████▉                   | 2969/5962 [00:13<00:11, 268.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▋                  | 3097/5962 [00:12<00:09, 296.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 50%|██████████████████▉                   | 2964/5962 [00:13<00:12, 243.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▎                     | 2565/5962 [00:13<00:12, 263.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|████████████████████▎                 | 3188/5962 [00:13<00:17, 162.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▉                  | 3127/5962 [00:13<00:09, 295.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████                   | 2997/5962 [00:13<00:11, 247.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 50%|███████████████████                   | 2994/5962 [00:13<00:11, 256.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▌                     | 2598/5962 [00:13<00:11, 280.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 54%|████████████████████▋                 | 3237/5962 [00:13<00:13, 203.81it/s]\u001b[A\n",
      "\n",
      "\n",
      " 54%|████████████████████▍                 | 3205/5962 [00:13<00:17, 160.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████                  | 3157/5962 [00:13<00:09, 291.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 51%|███████████████████▎                  | 3031/5962 [00:13<00:10, 286.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████▋                     | 2627/5962 [00:13<00:11, 283.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 55%|████████████████████▊                 | 3268/5962 [00:13<00:11, 225.87it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▎                  | 3023/5962 [00:13<00:12, 229.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|████████████████████▌                 | 3222/5962 [00:13<00:16, 161.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▎                 | 3187/5962 [00:13<00:10, 272.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|████████████████▉                     | 2656/5962 [00:13<00:11, 278.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▍                  | 3049/5962 [00:13<00:12, 229.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 55%|████████████████████▉                 | 3293/5962 [00:13<00:12, 218.38it/s]\u001b[A\n",
      "\n",
      " 54%|████████████████████▋                 | 3241/5962 [00:13<00:16, 169.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|████████████████████▌                 | 3218/5962 [00:13<00:13, 198.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████▏                    | 2688/5962 [00:13<00:11, 290.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▍                 | 3215/5962 [00:13<00:10, 261.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▋                  | 3081/5962 [00:13<00:11, 248.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 55%|████████████████████▊                 | 3266/5962 [00:13<00:14, 188.26it/s]\u001b[A\n",
      "\n",
      "\n",
      " 54%|████████████████████▋                 | 3241/5962 [00:13<00:13, 205.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 52%|███████████████████▋                  | 3089/5962 [00:13<00:11, 239.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▋                 | 3248/5962 [00:13<00:09, 272.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▎                    | 2718/5962 [00:13<00:12, 269.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 56%|█████████████████████▎                | 3347/5962 [00:13<00:10, 238.74it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████                 | 3295/5962 [00:13<00:12, 215.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|████████████████████▊                 | 3262/5962 [00:13<00:13, 201.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▌                    | 2752/5962 [00:13<00:11, 286.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|████████████████████▉                 | 3276/5962 [00:13<00:10, 259.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 57%|█████████████████████▌                | 3377/5962 [00:13<00:10, 254.75it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▏                | 3317/5962 [00:13<00:12, 215.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|████████████████████▉                 | 3283/5962 [00:13<00:13, 192.19it/s]\u001b[A\u001b[A\u001b[A\n",
      " 57%|█████████████████████▋                | 3404/5962 [00:13<00:10, 247.89it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▋                    | 2782/5962 [00:13<00:11, 266.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|███████████████████▉                  | 3136/5962 [00:13<00:14, 195.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████                 | 3303/5962 [00:13<00:11, 236.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▏                 | 3168/5962 [00:13<00:11, 235.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|█████████████████████▏                | 3316/5962 [00:13<00:11, 227.68it/s]\u001b[A\u001b[A\u001b[A\n",
      " 58%|█████████████████████▉                | 3441/5962 [00:14<00:08, 280.28it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▉                    | 2810/5962 [00:13<00:11, 269.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▍                | 3364/5962 [00:14<00:11, 218.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|████████████████████                  | 3157/5962 [00:14<00:14, 187.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|█████████████████████▎                | 3350/5962 [00:14<00:10, 257.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▎                 | 3193/5962 [00:13<00:12, 227.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 58%|██████████████████████                | 3470/5962 [00:14<00:08, 279.46it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▌                | 3389/5962 [00:14<00:11, 227.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▍                | 3354/5962 [00:13<00:10, 238.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|████████████████████▏                 | 3177/5962 [00:14<00:15, 185.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████████████████████▌                | 3385/5962 [00:14<00:09, 279.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▌                 | 3222/5962 [00:14<00:11, 243.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 59%|██████████████████████▎               | 3508/5962 [00:14<00:08, 306.49it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▌                | 3379/5962 [00:14<00:10, 241.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▋                | 3412/5962 [00:14<00:12, 209.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████████████████████▊                | 3421/5962 [00:14<00:08, 301.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 54%|████████████████████▍                 | 3197/5962 [00:14<00:14, 187.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▋                 | 3248/5962 [00:14<00:11, 240.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 59%|██████████████████████▌               | 3540/5962 [00:14<00:08, 301.54it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▊                | 3413/5962 [00:14<00:09, 267.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▍                   | 2891/5962 [00:14<00:12, 251.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 54%|████████████████████▍                 | 3216/5962 [00:14<00:15, 180.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|████████████████████▉                 | 3278/5962 [00:14<00:10, 255.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|█████████████████████▉                | 3434/5962 [00:14<00:12, 196.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████████████████████▉                | 3446/5962 [00:14<00:08, 284.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▌                   | 2918/5962 [00:14<00:11, 255.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 58%|██████████████████████                | 3465/5962 [00:14<00:11, 225.93it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████                 | 3305/5962 [00:14<00:10, 251.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|██████████████████████▏               | 3482/5962 [00:14<00:08, 282.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▎               | 3491/5962 [00:14<00:07, 331.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 54%|████████████████████▌                 | 3235/5962 [00:14<00:18, 150.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|██████████████████████▍               | 3511/5962 [00:14<00:09, 268.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▏                | 3331/5962 [00:14<00:11, 237.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▊                   | 2944/5962 [00:14<00:14, 209.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 59%|██████████████████████▏               | 3489/5962 [00:14<00:13, 186.81it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▍               | 3525/5962 [00:14<00:08, 279.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|██████████████████████▌               | 3545/5962 [00:14<00:08, 284.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▍                | 3362/5962 [00:14<00:10, 256.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████████▉                   | 2967/5962 [00:14<00:14, 202.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 59%|██████████████████████▎               | 3510/5962 [00:14<00:13, 188.29it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▋               | 3561/5962 [00:14<00:08, 298.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 55%|████████████████████▉                 | 3289/5962 [00:14<00:13, 193.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▋                | 3408/5962 [00:14<00:08, 312.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████████████████████▊               | 3574/5962 [00:14<00:08, 275.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████                   | 2997/5962 [00:14<00:13, 226.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 60%|██████████████████████▌               | 3549/5962 [00:14<00:10, 238.07it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▉               | 3593/5962 [00:14<00:07, 301.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████████████████████▏                | 3329/5962 [00:14<00:10, 240.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████████████████████▉               | 3602/5962 [00:14<00:08, 265.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▎                  | 3023/5962 [00:14<00:12, 235.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 62%|███████████████████████▍              | 3678/5962 [00:14<00:09, 241.14it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▊               | 3575/5962 [00:15<00:09, 239.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▏              | 3635/5962 [00:14<00:06, 333.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████████████████████▍                | 3356/5962 [00:15<00:11, 232.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▍                  | 3048/5962 [00:15<00:12, 229.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 62%|███████████████████████▌              | 3703/5962 [00:15<00:09, 235.54it/s]\u001b[A\n",
      "\n",
      "\n",
      " 61%|███████████████████████▏              | 3629/5962 [00:15<00:09, 240.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▉               | 3601/5962 [00:15<00:10, 217.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▍              | 3670/5962 [00:14<00:07, 314.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▌                  | 3073/5962 [00:15<00:12, 234.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████████████████████▌                | 3381/5962 [00:15<00:11, 219.89it/s]\u001b[A\u001b[A\n",
      " 63%|███████████████████████▊              | 3729/5962 [00:15<00:09, 238.34it/s]\u001b[A\n",
      "\n",
      "\n",
      " 62%|███████████████████████▍              | 3671/5962 [00:15<00:07, 286.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▌              | 3705/5962 [00:15<00:06, 323.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████████████████████▋                | 3407/5962 [00:15<00:11, 229.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▎               | 3495/5962 [00:15<00:11, 222.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▋                  | 3097/5962 [00:15<00:12, 228.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|███████████████████████▋              | 3709/5962 [00:15<00:07, 309.83it/s]\u001b[A\u001b[A\u001b[A\n",
      " 61%|███████████████████████               | 3624/5962 [00:15<00:12, 189.60it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|███████████████████████▊              | 3741/5962 [00:15<00:06, 332.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████████████████████▉                | 3434/5962 [00:15<00:10, 240.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▍               | 3526/5962 [00:15<00:10, 240.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|███████████████████████▏              | 3645/5962 [00:15<00:11, 193.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▉                  | 3121/5962 [00:15<00:13, 218.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 63%|████████████████████████              | 3778/5962 [00:15<00:09, 225.14it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████              | 3778/5962 [00:15<00:06, 342.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 58%|██████████████████████                | 3463/5962 [00:15<00:09, 251.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▍              | 3677/5962 [00:15<00:10, 224.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|████████████████████████              | 3774/5962 [00:15<00:07, 294.38it/s]\u001b[A\u001b[A\u001b[A\n",
      " 64%|████████████████████████▏             | 3801/5962 [00:15<00:09, 217.36it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████                  | 3144/5962 [00:15<00:13, 201.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▎             | 3813/5962 [00:15<00:06, 317.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▌              | 3701/5962 [00:15<00:10, 224.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|██████████████████████▏               | 3489/5962 [00:15<00:11, 221.30it/s]\u001b[A\u001b[A\n",
      " 64%|████████████████████████▎             | 3823/5962 [00:15<00:10, 213.78it/s]\u001b[A\n",
      "\n",
      "\n",
      " 64%|████████████████████████▎             | 3805/5962 [00:15<00:07, 270.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▏                 | 3165/5962 [00:15<00:15, 180.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|███████████████████████▊              | 3730/5962 [00:15<00:09, 240.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████               | 3625/5962 [00:15<00:08, 261.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|██████████████████████▍               | 3517/5962 [00:15<00:10, 236.19it/s]\u001b[A\u001b[A\n",
      " 65%|████████████████████████▌             | 3850/5962 [00:15<00:09, 229.11it/s]\u001b[A\n",
      "\n",
      "\n",
      " 64%|████████████████████████▍             | 3833/5962 [00:15<00:08, 265.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▎                 | 3196/5962 [00:15<00:13, 212.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▋             | 3877/5962 [00:15<00:07, 293.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▎              | 3659/5962 [00:15<00:08, 278.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|██████████████████████▌               | 3543/5962 [00:15<00:09, 242.47it/s]\u001b[A\u001b[A\n",
      " 63%|███████████████████████▉              | 3755/5962 [00:15<00:09, 228.94it/s]\u001b[A\n",
      "\n",
      "\n",
      " 65%|████████████████████████▌             | 3860/5962 [00:15<00:08, 260.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▌                 | 3225/5962 [00:15<00:11, 231.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████████████████████▋               | 3568/5962 [00:16<00:11, 208.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|████████████████████████▉             | 3907/5962 [00:15<00:08, 229.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|████████████████████████              | 3779/5962 [00:16<00:11, 187.15it/s]\u001b[A\u001b[A\u001b[A\n",
      " 65%|████████████████████████▊             | 3898/5962 [00:16<00:12, 171.98it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▌              | 3688/5962 [00:15<00:11, 203.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|████████████████████▋                 | 3250/5962 [00:16<00:14, 181.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 64%|████████████████████████▏             | 3800/5962 [00:16<00:11, 181.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████             | 3933/5962 [00:16<00:10, 201.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▋              | 3712/5962 [00:16<00:11, 196.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|████████████████████▉                 | 3282/5962 [00:16<00:12, 212.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|████████████████████████▉             | 3918/5962 [00:16<00:12, 158.75it/s]\u001b[A\n",
      "\n",
      " 61%|███████████████████████               | 3613/5962 [00:16<00:11, 203.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|█████████████████████████             | 3935/5962 [00:16<00:09, 218.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▎             | 3820/5962 [00:16<00:13, 163.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|███████████████████████▊              | 3738/5962 [00:16<00:10, 209.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████                 | 3306/5962 [00:16<00:12, 213.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|███████████████████████▏              | 3636/5962 [00:16<00:11, 209.77it/s]\u001b[A\u001b[A\n",
      " 66%|█████████████████████████             | 3936/5962 [00:16<00:13, 153.65it/s]\u001b[A\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▎            | 3963/5962 [00:16<00:08, 233.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▍             | 3843/5962 [00:16<00:11, 178.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|███████████████████████▉              | 3765/5962 [00:16<00:09, 223.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|███████████████████████▎              | 3662/5962 [00:16<00:10, 222.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▏                | 3330/5962 [00:16<00:12, 215.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|█████████████████████████▏            | 3955/5962 [00:16<00:12, 160.93it/s]\u001b[A\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▍            | 3992/5962 [00:16<00:07, 248.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▋             | 3866/5962 [00:16<00:11, 189.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 62%|███████████████████████▌              | 3689/5962 [00:16<00:09, 235.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▍                | 3360/5962 [00:16<00:11, 234.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▏             | 3790/5962 [00:16<00:10, 215.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 67%|█████████████████████████▎            | 3979/5962 [00:16<00:11, 178.03it/s]\u001b[A\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▋            | 4021/5962 [00:16<00:07, 258.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▊             | 3889/5962 [00:16<00:10, 198.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 62%|███████████████████████▋              | 3720/5962 [00:16<00:08, 256.02it/s]\u001b[A\u001b[A\n",
      " 67%|█████████████████████████▌            | 4007/5962 [00:16<00:09, 204.37it/s]\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▊            | 4048/5962 [00:16<00:07, 256.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▎             | 3813/5962 [00:16<00:10, 197.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|████████████████████████▉             | 3910/5962 [00:16<00:10, 199.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 63%|███████████████████████▉              | 3752/5962 [00:16<00:08, 273.76it/s]\u001b[A\u001b[A\n",
      " 68%|█████████████████████████▋            | 4033/5962 [00:16<00:08, 218.20it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▉            | 4075/5962 [00:16<00:08, 227.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▉            | 4075/5962 [00:16<00:07, 258.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▊                | 3413/5962 [00:16<00:11, 217.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▍             | 3834/5962 [00:16<00:11, 186.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 64%|████████████████████████▏             | 3795/5962 [00:16<00:06, 318.45it/s]\u001b[A\u001b[A\n",
      " 68%|█████████████████████████▉            | 4062/5962 [00:16<00:08, 236.79it/s]\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▏           | 4113/5962 [00:16<00:06, 293.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████             | 3931/5962 [00:16<00:12, 167.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████████████████████▉                | 3443/5962 [00:16<00:10, 237.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▌             | 3854/5962 [00:16<00:11, 178.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 69%|██████████████████████████▏           | 4103/5962 [00:16<00:06, 284.33it/s]\u001b[A\n",
      "\n",
      " 64%|████████████████████████▍             | 3828/5962 [00:16<00:07, 280.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▍           | 4145/5962 [00:16<00:06, 298.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▏            | 3952/5962 [00:17<00:11, 175.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████                | 3469/5962 [00:16<00:10, 241.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▋             | 3877/5962 [00:16<00:10, 190.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 65%|████████████████████████▋             | 3865/5962 [00:17<00:06, 304.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▍           | 4155/5962 [00:16<00:07, 247.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▎            | 3972/5962 [00:17<00:10, 181.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▎               | 3495/5962 [00:17<00:10, 240.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▊             | 3902/5962 [00:17<00:10, 203.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▋           | 4181/5962 [00:17<00:07, 245.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▍            | 3991/5962 [00:17<00:10, 182.12it/s]\u001b[A\u001b[A\u001b[A\n",
      " 70%|██████████████████████████▌           | 4161/5962 [00:17<00:06, 257.29it/s]\u001b[A\n",
      "\n",
      " 65%|████████████████████████▊             | 3897/5962 [00:17<00:07, 280.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████             | 3925/5962 [00:17<00:09, 209.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|██████████████████████████▊           | 4210/5962 [00:17<00:06, 256.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▍               | 3520/5962 [00:17<00:12, 200.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 70%|██████████████████████████▋           | 4193/5962 [00:17<00:06, 273.59it/s]\u001b[A\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▌            | 4014/5962 [00:17<00:10, 191.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▏            | 3947/5962 [00:17<00:09, 212.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|█████████████████████████             | 3927/5962 [00:17<00:08, 240.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████           | 4237/5962 [00:17<00:06, 252.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▋            | 4039/5962 [00:17<00:09, 204.83it/s]\u001b[A\u001b[A\u001b[A\n",
      " 71%|██████████████████████████▉           | 4221/5962 [00:17<00:06, 263.87it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▎            | 3974/5962 [00:17<00:08, 225.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▌               | 3542/5962 [00:17<00:14, 172.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|█████████████████████████▏            | 3953/5962 [00:17<00:08, 241.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▏          | 4265/5962 [00:17<00:06, 258.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▉            | 4062/5962 [00:17<00:08, 211.46it/s]\u001b[A\u001b[A\u001b[A\n",
      " 71%|███████████████████████████           | 4248/5962 [00:17<00:06, 251.25it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▌            | 4002/5962 [00:17<00:08, 239.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████            | 4084/5962 [00:17<00:09, 204.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▋               | 3561/5962 [00:17<00:15, 151.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 67%|█████████████████████████▎            | 3979/5962 [00:17<00:09, 199.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▋            | 4027/5962 [00:17<00:08, 219.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 72%|███████████████████████████▏          | 4274/5962 [00:17<00:07, 217.65it/s]\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▌          | 4333/5962 [00:17<00:07, 222.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▏           | 4105/5962 [00:17<00:10, 174.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 72%|███████████████████████████▍          | 4297/5962 [00:17<00:07, 210.46it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████               | 3615/5962 [00:17<00:11, 200.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 67%|█████████████████████████▌            | 4001/5962 [00:17<00:11, 173.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▌          | 4324/5962 [00:17<00:08, 187.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▊          | 4359/5962 [00:17<00:07, 207.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▎           | 4124/5962 [00:17<00:10, 176.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 72%|███████████████████████████▌          | 4319/5962 [00:17<00:07, 208.78it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▎              | 3650/5962 [00:17<00:09, 237.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 67%|█████████████████████████▌            | 4020/5962 [00:17<00:11, 175.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▋          | 4347/5962 [00:17<00:08, 193.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▍           | 4147/5962 [00:18<00:09, 189.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▉            | 4069/5962 [00:17<00:11, 161.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 73%|███████████████████████████▋          | 4344/5962 [00:18<00:07, 216.00it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▌              | 3688/5962 [00:17<00:08, 274.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 68%|█████████████████████████▋            | 4039/5962 [00:18<00:10, 178.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▏         | 4416/5962 [00:18<00:06, 236.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▌           | 4170/5962 [00:18<00:09, 199.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████            | 4087/5962 [00:18<00:11, 162.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▋              | 3723/5962 [00:18<00:07, 293.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 73%|███████████████████████████▊          | 4366/5962 [00:18<00:07, 203.69it/s]\u001b[A\n",
      "\n",
      " 68%|█████████████████████████▉            | 4062/5962 [00:18<00:10, 188.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▎         | 4442/5962 [00:18<00:06, 237.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▋           | 4192/5962 [00:18<00:08, 204.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▏           | 4107/5962 [00:18<00:10, 170.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 74%|████████████████████████████          | 4394/5962 [00:18<00:07, 218.78it/s]\u001b[A\n",
      "\n",
      " 69%|██████████████████████████            | 4088/5962 [00:18<00:09, 205.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|██████████████████████████▊           | 4213/5962 [00:18<00:08, 197.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████          | 4412/5962 [00:18<00:08, 188.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|███████████████████████▉              | 3754/5962 [00:18<00:08, 248.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▎           | 4128/5962 [00:18<00:10, 179.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████████████████████████▏           | 4114/5962 [00:18<00:08, 219.42it/s]\u001b[A\u001b[A\n",
      " 71%|███████████████████████████           | 4238/5962 [00:18<00:08, 211.33it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▎         | 4442/5962 [00:18<00:07, 216.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▌         | 4491/5962 [00:18<00:06, 221.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▌           | 4158/5962 [00:18<00:08, 210.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████              | 3781/5962 [00:18<00:09, 234.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████████████████████████▍           | 4151/5962 [00:18<00:06, 260.21it/s]\u001b[A\u001b[A\n",
      " 74%|████████████████████████████▎         | 4440/5962 [00:18<00:07, 197.06it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▍         | 4465/5962 [00:18<00:06, 215.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▏          | 4260/5962 [00:18<00:08, 205.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▎             | 3811/5962 [00:18<00:08, 249.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▋           | 4180/5962 [00:18<00:08, 205.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 75%|████████████████████████████▍         | 4468/5962 [00:18<00:06, 217.41it/s]\u001b[A\n",
      "\n",
      " 70%|██████████████████████████▋           | 4178/5962 [00:18<00:07, 223.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▎          | 4294/5962 [00:18<00:06, 243.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▌         | 4488/5962 [00:18<00:06, 214.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▍             | 3843/5962 [00:18<00:07, 267.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▊           | 4202/5962 [00:18<00:09, 195.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 72%|███████████████████████████▌          | 4320/5962 [00:18<00:06, 247.40it/s]\u001b[A\n",
      "\n",
      " 71%|██████████████████████████▊           | 4204/5962 [00:18<00:07, 228.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████▊         | 4514/5962 [00:18<00:06, 223.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▏        | 4578/5962 [00:18<00:05, 241.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▋             | 3871/5962 [00:18<00:07, 266.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▋          | 4347/5962 [00:18<00:06, 252.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 76%|████████████████████████████▊         | 4517/5962 [00:18<00:06, 217.85it/s]\u001b[A\n",
      "\n",
      " 71%|██████████████████████████▉           | 4233/5962 [00:18<00:07, 244.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████▉         | 4539/5962 [00:18<00:06, 228.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▉             | 3903/5962 [00:18<00:07, 279.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████           | 4254/5962 [00:18<00:07, 219.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▉          | 4376/5962 [00:19<00:06, 261.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████████████████████████▏          | 4266/5962 [00:18<00:06, 265.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████         | 4565/5962 [00:18<00:05, 237.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████             | 3936/5962 [00:18<00:06, 291.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▎          | 4281/5962 [00:18<00:07, 231.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▍        | 4626/5962 [00:19<00:06, 213.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████████████████████████▍          | 4296/5962 [00:19<00:06, 274.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████          | 4403/5962 [00:19<00:06, 249.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 77%|█████████████████████████████         | 4565/5962 [00:19<00:06, 210.47it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▎            | 3966/5962 [00:19<00:07, 279.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▍          | 4305/5962 [00:19<00:07, 233.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▋        | 4652/5962 [00:19<00:05, 224.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▍        | 4624/5962 [00:19<00:05, 258.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████████████████████████▌          | 4325/5962 [00:19<00:06, 247.72it/s]\u001b[A\u001b[A\n",
      " 77%|█████████████████████████████▏        | 4587/5962 [00:19<00:07, 191.55it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▏         | 4429/5962 [00:19<00:07, 216.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▌          | 4333/5962 [00:19<00:06, 243.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▊        | 4677/5962 [00:19<00:05, 230.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▋        | 4666/5962 [00:19<00:04, 301.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████████████████████████▋          | 4351/5962 [00:19<00:06, 239.61it/s]\u001b[A\u001b[A\n",
      " 77%|█████████████████████████████▍        | 4615/5962 [00:19<00:06, 214.08it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▋            | 4023/5962 [00:19<00:07, 268.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▍         | 4452/5962 [00:19<00:07, 214.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████▉        | 4703/5962 [00:19<00:05, 237.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████▉        | 4701/5962 [00:19<00:04, 303.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████████████████████████▉          | 4386/5962 [00:19<00:05, 267.24it/s]\u001b[A\u001b[A\n",
      " 78%|█████████████████████████████▌        | 4638/5962 [00:19<00:06, 215.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████████████████████████▉          | 4391/5962 [00:19<00:05, 264.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▊            | 4051/5962 [00:19<00:07, 259.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▏       | 4736/5962 [00:19<00:04, 260.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▌         | 4475/5962 [00:19<00:07, 190.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 78%|█████████████████████████████▋        | 4662/5962 [00:19<00:05, 219.99it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████            | 4080/5962 [00:19<00:07, 266.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▎       | 4765/5962 [00:19<00:04, 267.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▏         | 4418/5962 [00:19<00:06, 238.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 75%|████████████████████████████▋         | 4495/5962 [00:19<00:07, 185.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▍       | 4773/5962 [00:19<00:03, 306.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 79%|█████████████████████████████▊        | 4685/5962 [00:19<00:05, 222.14it/s]\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▌       | 4794/5962 [00:19<00:04, 271.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▏           | 4107/5962 [00:19<00:07, 244.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▎         | 4443/5962 [00:19<00:06, 238.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 76%|████████████████████████████▊         | 4515/5962 [00:19<00:07, 180.90it/s]\u001b[A\u001b[A\n",
      " 79%|██████████████████████████████        | 4713/5962 [00:19<00:05, 237.72it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▌       | 4804/5962 [00:19<00:04, 285.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▋       | 4822/5962 [00:19<00:04, 272.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▎           | 4132/5962 [00:19<00:07, 246.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▍         | 4468/5962 [00:19<00:06, 237.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 76%|█████████████████████████████         | 4560/5962 [00:19<00:05, 249.73it/s]\u001b[A\u001b[A\n",
      " 80%|██████████████████████████████▏       | 4743/5962 [00:19<00:04, 252.98it/s]\u001b[A\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▉       | 4850/5962 [00:19<00:04, 274.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▊       | 4833/5962 [00:19<00:04, 268.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▋         | 4494/5962 [00:19<00:06, 242.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 76%|████████████████████████████▊         | 4517/5962 [00:19<00:04, 301.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▍           | 4157/5962 [00:19<00:07, 231.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 80%|██████████████████████████████▍       | 4769/5962 [00:19<00:04, 246.80it/s]\u001b[A\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▏        | 4587/5962 [00:20<00:06, 213.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 76%|████████████████████████████▉         | 4549/5962 [00:20<00:04, 295.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|██████████████████████████████▉       | 4861/5962 [00:19<00:04, 238.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▋           | 4181/5962 [00:19<00:08, 217.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████▊         | 4519/5962 [00:19<00:07, 201.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 80%|██████████████████████████████▌       | 4794/5962 [00:20<00:05, 226.80it/s]\u001b[A\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▎      | 4904/5962 [00:20<00:04, 249.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 77%|█████████████████████████████▏        | 4580/5962 [00:20<00:04, 282.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▏      | 4886/5962 [00:19<00:04, 232.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|██████████████████████████▊           | 4208/5962 [00:20<00:07, 227.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▍        | 4611/5962 [00:20<00:07, 181.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 81%|██████████████████████████████▋       | 4818/5962 [00:20<00:05, 220.66it/s]\u001b[A\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▍      | 4930/5962 [00:20<00:04, 242.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▎      | 4910/5962 [00:20<00:04, 214.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████         | 4564/5962 [00:20<00:06, 200.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|██████████████████████████▉           | 4232/5962 [00:20<00:08, 193.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 77%|█████████████████████████████▍        | 4609/5962 [00:20<00:05, 231.58it/s]\u001b[A\u001b[A\n",
      " 81%|██████████████████████████████▊       | 4841/5962 [00:20<00:05, 215.17it/s]\u001b[A\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▌        | 4632/5962 [00:20<00:07, 168.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▌      | 4944/5962 [00:20<00:04, 243.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▏          | 4257/5962 [00:20<00:08, 206.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▋      | 4980/5962 [00:20<00:04, 241.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 78%|█████████████████████████████▌        | 4634/5962 [00:20<00:05, 232.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▋        | 4651/5962 [00:20<00:08, 157.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 82%|██████████████████████████████▉       | 4863/5962 [00:20<00:06, 176.63it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▎          | 4279/5962 [00:20<00:08, 202.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▉      | 5005/5962 [00:20<00:04, 236.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▎        | 4604/5962 [00:20<00:08, 155.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▋      | 4970/5962 [00:20<00:05, 181.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▊        | 4668/5962 [00:20<00:08, 147.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████      | 5029/5962 [00:20<00:04, 221.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▍          | 4300/5962 [00:20<00:09, 182.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 82%|███████████████████████████████▎      | 4907/5962 [00:20<00:05, 187.77it/s]\u001b[A\n",
      "\n",
      " 78%|█████████████████████████████▊        | 4680/5962 [00:20<00:06, 184.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▍        | 4621/5962 [00:20<00:09, 145.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████▊        | 4684/5962 [00:20<00:09, 138.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▌          | 4323/5962 [00:20<00:08, 189.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▊      | 4991/5962 [00:20<00:06, 150.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 83%|███████████████████████████████▍      | 4931/5962 [00:20<00:05, 199.52it/s]\u001b[A\n",
      "\n",
      " 79%|█████████████████████████████▉        | 4700/5962 [00:20<00:07, 177.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▋          | 4346/5962 [00:20<00:08, 200.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▌        | 4637/5962 [00:20<00:10, 125.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████▉        | 4699/5962 [00:21<00:12, 102.11it/s]\u001b[A\u001b[A\u001b[A\n",
      " 83%|███████████████████████████████▌      | 4952/5962 [00:21<00:06, 155.60it/s]\u001b[A\n",
      "\n",
      " 79%|██████████████████████████████        | 4719/5962 [00:21<00:08, 143.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▋        | 4651/5962 [00:21<00:12, 109.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▊          | 4367/5962 [00:21<00:10, 149.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████        | 4714/5962 [00:21<00:11, 110.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▉      | 5009/5962 [00:21<00:08, 108.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████▎       | 4750/5962 [00:21<00:06, 178.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▊        | 4673/5962 [00:21<00:09, 131.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████████████████████████▉          | 4385/5962 [00:21<00:10, 155.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▏       | 4734/5962 [00:21<00:09, 128.85it/s]\u001b[A\u001b[A\u001b[A\n",
      " 84%|███████████████████████████████▊      | 4999/5962 [00:21<00:05, 190.99it/s]\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████▍       | 4783/5962 [00:21<00:05, 213.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████      | 5023/5962 [00:21<00:09, 104.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████▉        | 4690/5962 [00:21<00:09, 139.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▎       | 4749/5962 [00:21<00:09, 126.48it/s]\u001b[A\u001b[A\u001b[A\n",
      " 84%|███████████████████████████████▉      | 5020/5962 [00:21<00:05, 178.70it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████          | 4403/5962 [00:21<00:11, 136.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████      | 5038/5962 [00:21<00:08, 111.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████        | 4711/5962 [00:21<00:08, 155.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████▋       | 4807/5962 [00:21<00:05, 195.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▎       | 4765/5962 [00:21<00:08, 133.62it/s]\u001b[A\u001b[A\u001b[A\n",
      " 85%|████████████████████████████████▏     | 5051/5962 [00:21<00:04, 210.99it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▏         | 4425/5962 [00:21<00:10, 153.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▏       | 4731/5962 [00:21<00:07, 166.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▏     | 5056/5962 [00:21<00:07, 124.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████▊       | 4831/5962 [00:21<00:05, 204.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▍       | 4784/5962 [00:21<00:08, 146.55it/s]\u001b[A\u001b[A\u001b[A\n",
      " 85%|████████████████████████████████▎     | 5074/5962 [00:21<00:04, 210.47it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▎         | 4446/5962 [00:21<00:09, 166.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▎       | 4753/5962 [00:21<00:06, 180.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▎     | 5075/5962 [00:21<00:06, 135.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████▉       | 4854/5962 [00:21<00:05, 204.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▌       | 4800/5962 [00:21<00:07, 146.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▍         | 4470/5962 [00:21<00:08, 185.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 85%|████████████████████████████████▍     | 5097/5962 [00:21<00:04, 212.04it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▍       | 4777/5962 [00:21<00:06, 194.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▍     | 5095/5962 [00:21<00:05, 150.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 82%|███████████████████████████████       | 4876/5962 [00:21<00:05, 207.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▋       | 4819/5962 [00:21<00:07, 152.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▌         | 4490/5962 [00:21<00:08, 174.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▌     | 5117/5962 [00:21<00:05, 166.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 86%|████████████████████████████████▋     | 5119/5962 [00:21<00:04, 178.76it/s]\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▍    | 5241/5962 [00:21<00:04, 173.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 82%|███████████████████████████████▏      | 4898/5962 [00:21<00:05, 177.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▌       | 4798/5962 [00:21<00:07, 151.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▋     | 5135/5962 [00:21<00:04, 167.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▊     | 5139/5962 [00:22<00:04, 176.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▊       | 4835/5962 [00:22<00:10, 111.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▋       | 4816/5962 [00:21<00:07, 156.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 82%|███████████████████████████████▎      | 4917/5962 [00:22<00:06, 166.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▊     | 5153/5962 [00:21<00:04, 163.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████▊         | 4526/5962 [00:22<00:10, 143.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▋    | 5276/5962 [00:22<00:04, 161.75it/s]\u001b[A\u001b[A\u001b[A\n",
      " 87%|████████████████████████████████▉     | 5158/5962 [00:22<00:05, 158.48it/s]\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████▉       | 4850/5962 [00:22<00:09, 115.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▊       | 4834/5962 [00:22<00:07, 151.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████████████████████████████▉     | 5171/5962 [00:22<00:05, 158.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████▉         | 4548/5962 [00:22<00:08, 161.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▊    | 5297/5962 [00:22<00:03, 171.98it/s]\u001b[A\u001b[A\u001b[A\n",
      " 87%|█████████████████████████████████     | 5180/5962 [00:22<00:04, 171.27it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▉       | 4855/5962 [00:22<00:06, 165.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|██████████████████████████████▉       | 4863/5962 [00:22<00:09, 110.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 83%|███████████████████████████████▌      | 4960/5962 [00:22<00:05, 172.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████         | 4568/5962 [00:22<00:08, 170.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 5315/5962 [00:22<00:03, 172.48it/s]\u001b[A\u001b[A\u001b[A\n",
      " 87%|█████████████████████████████████▏    | 5202/5962 [00:22<00:04, 183.25it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▏      | 4888/5962 [00:22<00:07, 143.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▎    | 5218/5962 [00:22<00:03, 194.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 84%|███████████████████████████████▊      | 4982/5962 [00:22<00:05, 183.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▎        | 4590/5962 [00:22<00:07, 181.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 88%|█████████████████████████████████▎    | 5226/5962 [00:22<00:03, 196.00it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▎      | 4909/5962 [00:22<00:06, 158.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 5333/5962 [00:22<00:04, 153.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▍    | 5238/5962 [00:22<00:04, 175.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▍        | 4612/5962 [00:22<00:07, 189.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 88%|█████████████████████████████████▍    | 5247/5962 [00:22<00:03, 192.29it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▎      | 4912/5962 [00:22<00:05, 176.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████████████████████████████      | 5021/5962 [00:22<00:05, 178.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▍      | 4927/5962 [00:22<00:07, 146.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▌    | 5257/5962 [00:22<00:04, 174.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▌        | 4642/5962 [00:22<00:06, 218.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▍      | 4931/5962 [00:22<00:05, 172.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▏   | 5365/5962 [00:22<00:03, 150.90it/s]\u001b[A\u001b[A\u001b[A\n",
      " 83%|███████████████████████████████▌      | 4950/5962 [00:22<00:06, 167.48it/s]\u001b[A\n",
      "\n",
      " 85%|████████████████████████████████      | 5040/5962 [00:22<00:05, 164.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▋    | 5277/5962 [00:22<00:03, 176.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▊        | 4671/5962 [00:22<00:05, 238.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▌      | 4949/5962 [00:22<00:06, 162.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▋      | 4972/5962 [00:22<00:05, 179.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▋    | 5295/5962 [00:22<00:03, 170.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████▉        | 4696/5962 [00:22<00:05, 230.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 89%|█████████████████████████████████▋    | 5285/5962 [00:22<00:04, 152.97it/s]\u001b[A\n",
      "\n",
      " 85%|████████████████████████████████▏     | 5057/5962 [00:22<00:05, 152.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▍   | 5397/5962 [00:22<00:03, 151.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▊      | 4997/5962 [00:22<00:04, 198.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▊    | 5313/5962 [00:22<00:03, 169.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 89%|█████████████████████████████████▊    | 5305/5962 [00:23<00:04, 164.20it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████        | 4720/5962 [00:22<00:05, 223.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▊      | 4983/5962 [00:22<00:06, 162.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▉      | 5019/5962 [00:23<00:04, 199.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████████████████████████████▎     | 5073/5962 [00:23<00:06, 131.22it/s]\u001b[A\u001b[A\n",
      " 89%|█████████████████████████████████▉    | 5326/5962 [00:23<00:03, 174.54it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 5331/5962 [00:22<00:03, 162.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████      | 5040/5962 [00:23<00:05, 184.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▌   | 5429/5962 [00:23<00:03, 134.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████████████████████████████▍     | 5087/5962 [00:23<00:07, 121.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████    | 5348/5962 [00:23<00:03, 158.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 90%|██████████████████████████████████    | 5345/5962 [00:23<00:03, 160.27it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▎       | 4765/5962 [00:23<00:06, 189.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▎     | 5067/5962 [00:23<00:04, 203.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████████████████████████████▌     | 5100/5962 [00:23<00:07, 120.10it/s]\u001b[A\u001b[A\n",
      " 90%|██████████████████████████████████▏   | 5362/5962 [00:23<00:03, 161.28it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▏   | 5364/5962 [00:23<00:04, 149.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▉      | 5019/5962 [00:23<00:06, 142.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▍     | 5089/5962 [00:23<00:04, 206.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▊   | 5459/5962 [00:23<00:03, 138.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████████████████████████████▋     | 5123/5962 [00:23<00:05, 145.18it/s]\u001b[A\u001b[A\n",
      " 90%|██████████████████████████████████▎   | 5379/5962 [00:23<00:03, 161.13it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▏     | 5045/5962 [00:23<00:05, 171.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▎   | 5380/5962 [00:23<00:03, 147.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▌     | 5113/5962 [00:23<00:03, 213.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████   | 5494/5962 [00:23<00:02, 194.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████████████████████████████▊     | 5154/5962 [00:23<00:04, 186.25it/s]\u001b[A\u001b[A\n",
      " 91%|██████████████████████████████████▍   | 5396/5962 [00:23<00:03, 161.53it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▎     | 5070/5962 [00:23<00:04, 191.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▍   | 5405/5962 [00:23<00:03, 174.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▊     | 5148/5962 [00:23<00:03, 250.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▊       | 4843/5962 [00:23<00:05, 222.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 87%|█████████████████████████████████     | 5178/5962 [00:23<00:03, 200.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▌   | 5418/5962 [00:23<00:03, 176.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▌   | 5431/5962 [00:23<00:02, 197.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|████████████████████████████████▉     | 5175/5962 [00:23<00:03, 252.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████       | 4866/5962 [00:23<00:05, 216.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 87%|█████████████████████████████████▏    | 5206/5962 [00:23<00:03, 217.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▋   | 5452/5962 [00:23<00:02, 185.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▌     | 5118/5962 [00:23<00:04, 188.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 91%|██████████████████████████████████▋   | 5436/5962 [00:23<00:03, 149.51it/s]\u001b[A\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▌  | 5584/5962 [00:23<00:01, 242.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▏      | 4889/5962 [00:23<00:05, 194.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 87%|█████████████████████████████████▏    | 5201/5962 [00:23<00:03, 210.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▊     | 5143/5962 [00:23<00:04, 204.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▊   | 5471/5962 [00:23<00:02, 174.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 91%|██████████████████████████████████▊   | 5455/5962 [00:23<00:03, 157.66it/s]\u001b[A\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▊  | 5612/5962 [00:23<00:01, 252.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▎      | 4910/5962 [00:23<00:05, 193.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 88%|█████████████████████████████████▍    | 5255/5962 [00:23<00:03, 210.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▎    | 5224/5962 [00:24<00:03, 204.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 5489/5962 [00:23<00:02, 162.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|███████████████████████████████████▉  | 5638/5962 [00:24<00:01, 240.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████     | 5188/5962 [00:23<00:03, 213.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▍      | 4930/5962 [00:24<00:05, 183.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 92%|███████████████████████████████████   | 5497/5962 [00:24<00:02, 182.44it/s]\u001b[A\n",
      "\n",
      " 89%|█████████████████████████████████▋    | 5277/5962 [00:24<00:03, 189.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████   | 5506/5962 [00:24<00:03, 139.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▍    | 5246/5962 [00:24<00:04, 157.05it/s]\n",
      "\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████▏    | 5210/5962 [00:24<00:03, 194.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████  | 5663/5962 [00:24<00:01, 190.78it/s]\u001b[A\u001b[A\u001b[A\n",
      " 93%|███████████████████████████████████▏  | 5516/5962 [00:24<00:02, 162.82it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▏  | 5521/5962 [00:24<00:03, 138.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 89%|█████████████████████████████████▊    | 5297/5962 [00:24<00:04, 154.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▎    | 5231/5962 [00:24<00:04, 181.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▋      | 4967/5962 [00:24<00:06, 159.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 93%|███████████████████████████████████▎  | 5534/5962 [00:24<00:02, 164.37it/s]\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▌    | 5264/5962 [00:24<00:05, 133.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 89%|█████████████████████████████████▊    | 5314/5962 [00:24<00:04, 152.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▎  | 5536/5962 [00:24<00:03, 134.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 93%|███████████████████████████████████▍  | 5562/5962 [00:24<00:02, 195.31it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▊      | 4984/5962 [00:24<00:06, 158.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▎  | 5550/5962 [00:24<00:03, 130.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 5331/5962 [00:24<00:04, 145.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▍    | 5250/5962 [00:24<00:05, 139.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▋    | 5280/5962 [00:24<00:05, 117.12it/s]\u001b[A\u001b[A\u001b[A\n",
      " 94%|███████████████████████████████████▌  | 5583/5962 [00:24<00:02, 169.66it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▊      | 5001/5962 [00:24<00:07, 137.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▍  | 5564/5962 [00:24<00:03, 125.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 90%|██████████████████████████████████    | 5347/5962 [00:24<00:04, 138.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▌    | 5266/5962 [00:24<00:05, 134.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▍ | 5719/5962 [00:24<00:01, 138.25it/s]\u001b[A\u001b[A\u001b[A\n",
      " 94%|███████████████████████████████████▋  | 5603/5962 [00:24<00:02, 177.20it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████      | 5022/5962 [00:24<00:06, 153.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▋    | 5294/5962 [00:24<00:06, 109.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▋    | 5281/5962 [00:24<00:05, 134.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 90%|██████████████████████████████████▏   | 5362/5962 [00:24<00:04, 124.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████      | 5039/5962 [00:24<00:06, 144.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▊    | 5306/5962 [00:24<00:06, 104.67it/s]\u001b[A\u001b[A\u001b[A\n",
      " 94%|███████████████████████████████████▊  | 5622/5962 [00:24<00:02, 142.52it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▊    | 5296/5962 [00:24<00:05, 123.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 90%|██████████████████████████████████▎   | 5375/5962 [00:24<00:04, 118.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 5598/5962 [00:24<00:03, 109.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▋ | 5748/5962 [00:24<00:01, 125.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 5319/5962 [00:25<00:05, 109.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 95%|███████████████████████████████████▉  | 5638/5962 [00:25<00:02, 135.59it/s]\u001b[A\n",
      "\n",
      " 90%|██████████████████████████████████▎   | 5390/5962 [00:25<00:04, 126.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▊    | 5309/5962 [00:24<00:05, 118.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▎     | 5073/5962 [00:25<00:05, 148.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▋ | 5764/5962 [00:25<00:01, 129.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▊  | 5611/5962 [00:24<00:03, 108.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 91%|██████████████████████████████████▍   | 5403/5962 [00:25<00:04, 126.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████    | 5336/5962 [00:25<00:04, 154.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▏   | 5358/5962 [00:25<00:04, 148.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▊ | 5778/5962 [00:25<00:01, 128.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▊  | 5623/5962 [00:25<00:03, 108.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 95%|████████████████████████████████████  | 5653/5962 [00:25<00:02, 119.62it/s]\u001b[A\n",
      "\n",
      " 91%|██████████████████████████████████▌   | 5420/5962 [00:25<00:03, 137.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▏   | 5361/5962 [00:25<00:03, 177.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▎   | 5383/5962 [00:25<00:03, 172.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▉ | 5792/5962 [00:25<00:01, 130.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|███████████████████████████████████▉  | 5643/5962 [00:25<00:02, 129.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 95%|████████████████████████████████████  | 5666/5962 [00:25<00:02, 116.31it/s]\u001b[A\n",
      "\n",
      " 91%|██████████████████████████████████▋   | 5441/5962 [00:25<00:03, 155.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▎   | 5386/5962 [00:25<00:02, 193.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▍   | 5407/5962 [00:25<00:02, 185.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████  | 5660/5962 [00:25<00:02, 139.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|█████████████████████████████████████ | 5810/5962 [00:25<00:01, 140.79it/s]\u001b[A\u001b[A\u001b[A\n",
      " 95%|████████████████████████████████████▏ | 5681/5962 [00:25<00:02, 122.44it/s]\u001b[A\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 5473/5962 [00:25<00:02, 199.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▌   | 5430/5962 [00:25<00:02, 196.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▍   | 5407/5962 [00:25<00:02, 187.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▏| 5827/5962 [00:25<00:00, 148.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████▏ | 5678/5962 [00:25<00:01, 147.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 96%|████████████████████████████████████▎ | 5701/5962 [00:25<00:01, 141.20it/s]\u001b[A\n",
      "\n",
      " 92%|███████████████████████████████████   | 5510/5962 [00:25<00:01, 246.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▊   | 5457/5962 [00:25<00:02, 216.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▌   | 5427/5962 [00:25<00:02, 189.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▎| 5850/5962 [00:25<00:00, 170.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▎ | 5694/5962 [00:25<00:01, 151.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 96%|████████████████████████████████████▍ | 5723/5962 [00:25<00:01, 161.31it/s]\u001b[A\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 5489/5962 [00:25<00:01, 244.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▊   | 5454/5962 [00:25<00:02, 210.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▍| 5878/5962 [00:25<00:00, 201.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████▏    | 5211/5962 [00:25<00:03, 200.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▍ | 5710/5962 [00:25<00:01, 149.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 93%|███████████████████████████████████▏  | 5515/5962 [00:25<00:01, 248.18it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 5482/5962 [00:25<00:02, 229.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▌| 5902/5962 [00:25<00:00, 211.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▎    | 5233/5962 [00:25<00:03, 205.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 93%|███████████████████████████████████▌  | 5573/5962 [00:25<00:01, 240.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▌ | 5728/5962 [00:25<00:01, 156.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 97%|████████████████████████████████████▊ | 5775/5962 [00:25<00:00, 203.55it/s]\u001b[A\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▎  | 5541/5962 [00:25<00:01, 247.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▏  | 5516/5962 [00:25<00:01, 257.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▍    | 5254/5962 [00:25<00:03, 202.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 5599/5962 [00:25<00:01, 242.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▋ | 5757/5962 [00:25<00:01, 193.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 97%|████████████████████████████████████▉ | 5802/5962 [00:26<00:00, 218.46it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▍  | 5566/5962 [00:26<00:01, 238.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▊ | 5782/5962 [00:25<00:00, 209.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████▉| 5955/5962 [00:26<00:00, 217.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████| 5962/5962 [00:26<00:00, 228.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 5593/5962 [00:26<00:01, 246.54it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▉ | 5805/5962 [00:26<00:00, 213.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▌  | 5577/5962 [00:26<00:01, 257.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 95%|███████████████████████████████████▉  | 5648/5962 [00:26<00:01, 210.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▊    | 5305/5962 [00:26<00:03, 188.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 94%|███████████████████████████████████▊  | 5624/5962 [00:26<00:01, 263.41it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 5604/5962 [00:26<00:01, 250.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▏| 5827/5962 [00:26<00:00, 191.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 95%|████████████████████████████████████▏ | 5676/5962 [00:26<00:01, 227.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 5333/5962 [00:26<00:02, 210.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 95%|████████████████████████████████████  | 5651/5962 [00:26<00:01, 252.89it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▉  | 5630/5962 [00:26<00:01, 250.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▏   | 5356/5962 [00:26<00:02, 211.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▎| 5847/5962 [00:26<00:00, 176.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 95%|████████████████████████████████████▏ | 5677/5962 [00:26<00:01, 232.98it/s]\u001b[A\u001b[A\n",
      " 99%|█████████████████████████████████████▌| 5887/5962 [00:26<00:00, 158.74it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████  | 5656/5962 [00:26<00:01, 203.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▎   | 5378/5962 [00:26<00:03, 170.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▍| 5866/5962 [00:26<00:00, 144.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 96%|████████████████████████████████████▎ | 5701/5962 [00:26<00:01, 205.17it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████▏ | 5678/5962 [00:26<00:01, 204.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 96%|████████████████████████████████████▍ | 5721/5962 [00:26<00:01, 155.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▍   | 5410/5962 [00:26<00:02, 205.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▌ | 5730/5962 [00:26<00:01, 226.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 99%|█████████████████████████████████████▋| 5920/5962 [00:26<00:00, 144.79it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▎ | 5701/5962 [00:26<00:01, 210.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▋   | 5439/5962 [00:26<00:02, 225.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 96%|████████████████████████████████████▌ | 5739/5962 [00:26<00:01, 146.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▌ | 5731/5962 [00:26<00:00, 231.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▋ | 5754/5962 [00:26<00:00, 208.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|█████████████████████████████████████▊| 5935/5962 [00:26<00:00, 133.28it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▊   | 5464/5962 [00:26<00:02, 199.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 97%|████████████████████████████████████▋ | 5755/5962 [00:26<00:01, 137.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▊| 5924/5962 [00:26<00:00, 157.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▊ | 5776/5962 [00:27<00:00, 194.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|█████████████████████████████████████▉| 5949/5962 [00:27<00:00, 120.87it/s]\u001b[A\n",
      "\n",
      " 97%|████████████████████████████████████▊ | 5771/5962 [00:27<00:01, 141.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████▉| 5949/5962 [00:26<00:00, 177.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 5486/5962 [00:27<00:02, 172.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████| 5962/5962 [00:27<00:00, 219.06it/s]\u001b[A\n",
      " 97%|████████████████████████████████████▉ | 5797/5962 [00:27<00:00, 170.24it/s]\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▉ | 5786/5962 [00:27<00:01, 135.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████ | 5815/5962 [00:27<00:00, 166.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████   | 5505/5962 [00:27<00:02, 153.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 97%|████████████████████████████████████▉ | 5801/5962 [00:27<00:01, 134.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▏| 5844/5962 [00:27<00:00, 196.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▎  | 5535/5962 [00:27<00:02, 185.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████████████████████████████████ | 5823/5962 [00:27<00:00, 156.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▍| 5871/5962 [00:27<00:00, 214.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▍  | 5563/5962 [00:27<00:01, 207.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████████████████████████████████▎| 5849/5962 [00:27<00:00, 183.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▋| 5912/5962 [00:27<00:00, 266.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████████████████████████████████▌| 5887/5962 [00:27<00:00, 236.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 5608/5962 [00:27<00:01, 268.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████▊| 5941/5962 [00:27<00:00, 271.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████████████████████████████████▊| 5930/5962 [00:27<00:00, 291.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 95%|███████████████████████████████████▉  | 5642/5962 [00:27<00:01, 287.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 5962/5962 [00:27<00:00, 214.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████| 5962/5962 [00:27<00:00, 214.47it/s]\n",
      "100%|██████████████████████████████████████| 5962/5962 [00:27<00:00, 215.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████▏ | 5677/5962 [00:27<00:00, 302.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▍ | 5713/5962 [00:27<00:00, 316.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▌ | 5746/5962 [00:27<00:00, 314.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████ | 5817/5962 [00:28<00:00, 426.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▍| 5866/5962 [00:28<00:00, 443.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 5962/5962 [00:28<00:00, 210.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_train.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa.tsv\n",
      "Processing Started...\n",
      "Data Size:  9202\n",
      "number of threads:  7\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  1%|▌                                       | 19/1314 [00:00<00:06, 189.81it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  2%|▉                                       | 32/1314 [00:00<00:04, 312.35it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/1314 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "  1%|▌                                       | 17/1314 [00:00<00:07, 164.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▋                                       | 22/1314 [00:00<00:06, 214.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▍                                       | 13/1314 [00:00<00:12, 107.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▏                                      | 38/1314 [00:00<00:09, 135.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|█                                       | 35/1314 [00:00<00:10, 121.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▊                                       | 28/1314 [00:00<00:09, 130.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▊                                       | 25/1314 [00:00<00:11, 113.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▎                                      | 44/1314 [00:00<00:08, 147.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  4%|█▌                                      | 53/1314 [00:00<00:10, 124.03it/s]\u001b[A\u001b[A\n",
      "  5%|█▉                                      | 64/1314 [00:00<00:08, 142.26it/s]\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▋                                      | 55/1314 [00:00<00:08, 146.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▎                                      | 42/1314 [00:00<00:10, 120.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▏                                      | 37/1314 [00:00<00:11, 112.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  4%|█▍                                      | 49/1314 [00:00<00:10, 116.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▏                                     | 73/1314 [00:00<00:08, 147.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  6%|██▌                                     | 83/1314 [00:00<00:08, 153.59it/s]\u001b[A\n",
      "\n",
      "\n",
      "  5%|██▏                                     | 71/1314 [00:00<00:08, 147.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▉                                      | 65/1314 [00:00<00:07, 158.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▊                                      | 58/1314 [00:00<00:08, 148.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  6%|██▎                                     | 75/1314 [00:00<00:07, 161.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▉                                     | 98/1314 [00:00<00:06, 177.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  8%|███                                    | 102/1314 [00:00<00:07, 163.09it/s]\u001b[A\n",
      "\n",
      "\n",
      "  7%|██▉                                     | 96/1314 [00:00<00:06, 179.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▏                                   | 106/1314 [00:00<00:04, 242.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|███                                     | 99/1314 [00:00<00:05, 237.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  8%|███                                     | 99/1314 [00:00<00:06, 186.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▊                                   | 128/1314 [00:00<00:05, 206.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  9%|███▌                                   | 121/1314 [00:00<00:07, 167.73it/s]\u001b[A\n",
      "\n",
      "\n",
      "  9%|███▌                                   | 121/1314 [00:00<00:06, 178.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 11%|████▍                                  | 151/1314 [00:00<00:05, 210.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▋                                   | 124/1314 [00:00<00:06, 173.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▌                                   | 122/1314 [00:00<00:08, 136.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|████▏                                  | 140/1314 [00:00<00:06, 173.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 11%|████▏                                  | 141/1314 [00:00<00:06, 183.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▉                                   | 132/1314 [00:00<00:07, 164.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 13%|█████▏                                 | 175/1314 [00:00<00:05, 214.75it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▋                                  | 156/1314 [00:00<00:05, 209.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▎                                  | 145/1314 [00:00<00:07, 158.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|████▊                                  | 164/1314 [00:00<00:06, 187.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 13%|█████                                  | 172/1314 [00:00<00:05, 218.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▋                                  | 159/1314 [00:00<00:06, 187.59it/s]\n",
      " 15%|█████▊                                 | 197/1314 [00:01<00:05, 214.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▍                                 | 182/1314 [00:00<00:05, 221.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 13%|████▉                                  | 168/1314 [00:01<00:06, 174.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|█████▍                                 | 184/1314 [00:01<00:06, 182.43it/s]\u001b[A\u001b[A\u001b[A\n",
      " 17%|██████▋                                | 225/1314 [00:01<00:04, 233.19it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▍                                 | 182/1314 [00:01<00:06, 181.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▌                                 | 187/1314 [00:01<00:06, 172.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 15%|█████▊                                 | 195/1314 [00:01<00:06, 169.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▌                               | 256/1314 [00:01<00:04, 254.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████                                 | 206/1314 [00:01<00:05, 193.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 15%|██████                                 | 203/1314 [00:01<00:06, 159.73it/s]\u001b[A\n",
      "\n",
      " 21%|████████▎                              | 282/1314 [00:01<00:04, 244.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████▊                                | 230/1314 [00:01<00:05, 197.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████                                 | 206/1314 [00:01<00:06, 158.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|██████▌                                | 222/1314 [00:01<00:06, 163.22it/s]\u001b[A\u001b[A\u001b[A\n",
      " 17%|██████▌                                | 220/1314 [00:01<00:07, 153.90it/s]\u001b[A\n",
      "\n",
      " 18%|██████▉                                | 234/1314 [00:01<00:06, 165.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|█████████                              | 307/1314 [00:01<00:04, 211.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▍                               | 251/1314 [00:01<00:06, 172.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▌                                | 223/1314 [00:01<00:08, 133.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|███████                                | 239/1314 [00:01<00:08, 131.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 19%|███████▍                               | 252/1314 [00:01<00:06, 155.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▎                               | 246/1314 [00:01<00:06, 153.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 25%|█████████▊                             | 330/1314 [00:01<00:04, 207.09it/s]\u001b[A\n",
      "\n",
      "\n",
      " 20%|███████▋                               | 257/1314 [00:01<00:07, 141.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████                               | 270/1314 [00:01<00:06, 156.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 21%|████████                               | 272/1314 [00:01<00:06, 164.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▏                              | 274/1314 [00:01<00:05, 180.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 27%|██████████▍                            | 353/1314 [00:01<00:04, 209.45it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████                                | 238/1314 [00:01<00:09, 109.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|████████▏                              | 277/1314 [00:01<00:06, 154.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▌                              | 288/1314 [00:01<00:06, 159.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 23%|████████▊                              | 299/1314 [00:01<00:05, 189.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████▎                           | 381/1314 [00:01<00:04, 226.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 20%|███████▊                               | 263/1314 [00:01<00:09, 115.75it/s]\u001b[A\n",
      "\n",
      "\n",
      " 23%|████████▉                              | 301/1314 [00:01<00:05, 173.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▏                             | 309/1314 [00:01<00:05, 169.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▍                               | 251/1314 [00:01<00:10, 106.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 31%|████████████▏                          | 411/1314 [00:01<00:03, 246.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▌                             | 323/1314 [00:01<00:05, 182.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 21%|████████▏                              | 276/1314 [00:01<00:09, 111.72it/s]\u001b[A\n",
      "\n",
      "\n",
      " 25%|█████████▌                             | 324/1314 [00:01<00:05, 187.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▊                             | 332/1314 [00:01<00:05, 184.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▊                               | 265/1314 [00:01<00:09, 113.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 33%|█████████████                          | 439/1314 [00:02<00:03, 254.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████▏                            | 343/1314 [00:01<00:05, 181.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 22%|████████▋                              | 294/1314 [00:02<00:07, 127.98it/s]\u001b[A\n",
      "\n",
      "\n",
      " 26%|██████████▏                            | 344/1314 [00:02<00:05, 189.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▍                            | 353/1314 [00:01<00:05, 190.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▎                              | 281/1314 [00:02<00:08, 124.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 36%|█████████████▊                         | 467/1314 [00:02<00:03, 260.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▉                            | 368/1314 [00:02<00:04, 197.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 23%|█████████▏                             | 308/1314 [00:02<00:07, 127.25it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▊                              | 295/1314 [00:02<00:08, 126.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████▎                           | 382/1314 [00:02<00:04, 211.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██████████▊                            | 364/1314 [00:02<00:05, 180.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 31%|████████████                           | 405/1314 [00:02<00:03, 230.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▋                        | 494/1314 [00:02<00:03, 228.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▎                             | 314/1314 [00:02<00:07, 142.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 25%|█████████▌                             | 322/1314 [00:02<00:07, 124.31it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███████████▉                           | 404/1314 [00:02<00:04, 201.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 33%|████████████▋                          | 429/1314 [00:02<00:03, 229.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▋                          | 427/1314 [00:02<00:03, 236.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▊                             | 329/1314 [00:02<00:07, 139.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|███████████▉                           | 402/1314 [00:02<00:05, 179.86it/s]\u001b[A\u001b[A\u001b[A\n",
      " 25%|█████████▉                             | 335/1314 [00:02<00:08, 116.11it/s]\u001b[A\n",
      "\n",
      " 34%|█████████████▍                         | 453/1314 [00:02<00:03, 221.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|████████████████                       | 542/1314 [00:02<00:03, 212.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████▍                         | 452/1314 [00:02<00:04, 201.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████▏                            | 344/1314 [00:02<00:07, 127.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 26%|██████████▎                            | 347/1314 [00:02<00:09, 105.54it/s]\u001b[A\n",
      "\n",
      "\n",
      " 32%|████████████▍                          | 421/1314 [00:02<00:05, 154.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▊                      | 565/1314 [00:02<00:03, 213.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▋                            | 358/1314 [00:02<00:07, 125.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|██████████████                         | 474/1314 [00:02<00:04, 193.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 27%|██████████▋                            | 360/1314 [00:02<00:08, 110.66it/s]\u001b[A\n",
      "\n",
      " 36%|██████████████▏                        | 476/1314 [00:02<00:05, 162.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|█████████████████▍                     | 587/1314 [00:02<00:03, 212.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▋                        | 496/1314 [00:02<00:04, 199.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████████                            | 371/1314 [00:02<00:07, 118.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|█████████████▌                         | 456/1314 [00:02<00:05, 154.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████████                            | 372/1314 [00:02<00:09, 101.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|███████████████▎                       | 518/1314 [00:02<00:03, 202.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 46%|██████████████████                     | 609/1314 [00:02<00:03, 192.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████▍                           | 384/1314 [00:02<00:08, 110.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|██████████████                         | 472/1314 [00:02<00:05, 148.81it/s]\u001b[A\u001b[A\u001b[A\n",
      " 29%|███████████▋                            | 383/1314 [00:02<00:09, 96.22it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|██████████████▏                        | 476/1314 [00:02<00:06, 123.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 39%|███████████████▏                       | 512/1314 [00:02<00:05, 139.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▉                    | 637/1314 [00:03<00:03, 204.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|██████████████▍                        | 488/1314 [00:03<00:05, 149.01it/s]\u001b[A\u001b[A\u001b[A\n",
      " 31%|███████████▉                           | 402/1314 [00:03<00:07, 119.42it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▊                           | 396/1314 [00:02<00:08, 102.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 40%|███████████████▋                       | 528/1314 [00:03<00:05, 142.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▋                   | 664/1314 [00:03<00:02, 220.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▌                      | 559/1314 [00:03<00:04, 177.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|███████████████▎                       | 515/1314 [00:03<00:04, 180.09it/s]\u001b[A\u001b[A\u001b[A\n",
      " 32%|████████████▌                          | 422/1314 [00:03<00:06, 140.17it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|████████████▎                          | 413/1314 [00:03<00:07, 119.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|████████████████████▌                  | 692/1314 [00:03<00:02, 234.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▉                        | 505/1314 [00:03<00:06, 121.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|███████████████▉                       | 539/1314 [00:03<00:03, 194.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|█████████████████▏                     | 578/1314 [00:03<00:04, 174.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 33%|████████████▉                          | 437/1314 [00:03<00:06, 139.73it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▊                          | 430/1314 [00:03<00:06, 132.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████████████████████▎                 | 716/1314 [00:03<00:02, 235.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▍                       | 522/1314 [00:03<00:05, 133.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▊                     | 602/1314 [00:03<00:03, 190.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████▍                         | 453/1314 [00:03<00:05, 144.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████▍                         | 452/1314 [00:03<00:05, 154.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 45%|█████████████████▎                     | 585/1314 [00:03<00:04, 164.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▉                 | 740/1314 [00:03<00:02, 218.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|██████████████████▍                    | 622/1314 [00:03<00:03, 189.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|█████████████████▏                     | 580/1314 [00:03<00:03, 193.56it/s]\u001b[A\u001b[A\u001b[A\n",
      " 36%|██████████████                         | 473/1314 [00:03<00:05, 159.28it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████▉                         | 468/1314 [00:03<00:05, 153.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 46%|█████████████████▉                     | 605/1314 [00:03<00:04, 173.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▋                | 763/1314 [00:03<00:02, 206.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████████▎                        | 484/1314 [00:03<00:05, 147.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|█████████████████▊                     | 600/1314 [00:03<00:04, 177.71it/s]\u001b[A\u001b[A\u001b[A\n",
      " 37%|██████████████▌                        | 490/1314 [00:03<00:05, 146.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|███████████████████                    | 642/1314 [00:03<00:03, 170.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 47%|██████████████████▍                    | 623/1314 [00:03<00:04, 166.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|███████████████████████▎               | 785/1314 [00:03<00:02, 189.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████▌                   | 660/1314 [00:03<00:03, 163.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▊                        | 500/1314 [00:03<00:06, 133.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████▋                     | 594/1314 [00:03<00:04, 150.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 49%|███████████████████                    | 641/1314 [00:03<00:04, 157.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|██████████████████▎                    | 619/1314 [00:03<00:04, 141.55it/s]\u001b[A\u001b[A\u001b[A\n",
      " 39%|███████████████                        | 506/1314 [00:03<00:06, 116.09it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|████████████████████                   | 678/1314 [00:03<00:03, 167.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|███████████████▎                       | 514/1314 [00:03<00:06, 125.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 50%|███████████████████▌                   | 658/1314 [00:03<00:04, 156.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▉               | 805/1314 [00:03<00:03, 146.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▋                  | 699/1314 [00:03<00:03, 177.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 51%|████████████████████                   | 676/1314 [00:03<00:03, 161.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|██████████████████▊                    | 635/1314 [00:03<00:05, 122.47it/s]\u001b[A\u001b[A\u001b[A\n",
      " 63%|████████████████████████▍              | 823/1314 [00:04<00:03, 153.42it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▌                    | 625/1314 [00:03<00:05, 123.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████▎                 | 718/1314 [00:03<00:03, 159.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 64%|████████████████████████▉              | 840/1314 [00:04<00:03, 146.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▉                       | 539/1314 [00:04<00:07, 103.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▉                    | 638/1314 [00:04<00:05, 117.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▊                 | 735/1314 [00:04<00:03, 157.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 40%|████████████████▏                       | 530/1314 [00:04<00:09, 83.53it/s]\u001b[A\n",
      "\n",
      "\n",
      " 49%|███████████████████▊                    | 649/1314 [00:04<00:06, 98.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████████████████████                  | 711/1314 [00:04<00:04, 147.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▍             | 856/1314 [00:04<00:03, 142.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|██████████████████████▎                | 752/1314 [00:04<00:03, 153.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 42%|████████████████▋                       | 550/1314 [00:04<00:07, 96.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 41%|████████████████▍                       | 540/1314 [00:04<00:09, 80.01it/s]\u001b[A\n",
      "\n",
      "\n",
      " 50%|████████████████████                    | 661/1314 [00:04<00:06, 95.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████████████████████▌                 | 727/1314 [00:04<00:04, 142.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▊             | 871/1314 [00:04<00:03, 125.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▊                | 768/1314 [00:04<00:03, 141.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 42%|████████████████▋                       | 550/1314 [00:04<00:09, 83.49it/s]\u001b[A\n",
      "\n",
      " 56%|██████████████████████                 | 742/1314 [00:04<00:04, 142.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|█████████████████                       | 560/1314 [00:04<00:09, 83.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|████████████████████▍                   | 672/1314 [00:04<00:07, 90.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|████████████████████                   | 676/1314 [00:04<00:05, 110.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|███████████████████████▏               | 783/1314 [00:04<00:03, 142.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 43%|████████████████▊                      | 568/1314 [00:04<00:07, 103.75it/s]\u001b[A\n",
      "\n",
      " 67%|██████████████████████████▎            | 885/1314 [00:04<00:03, 117.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|████████████████████▊                   | 683/1314 [00:04<00:06, 92.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|█████████████████▎                      | 569/1314 [00:04<00:09, 79.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 44%|█████████████████▎                     | 582/1314 [00:04<00:06, 111.06it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|████████████████████▉                   | 688/1314 [00:04<00:06, 97.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|█████████████████████                   | 693/1314 [00:04<00:06, 89.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|█████████████████▌                      | 578/1314 [00:04<00:10, 69.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|███████████████████████████▎            | 898/1314 [00:04<00:04, 92.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|█████████████████████▍                  | 703/1314 [00:04<00:06, 88.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|███████████████████████                | 777/1314 [00:04<00:05, 102.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████████████████████▎                  | 699/1314 [00:04<00:07, 85.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|███████████████████████████▋            | 911/1314 [00:04<00:04, 99.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|█████████████████████▊                  | 715/1314 [00:04<00:06, 95.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████▊                      | 586/1314 [00:04<00:12, 59.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 45%|██████████████████                      | 594/1314 [00:05<00:10, 69.97it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▍           | 924/1314 [00:05<00:03, 106.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████████████████████▌                  | 709/1314 [00:04<00:07, 77.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 60%|████████████████████████                | 790/1314 [00:05<00:05, 89.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|██████████████████████                  | 725/1314 [00:05<00:06, 93.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|██████████████████                      | 593/1314 [00:05<00:12, 59.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████████████████████▊                  | 718/1314 [00:05<00:07, 78.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|████████████████████████▍               | 801/1314 [00:05<00:05, 91.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▉              | 839/1314 [00:05<00:04, 101.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|██████████████████████▎                 | 735/1314 [00:05<00:07, 80.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████████████▏          | 948/1314 [00:05<00:03, 103.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 46%|██████████████████▍                     | 604/1314 [00:05<00:12, 58.91it/s]\u001b[A\n",
      "\n",
      " 62%|████████████████████████▋               | 812/1314 [00:05<00:05, 91.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▉              | 851/1314 [00:05<00:04, 98.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|██████████████████████▏                 | 727/1314 [00:05<00:08, 68.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|██████████████████████▋                 | 745/1314 [00:05<00:06, 85.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▌          | 963/1314 [00:05<00:03, 113.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 63%|████████████████████████▋              | 833/1314 [00:05<00:04, 118.27it/s]\u001b[A\u001b[A\n",
      " 47%|██████████████████▋                     | 612/1314 [00:05<00:11, 58.80it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▋             | 867/1314 [00:05<00:03, 113.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|██████████████████████▏                | 749/1314 [00:05<00:05, 100.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|██████████████████████▋                | 763/1314 [00:05<00:05, 106.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████          | 981/1314 [00:05<00:02, 127.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 65%|█████████████████████████▎             | 851/1314 [00:05<00:03, 131.33it/s]\u001b[A\u001b[A\n",
      " 47%|██████████████████▉                     | 621/1314 [00:05<00:10, 63.28it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████▏            | 884/1314 [00:05<00:03, 126.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▋                | 763/1314 [00:05<00:05, 107.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|███████████████████████▎               | 784/1314 [00:05<00:03, 133.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████▌         | 995/1314 [00:05<00:02, 129.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|█████████████████████████▊             | 869/1314 [00:05<00:03, 142.87it/s]\u001b[A\u001b[A\n",
      " 48%|███████████████████▏                    | 631/1314 [00:05<00:09, 70.67it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▊            | 903/1314 [00:05<00:02, 140.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|███████████████████████                | 779/1314 [00:05<00:04, 120.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|███████████████████████▉               | 805/1314 [00:05<00:03, 152.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|███████████████████▌                   | 661/1314 [00:05<00:05, 119.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 77%|█████████████████████████████▏        | 1009/1314 [00:05<00:02, 118.53it/s]\u001b[A\n",
      "\n",
      " 67%|██████████████████████████▎            | 885/1314 [00:05<00:03, 132.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▊               | 803/1314 [00:05<00:03, 151.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|████████████████████████▍              | 825/1314 [00:05<00:02, 165.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|████████████████████▏                  | 681/1314 [00:05<00:04, 132.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▏           | 918/1314 [00:05<00:03, 123.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 50%|███████████████████▊                    | 651/1314 [00:05<00:08, 81.26it/s]\u001b[A\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▌        | 1022/1314 [00:05<00:02, 113.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████▎              | 820/1314 [00:05<00:03, 150.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████████████████████████▋            | 900/1314 [00:05<00:03, 129.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▋           | 932/1314 [00:05<00:03, 125.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 79%|█████████████████████████████▉        | 1035/1314 [00:05<00:02, 114.95it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▊              | 838/1314 [00:05<00:03, 147.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████████████████████████▏           | 914/1314 [00:05<00:03, 120.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▋                  | 695/1314 [00:05<00:06, 103.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████████████           | 946/1314 [00:05<00:03, 103.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 51%|████████████████████▍                   | 673/1314 [00:06<00:08, 72.42it/s]\u001b[A\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▌             | 863/1314 [00:06<00:04, 102.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|███████████████████████████████        | 1047/1314 [00:06<00:03, 82.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████████████████████▌                  | 707/1314 [00:06<00:07, 83.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▎             | 854/1314 [00:06<00:04, 106.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 52%|████████████████████▊                   | 682/1314 [00:06<00:09, 66.85it/s]\u001b[A\n",
      "\n",
      " 71%|████████████████████████████▏           | 927/1314 [00:06<00:04, 81.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▍       | 1058/1314 [00:06<00:02, 85.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|██████████████████████████             | 877/1314 [00:06<00:04, 102.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▊             | 870/1314 [00:06<00:03, 116.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|█████████████████████████████▍          | 969/1314 [00:06<00:03, 96.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 53%|█████████████████████                   | 691/1314 [00:06<00:08, 71.66it/s]\u001b[A\n",
      "\n",
      " 71%|████████████████████████████▌           | 939/1314 [00:06<00:04, 87.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▊                 | 737/1314 [00:06<00:05, 106.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▍            | 890/1314 [00:06<00:03, 106.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▋       | 1068/1314 [00:06<00:02, 84.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▊          | 980/1314 [00:06<00:03, 95.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 53%|█████████████████████▎                  | 702/1314 [00:06<00:07, 80.37it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▍                | 756/1314 [00:06<00:04, 125.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|████████████████████████████▉           | 950/1314 [00:06<00:04, 85.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▊            | 905/1314 [00:06<00:03, 115.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▉       | 1078/1314 [00:06<00:02, 87.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████▌         | 997/1314 [00:06<00:02, 111.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 54%|█████████████████████▋                  | 713/1314 [00:06<00:06, 86.81it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▉                | 771/1314 [00:06<00:04, 129.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████████████████████████████▍      | 1092/1314 [00:06<00:02, 99.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▎           | 919/1314 [00:06<00:03, 110.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 55%|█████████████████████▋                 | 729/1314 [00:06<00:05, 101.04it/s]\u001b[A\n",
      "\n",
      " 75%|█████████████████████████████          | 979/1314 [00:06<00:03, 108.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|███████████████████████▎               | 786/1314 [00:06<00:03, 132.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████▋      | 1103/1314 [00:06<00:02, 99.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▋           | 932/1314 [00:06<00:03, 113.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████        | 1041/1314 [00:06<00:01, 156.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 76%|█████████████████████████████▌         | 997/1314 [00:06<00:02, 125.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▊               | 802/1314 [00:06<00:03, 138.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 56%|██████████████████████▌                 | 740/1314 [00:06<00:06, 95.35it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▎     | 1119/1314 [00:06<00:01, 114.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|████████████████████████████▏          | 950/1314 [00:06<00:02, 130.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▋       | 1061/1314 [00:06<00:01, 164.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|████████████████████████▏              | 817/1314 [00:06<00:03, 140.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|██████████████████████▍                | 754/1314 [00:06<00:05, 105.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████████████████████████████▉     | 1138/1314 [00:06<00:01, 132.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▋          | 965/1314 [00:06<00:02, 134.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 78%|█████████████████████████████▋        | 1025/1314 [00:07<00:02, 125.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████▋              | 832/1314 [00:07<00:03, 134.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 58%|██████████████████████▋                | 765/1314 [00:07<00:05, 101.79it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▎    | 1152/1314 [00:07<00:01, 117.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████          | 980/1314 [00:07<00:02, 121.57it/s]\u001b[A\u001b[A\u001b[A\n",
      " 59%|███████████████████████                | 778/1314 [00:07<00:04, 108.18it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▏             | 848/1314 [00:07<00:03, 137.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▋    | 1165/1314 [00:07<00:01, 115.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▊      | 1100/1314 [00:07<00:01, 122.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 79%|██████████████████████████████        | 1039/1314 [00:07<00:02, 103.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████▍         | 993/1314 [00:07<00:02, 112.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▋             | 866/1314 [00:07<00:03, 147.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 60%|███████████████████████▍               | 790/1314 [00:07<00:04, 107.98it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▎       | 1047/1314 [00:07<00:01, 163.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████▍       | 1051/1314 [00:07<00:02, 103.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████         | 1007/1314 [00:07<00:02, 118.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▏     | 1115/1314 [00:07<00:01, 120.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 61%|███████████████████████▊               | 801/1314 [00:07<00:04, 107.33it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████▏            | 882/1314 [00:07<00:02, 147.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████▊       | 1066/1314 [00:07<00:02, 115.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▍   | 1189/1314 [00:07<00:01, 107.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 62%|████████████████████████▏              | 813/1314 [00:07<00:04, 110.64it/s]\u001b[A\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▍        | 1020/1314 [00:07<00:02, 109.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▋     | 1129/1314 [00:07<00:01, 111.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▌            | 897/1314 [00:07<00:03, 122.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 83%|███████████████████████████████▍      | 1089/1314 [00:07<00:01, 141.69it/s]\u001b[A\u001b[A\n",
      " 63%|████████████████████████▋              | 830/1314 [00:07<00:03, 123.85it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▏      | 1080/1314 [00:07<00:01, 140.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|███████████████████████████████████▌   | 1200/1314 [00:07<00:01, 86.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|███████████████████████████            | 911/1314 [00:07<00:03, 117.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 64%|█████████████████████████              | 845/1314 [00:07<00:03, 129.09it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████▉     | 1142/1314 [00:07<00:01, 99.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 84%|███████████████████████████████▉      | 1104/1314 [00:07<00:01, 120.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▋      | 1095/1314 [00:07<00:01, 127.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████▉   | 1210/1314 [00:07<00:01, 85.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▍    | 1156/1314 [00:07<00:01, 107.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 65%|█████████████████████████▍             | 858/1314 [00:07<00:03, 119.67it/s]\u001b[A\n",
      "\n",
      " 85%|████████████████████████████████▍     | 1123/1314 [00:07<00:01, 136.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▍           | 924/1314 [00:07<00:03, 103.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████      | 1109/1314 [00:07<00:01, 129.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|████████████████████████████████████▏  | 1221/1314 [00:07<00:01, 90.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▊    | 1168/1314 [00:07<00:01, 102.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▊           | 935/1314 [00:07<00:03, 104.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▍     | 1123/1314 [00:07<00:01, 130.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████████████████████████████▉     | 1138/1314 [00:07<00:01, 132.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 94%|████████████████████████████████████▌  | 1232/1314 [00:08<00:00, 94.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▎   | 1185/1314 [00:07<00:01, 117.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████████████████████████████▉     | 1137/1314 [00:07<00:01, 132.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 67%|██████████████████████████▎            | 886/1314 [00:08<00:03, 120.85it/s]\u001b[A\n",
      "\n",
      " 88%|█████████████████████████████████▍    | 1158/1314 [00:08<00:01, 149.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████████████           | 946/1314 [00:08<00:03, 101.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████  | 1245/1314 [00:08<00:00, 102.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 1206/1314 [00:08<00:00, 139.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 69%|██████████████████████████▊            | 904/1314 [00:08<00:03, 133.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▌          | 961/1314 [00:08<00:03, 113.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▌ | 1263/1314 [00:08<00:00, 122.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▎    | 1151/1314 [00:08<00:01, 112.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▌  | 1228/1314 [00:08<00:00, 159.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▉          | 977/1314 [00:08<00:02, 122.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▌   | 1195/1314 [00:08<00:00, 155.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████ | 1283/1314 [00:08<00:00, 143.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▊    | 1168/1314 [00:08<00:01, 123.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████  | 1245/1314 [00:08<00:00, 151.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████▍         | 992/1314 [00:08<00:02, 128.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 71%|███████████████████████████▋           | 932/1314 [00:08<00:02, 129.29it/s]\u001b[A\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▌| 1298/1314 [00:08<00:00, 143.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 92%|███████████████████████████████████   | 1211/1314 [00:08<00:00, 145.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▏   | 1181/1314 [00:08<00:01, 123.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▍ | 1261/1314 [00:08<00:00, 149.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▏        | 1008/1314 [00:08<00:02, 133.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▊     | 1134/1314 [00:08<00:01, 119.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|█████████████████████████████████████▉| 1313/1314 [00:08<00:00, 123.72it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▌   | 1194/1314 [00:08<00:01, 113.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 1314/1314 [00:08<00:00, 152.32it/s]\n",
      " 97%|████████████████████████████████████▉ | 1277/1314 [00:08<00:00, 147.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▌        | 1023/1314 [00:08<00:02, 135.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 93%|███████████████████████████████████▍  | 1226/1314 [00:08<00:00, 117.28it/s]\u001b[A\u001b[A\n",
      " 73%|████████████████████████████▍          | 959/1314 [00:08<00:02, 123.76it/s]\u001b[A\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████▏    | 1147/1314 [00:08<00:01, 107.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▍| 1293/1314 [00:08<00:00, 137.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 1206/1314 [00:08<00:01, 101.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 74%|████████████████████████████▉          | 974/1314 [00:08<00:02, 124.28it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████▉        | 1037/1314 [00:08<00:02, 120.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▌    | 1159/1314 [00:08<00:01, 108.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 94%|███████████████████████████████████▊  | 1239/1314 [00:08<00:00, 102.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████▊| 1308/1314 [00:08<00:00, 139.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▍       | 1054/1314 [00:08<00:01, 132.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 75%|█████████████████████████████▍         | 991/1314 [00:08<00:02, 136.62it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 1314/1314 [00:08<00:00, 149.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 1172/1314 [00:08<00:01, 114.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████████████████████████████████▏ | 1251/1314 [00:08<00:00, 99.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████       | 1074/1314 [00:08<00:01, 149.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 76%|█████████████████████████████         | 1005/1314 [00:09<00:02, 127.36it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|████████████████████████████████████▍  | 1228/1314 [00:08<00:00, 97.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▎   | 1186/1314 [00:08<00:01, 117.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 96%|████████████████████████████████████▌ | 1266/1314 [00:09<00:00, 109.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▋      | 1096/1314 [00:09<00:01, 166.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████  | 1245/1314 [00:08<00:00, 115.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 78%|█████████████████████████████▋        | 1025/1314 [00:09<00:01, 145.11it/s]\u001b[A\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 1207/1314 [00:09<00:00, 143.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████████████████████████████████ | 1282/1314 [00:09<00:00, 120.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▍     | 1122/1314 [00:09<00:01, 191.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 79%|██████████████████████████████▏       | 1042/1314 [00:09<00:01, 151.37it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▌ | 1265/1314 [00:09<00:00, 135.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▌  | 1228/1314 [00:09<00:00, 160.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████████████████████████████████▌| 1301/1314 [00:09<00:00, 136.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████     | 1142/1314 [00:09<00:00, 188.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 81%|██████████████████████████████▋       | 1059/1314 [00:09<00:01, 155.53it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 1314/1314 [00:09<00:00, 141.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████  | 1245/1314 [00:09<00:00, 139.75it/s]\u001b[A\u001b[A\u001b[A\n",
      " 82%|███████████████████████████████       | 1075/1314 [00:09<00:01, 148.32it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▎| 1292/1314 [00:09<00:00, 125.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▌    | 1162/1314 [00:09<00:00, 154.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▍ | 1260/1314 [00:09<00:00, 123.02it/s]\u001b[A\u001b[A\u001b[A\n",
      " 83%|███████████████████████████████▌      | 1091/1314 [00:09<00:01, 138.80it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 1314/1314 [00:09<00:00, 138.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▊ | 1274/1314 [00:09<00:00, 124.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████    | 1179/1314 [00:09<00:00, 135.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 84%|████████████████████████████████      | 1108/1314 [00:09<00:01, 146.71it/s]\u001b[A\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▌| 1301/1314 [00:09<00:00, 161.37it/s]\u001b[A\u001b[A\u001b[A\n",
      " 86%|████████████████████████████████▋     | 1132/1314 [00:09<00:01, 170.34it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 1314/1314 [00:09<00:00, 134.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 88%|█████████████████████████████████▎    | 1154/1314 [00:09<00:00, 183.93it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████   | 1212/1314 [00:09<00:00, 143.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 90%|██████████████████████████████████    | 1180/1314 [00:09<00:00, 205.20it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 1234/1314 [00:09<00:00, 161.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 92%|██████████████████████████████████▉   | 1208/1314 [00:10<00:00, 226.05it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████▎ | 1254/1314 [00:10<00:00, 171.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 94%|███████████████████████████████████▊  | 1238/1314 [00:10<00:00, 245.38it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▉ | 1278/1314 [00:10<00:00, 188.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████| 1314/1314 [00:10<00:00, 128.72it/s]\u001b[A\n",
      "100%|██████████████████████████████████████| 1314/1314 [00:10<00:00, 126.80it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb.tsv\n",
      "Processing Started...\n",
      "Data Size:  18406\n",
      "number of threads:  7\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▎                                       | 23/2629 [00:00<00:11, 224.61it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▎                                       | 24/2629 [00:00<00:10, 239.97it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  1%|▎                                       | 22/2629 [00:00<00:11, 219.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                  | 0/2629 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  1%|▎                                       | 21/2629 [00:00<00:12, 202.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|▏                                       | 13/2629 [00:00<00:20, 124.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|▏                                        | 10/2629 [00:00<00:26, 99.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  2%|▋                                       | 48/2629 [00:00<00:10, 234.65it/s]\u001b[A\n",
      "\n",
      "  2%|▋                                       | 48/2629 [00:00<00:10, 239.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▋                                       | 46/2629 [00:00<00:15, 165.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|▋                                       | 42/2629 [00:00<00:12, 200.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|▍                                       | 28/2629 [00:00<00:19, 135.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▎                                       | 22/2629 [00:00<00:24, 105.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  3%|█                                       | 72/2629 [00:00<00:11, 217.31it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▉                                       | 64/2629 [00:00<00:17, 145.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|▉                                       | 63/2629 [00:00<00:15, 170.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▌                                       | 36/2629 [00:00<00:21, 118.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▋                                       | 42/2629 [00:00<00:23, 108.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  3%|█                                       | 72/2629 [00:00<00:17, 149.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▋                                       | 48/2629 [00:00<00:18, 141.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  4%|█▍                                      | 94/2629 [00:00<00:13, 182.19it/s]\u001b[A\n",
      "\n",
      "\n",
      "  3%|█▏                                      | 81/2629 [00:00<00:14, 172.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▏                                      | 80/2629 [00:00<00:18, 134.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▊                                       | 54/2629 [00:00<00:23, 108.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|█                                       | 67/2629 [00:00<00:16, 157.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  4%|█▋                                     | 113/2629 [00:00<00:14, 172.02it/s]\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▋                                     | 115/2629 [00:00<00:11, 227.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▍                                      | 98/2629 [00:00<00:17, 146.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|█                                       | 70/2629 [00:00<00:20, 124.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▋                                     | 111/2629 [00:00<00:14, 168.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  5%|██                                     | 136/2629 [00:00<00:13, 187.39it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▍                                      | 95/2629 [00:00<00:13, 183.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▋                                     | 116/2629 [00:00<00:16, 155.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|█▍                                      | 91/2629 [00:00<00:16, 150.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  5%|█▉                                     | 134/2629 [00:00<00:13, 185.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▍                                     | 101/2629 [00:00<00:16, 157.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|██                                     | 142/2629 [00:00<00:13, 183.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▋                                     | 114/2629 [00:00<00:14, 173.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  6%|██▎                                    | 156/2629 [00:00<00:15, 155.30it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▊                                     | 120/2629 [00:00<00:15, 167.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  6%|██▎                                    | 154/2629 [00:00<00:13, 177.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▌                                     | 107/2629 [00:00<00:18, 135.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▉                                     | 132/2629 [00:00<00:15, 160.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▍                                    | 161/2629 [00:01<00:15, 158.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|██▋                                    | 183/2629 [00:00<00:13, 178.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "  7%|██▌                                    | 173/2629 [00:01<00:16, 149.42it/s]\u001b[A\n",
      "\n",
      "  7%|██▌                                    | 173/2629 [00:01<00:14, 164.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▊                                     | 122/2629 [00:00<00:21, 117.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▋                                    | 178/2629 [00:01<00:15, 156.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  7%|██▊                                    | 189/2629 [00:01<00:16, 143.69it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▎                                    | 158/2629 [00:01<00:16, 153.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  7%|██▊                                    | 191/2629 [00:01<00:15, 154.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|██▉                                    | 202/2629 [00:01<00:16, 149.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|██                                     | 135/2629 [00:01<00:22, 112.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▉                                    | 195/2629 [00:01<00:15, 157.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  8%|███                                    | 204/2629 [00:01<00:16, 143.36it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▌                                    | 174/2629 [00:01<00:16, 151.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▏                                   | 219/2629 [00:01<00:15, 151.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  8%|███▏                                   | 219/2629 [00:01<00:13, 176.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|██▏                                    | 147/2629 [00:01<00:23, 105.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  8%|███▏                                   | 219/2629 [00:01<00:17, 139.11it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▉                                    | 197/2629 [00:01<00:14, 171.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▋                                    | 181/2629 [00:01<00:18, 134.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  9%|███▌                                   | 236/2629 [00:01<00:16, 146.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▌                                   | 238/2629 [00:01<00:14, 168.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  9%|███▌                                   | 238/2629 [00:01<00:15, 151.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▉                                    | 195/2629 [00:01<00:19, 126.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  9%|███▎                                   | 224/2629 [00:01<00:21, 113.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|███▋                                   | 252/2629 [00:01<00:15, 148.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▏                                   | 215/2629 [00:01<00:18, 131.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  7%|██▋                                    | 184/2629 [00:01<00:19, 123.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▊                                   | 256/2629 [00:01<00:15, 149.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 10%|███▊                                   | 254/2629 [00:01<00:17, 134.18it/s]\u001b[A\n",
      "\n",
      "  9%|███▌                                   | 237/2629 [00:01<00:22, 107.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|███▉                                   | 268/2629 [00:01<00:17, 136.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▍                                   | 230/2629 [00:01<00:18, 126.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 10%|████                                   | 272/2629 [00:01<00:16, 142.99it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▍                                   | 233/2629 [00:01<00:16, 141.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|████                                   | 272/2629 [00:01<00:16, 140.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 10%|███▋                                   | 250/2629 [00:01<00:21, 110.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▋                                   | 246/2629 [00:01<00:17, 132.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 11%|████▎                                  | 292/2629 [00:01<00:15, 154.93it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|███                                    | 209/2629 [00:01<00:22, 109.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|████▏                                  | 283/2629 [00:01<00:20, 116.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▋                                   | 248/2629 [00:01<00:19, 122.31it/s]\n",
      "\n",
      " 10%|███▉                                   | 262/2629 [00:01<00:21, 109.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▊                                  | 323/2629 [00:01<00:11, 195.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▎                                   | 222/2629 [00:01<00:21, 114.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|████▍                                  | 300/2629 [00:01<00:18, 128.98it/s]\u001b[A\u001b[A\u001b[A\n",
      " 11%|████▍                                  | 302/2629 [00:01<00:18, 127.10it/s]\u001b[A\n",
      "\n",
      " 11%|████▏                                  | 284/2629 [00:01<00:17, 135.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▉                                   | 264/2629 [00:01<00:18, 129.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████▏                                 | 348/2629 [00:02<00:10, 209.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|████▊                                  | 326/2629 [00:02<00:14, 160.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|███▋                                   | 245/2629 [00:01<00:16, 142.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 12%|████▌                                  | 305/2629 [00:02<00:15, 154.04it/s]\u001b[A\u001b[A\n",
      " 12%|████▋                                  | 316/2629 [00:02<00:18, 123.06it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▏                                  | 279/2629 [00:02<00:18, 128.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|█████▍                                 | 370/2629 [00:02<00:10, 206.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|███▉                                   | 265/2629 [00:02<00:15, 152.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▌                                  | 304/2629 [00:02<00:16, 144.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 13%|████▉                                  | 333/2629 [00:02<00:17, 133.59it/s]\u001b[A\n",
      "\n",
      " 12%|████▊                                  | 322/2629 [00:02<00:15, 145.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▎                                  | 293/2629 [00:02<00:19, 119.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█████▊                                 | 392/2629 [00:02<00:12, 181.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▋                                  | 320/2629 [00:02<00:18, 126.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 13%|█████▏                                 | 347/2629 [00:02<00:18, 120.28it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▏                                  | 281/2629 [00:02<00:19, 122.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▌                                  | 306/2629 [00:02<00:20, 112.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 13%|█████                                  | 338/2629 [00:02<00:18, 124.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|██████                                 | 412/2629 [00:02<00:12, 175.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████                                  | 343/2629 [00:02<00:15, 149.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 14%|█████▍                                 | 369/2629 [00:02<00:15, 144.31it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▍                                  | 295/2629 [00:02<00:18, 125.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▋                                  | 319/2629 [00:02<00:19, 115.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|██████▏                                | 416/2629 [00:02<00:11, 189.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▍                                 | 365/2629 [00:02<00:13, 166.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 16%|██████▍                                | 431/2629 [00:02<00:13, 164.42it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▌                                  | 309/2629 [00:02<00:18, 125.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 13%|█████▏                                 | 352/2629 [00:02<00:21, 107.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▋                                | 453/2629 [00:02<00:12, 172.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|████▊                                  | 323/2629 [00:02<00:18, 125.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████                                   | 331/2629 [00:02<00:24, 92.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|██████▍                                | 436/2629 [00:02<00:14, 154.51it/s]\u001b[A\u001b[A\u001b[A\n",
      " 15%|█████▉                                 | 402/2629 [00:02<00:18, 120.60it/s]\u001b[A\n",
      "\n",
      " 14%|█████▌                                  | 364/2629 [00:02<00:24, 91.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████                                  | 342/2629 [00:02<00:16, 140.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|██████▉                                | 471/2629 [00:02<00:13, 160.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|██████                                 | 405/2629 [00:02<00:15, 147.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████▏                                  | 342/2629 [00:02<00:27, 83.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 16%|██████▎                                | 425/2629 [00:02<00:15, 144.40it/s]\u001b[A\n",
      "\n",
      " 14%|█████▌                                 | 378/2629 [00:02<00:22, 101.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|███████                                | 473/2629 [00:02<00:12, 166.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▏                               | 488/2629 [00:02<00:14, 151.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▎                                 | 360/2629 [00:02<00:21, 103.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 17%|██████▌                                | 446/2629 [00:02<00:13, 158.75it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▎                                | 422/2629 [00:02<00:16, 137.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 15%|█████▊                                 | 390/2629 [00:02<00:21, 103.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|███████▌                               | 512/2629 [00:03<00:12, 171.31it/s]\u001b[A\u001b[A\u001b[A\n",
      " 18%|██████▉                                | 466/2629 [00:03<00:13, 163.83it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▋                                  | 371/2629 [00:03<00:23, 95.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▋                                  | 373/2629 [00:03<00:27, 80.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 20%|███████▊                               | 530/2629 [00:03<00:15, 132.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▋                                 | 437/2629 [00:03<00:22, 96.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|███████▌                               | 509/2629 [00:03<00:19, 110.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|█████▊                                  | 383/2629 [00:03<00:23, 94.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 16%|██████▎                                 | 414/2629 [00:03<00:26, 84.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█████▊                                  | 383/2629 [00:03<00:28, 77.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████                               | 546/2629 [00:03<00:15, 136.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 18%|███████▏                               | 484/2629 [00:03<00:19, 108.41it/s]\u001b[A\n",
      "\n",
      "\n",
      " 20%|███████▊                               | 523/2629 [00:03<00:18, 112.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|█████▉                                  | 394/2629 [00:03<00:24, 92.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 16%|██████▍                                 | 427/2629 [00:03<00:23, 94.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|██████                                  | 396/2629 [00:03<00:25, 87.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▍                              | 566/2629 [00:03<00:13, 150.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 19%|███████▍                               | 499/2629 [00:03<00:18, 115.80it/s]\u001b[A\n",
      "\n",
      "\n",
      " 20%|███████▉                               | 538/2629 [00:03<00:17, 119.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 17%|██████▌                                | 445/2629 [00:03<00:19, 114.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|██████▏                                 | 404/2629 [00:03<00:24, 90.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▋                              | 588/2629 [00:03<00:12, 166.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████                                | 478/2629 [00:03<00:19, 112.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|████████▏                              | 552/2629 [00:03<00:17, 119.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 17%|██████▊                                | 458/2629 [00:03<00:19, 113.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▍                                | 433/2629 [00:03<00:17, 122.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▎                                 | 414/2629 [00:03<00:25, 85.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 20%|███████▉                               | 537/2629 [00:03<00:15, 131.49it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▎                               | 491/2629 [00:03<00:21, 101.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▋                                | 447/2629 [00:03<00:17, 126.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 18%|██████▉                                | 471/2629 [00:03<00:19, 108.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▍                                 | 423/2629 [00:03<00:27, 80.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 23%|████████▉                              | 606/2629 [00:03<00:17, 118.91it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▍                               | 504/2629 [00:03<00:19, 107.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|████████▋                              | 585/2629 [00:03<00:17, 118.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|██████▊                                | 461/2629 [00:03<00:18, 114.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▌                                 | 432/2629 [00:03<00:28, 78.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▋                               | 516/2629 [00:03<00:19, 110.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 18%|███████▎                                | 483/2629 [00:03<00:22, 95.86it/s]\u001b[A\u001b[A\n",
      " 22%|████████▍                              | 571/2629 [00:03<00:16, 127.27it/s]\u001b[A\n",
      "\n",
      "\n",
      " 24%|█████████▍                              | 621/2629 [00:04<00:20, 97.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▋                                 | 440/2629 [00:04<00:30, 71.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|████████                                | 528/2629 [00:03<00:22, 95.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|█████████                              | 610/2629 [00:04<00:18, 110.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▏                                | 474/2629 [00:04<00:24, 88.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 22%|████████▋                              | 585/2629 [00:04<00:19, 104.55it/s]\u001b[A\n",
      "\n",
      " 24%|█████████▋                              | 634/2629 [00:04<00:20, 98.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▊                                 | 448/2629 [00:04<00:32, 68.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▏                               | 539/2629 [00:04<00:24, 87.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|█████████▏                             | 622/2629 [00:04<00:18, 106.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▍                                | 485/2629 [00:04<00:24, 86.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|██████▉                                 | 460/2629 [00:04<00:27, 78.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 19%|███████▋                                | 503/2629 [00:04<00:30, 70.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▎                               | 550/2629 [00:04<00:22, 91.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|█████████▍                             | 635/2629 [00:04<00:18, 108.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▏                                | 469/2629 [00:04<00:26, 80.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▉                              | 657/2629 [00:04<00:20, 95.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 19%|███████▊                                | 511/2629 [00:04<00:30, 70.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▌                               | 562/2629 [00:04<00:20, 98.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|█████████▊                              | 647/2629 [00:04<00:20, 98.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▋                                | 504/2629 [00:04<00:26, 80.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 23%|█████████▏                              | 607/2629 [00:04<00:25, 80.04it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▎                                | 478/2629 [00:04<00:29, 73.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 25%|██████████▏                             | 668/2629 [00:04<00:21, 91.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▋                               | 573/2629 [00:04<00:23, 86.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 24%|█████████▍                              | 620/2629 [00:04<00:22, 89.24it/s]\u001b[A\n",
      "\n",
      " 20%|████████                                | 532/2629 [00:04<00:25, 81.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████▎                             | 678/2629 [00:04<00:21, 89.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██████████                              | 658/2629 [00:04<00:22, 85.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▊                                | 513/2629 [00:04<00:30, 69.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▊                               | 583/2629 [00:04<00:23, 87.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 24%|█████████▌                              | 632/2629 [00:04<00:20, 95.43it/s]\u001b[A\n",
      "\n",
      " 21%|████████▎                               | 544/2629 [00:04<00:23, 87.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████▍                             | 689/2629 [00:04<00:21, 91.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▉                                | 524/2629 [00:04<00:27, 77.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██████████▏                             | 668/2629 [00:04<00:23, 84.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|████████▉                              | 601/2629 [00:04<00:18, 109.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▊                                | 510/2629 [00:04<00:23, 88.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 24%|█████████▊                              | 643/2629 [00:04<00:23, 86.14it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|████████▏                               | 535/2629 [00:04<00:24, 85.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 27%|██████████▋                             | 699/2629 [00:04<00:22, 86.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|█████████                              | 613/2629 [00:04<00:18, 107.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|██████████▎                             | 677/2629 [00:04<00:24, 78.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|███████▊                               | 524/2629 [00:04<00:20, 101.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 22%|████████▌                              | 574/2629 [00:05<00:18, 110.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▏                              | 552/2629 [00:04<00:20, 102.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 27%|██████████▊                             | 709/2629 [00:05<00:22, 86.61it/s]\u001b[A\n",
      "\n",
      "\n",
      " 26%|██████████▍                             | 687/2629 [00:05<00:23, 82.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████                               | 541/2629 [00:05<00:17, 119.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▎                              | 563/2629 [00:05<00:19, 103.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▌                              | 625/2629 [00:05<00:22, 89.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 22%|████████▋                              | 587/2629 [00:05<00:18, 110.46it/s]\u001b[A\u001b[A\n",
      " 25%|██████████                              | 662/2629 [00:05<00:24, 81.61it/s]\u001b[A\n",
      "\n",
      "\n",
      " 26%|██████████▌                             | 696/2629 [00:05<00:24, 79.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▉                             | 718/2629 [00:05<00:25, 75.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▌                              | 576/2629 [00:05<00:19, 107.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 23%|████████▉                              | 600/2629 [00:05<00:17, 113.35it/s]\u001b[A\u001b[A\n",
      " 26%|██████████▎                             | 675/2629 [00:05<00:21, 92.76it/s]\u001b[A\n",
      "\n",
      "\n",
      " 27%|██████████▊                             | 707/2629 [00:05<00:22, 85.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████████▏                            | 733/2629 [00:05<00:20, 92.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 26%|██████████▍                             | 686/2629 [00:05<00:20, 95.61it/s]\u001b[A\n",
      "\n",
      " 23%|█████████                              | 612/2629 [00:05<00:18, 107.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▋                              | 588/2629 [00:05<00:20, 101.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|████████▍                              | 571/2629 [00:05<00:19, 106.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|███████████▎                            | 744/2629 [00:05<00:19, 95.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▎                             | 625/2629 [00:05<00:17, 112.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 29%|███████████▍                            | 754/2629 [00:05<00:21, 88.22it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|█████████                               | 599/2629 [00:05<00:23, 86.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|███████████                             | 726/2629 [00:05<00:25, 74.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 24%|█████████▍                             | 637/2629 [00:05<00:18, 106.51it/s]\u001b[A\u001b[A\n",
      " 27%|██████████▊                             | 709/2629 [00:05<00:20, 95.89it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████▋                            | 766/2629 [00:05<00:19, 95.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|█████████▎                              | 609/2629 [00:05<00:22, 88.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|███████████▏                            | 734/2629 [00:05<00:26, 71.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|█████████                               | 594/2629 [00:05<00:23, 85.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██████████                              | 659/2629 [00:05<00:29, 65.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 27%|██████████▉                             | 720/2629 [00:05<00:20, 92.54it/s]\u001b[A\n",
      "\n",
      " 30%|███████████▊                            | 776/2629 [00:05<00:21, 87.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▍                              | 619/2629 [00:05<00:24, 82.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|███████████▎                            | 745/2629 [00:05<00:24, 78.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|█████████▏                              | 604/2629 [00:05<00:23, 87.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████▏                             | 671/2629 [00:05<00:25, 78.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 30%|███████████▉                            | 787/2629 [00:05<00:20, 90.06it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▌                              | 628/2629 [00:05<00:24, 82.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|███████████▍                            | 755/2629 [00:05<00:22, 82.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▏                             | 621/2629 [00:05<00:18, 105.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████▍                             | 682/2629 [00:05<00:23, 83.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 28%|███████████▎                            | 743/2629 [00:06<00:19, 95.87it/s]\u001b[A\n",
      "\n",
      " 30%|████████████▏                           | 799/2629 [00:06<00:18, 97.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▊                              | 641/2629 [00:05<00:21, 93.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|███████████▌                            | 764/2629 [00:06<00:22, 81.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▍                             | 633/2629 [00:06<00:18, 106.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▍                            | 704/2629 [00:05<00:16, 117.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 29%|███████████▎                           | 761/2629 [00:06<00:16, 114.19it/s]\u001b[A\n",
      "\n",
      " 31%|████████████▏                          | 819/2629 [00:06<00:14, 122.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▉                              | 653/2629 [00:06<00:20, 98.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|███████████▊                            | 775/2629 [00:06<00:21, 88.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▌                             | 645/2629 [00:06<00:18, 108.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▋                            | 720/2629 [00:06<00:15, 126.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 30%|███████████▋                           | 785/2629 [00:06<00:12, 147.19it/s]\u001b[A\n",
      "\n",
      " 32%|████████████▍                          | 837/2629 [00:06<00:13, 137.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|█████████▉                             | 672/2629 [00:06<00:16, 122.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███████████▊                           | 793/2629 [00:06<00:16, 113.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▊                             | 658/2629 [00:06<00:17, 111.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████████                            | 743/2629 [00:06<00:12, 154.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 31%|███████████▉                           | 802/2629 [00:06<00:11, 152.88it/s]\u001b[A\n",
      "\n",
      " 32%|████████████▋                          | 852/2629 [00:06<00:12, 138.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████▎                            | 695/2629 [00:06<00:12, 150.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████▎                           | 764/2629 [00:06<00:11, 165.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████▉                             | 670/2629 [00:06<00:18, 107.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 28%|██████████▉                            | 736/2629 [00:06<00:11, 159.32it/s]\u001b[A\u001b[A\n",
      " 31%|████████████▏                          | 818/2629 [00:06<00:12, 143.31it/s]\u001b[A\n",
      "\n",
      "\n",
      " 32%|████████████▍                          | 835/2629 [00:06<00:11, 160.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▊                          | 867/2629 [00:06<00:13, 128.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████▎                             | 681/2629 [00:06<00:19, 99.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 29%|███████████▏                           | 754/2629 [00:06<00:12, 154.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▌                           | 782/2629 [00:06<00:13, 141.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|█████████████                          | 881/2629 [00:06<00:13, 130.92it/s]\u001b[A\u001b[A\u001b[A\n",
      " 32%|████████████▎                          | 833/2629 [00:06<00:13, 130.32it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▊                            | 726/2629 [00:06<00:14, 129.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██████████▍                            | 701/2629 [00:06<00:15, 125.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▊                           | 800/2629 [00:06<00:12, 148.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 34%|█████████████▎                         | 895/2629 [00:06<00:13, 133.31it/s]\u001b[A\u001b[A\n",
      " 32%|████████████▋                          | 854/2629 [00:06<00:11, 149.92it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████████                            | 747/2629 [00:06<00:12, 147.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|██████████▊                            | 725/2629 [00:06<00:12, 156.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████▎                         | 894/2629 [00:06<00:09, 183.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 30%|███████████▊                           | 795/2629 [00:06<00:10, 174.33it/s]\u001b[A\u001b[A\n",
      " 35%|█████████████▌                         | 911/2629 [00:06<00:12, 138.47it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▍                          | 837/2629 [00:06<00:10, 163.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|█████████████▊                         | 928/2629 [00:06<00:11, 145.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████▎                           | 763/2629 [00:06<00:15, 118.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 34%|█████████████▏                         | 893/2629 [00:06<00:11, 150.62it/s]\u001b[A\n",
      "\n",
      " 31%|████████████                           | 814/2629 [00:06<00:12, 150.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████████                            | 742/2629 [00:06<00:16, 117.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|█████████████▉                         | 943/2629 [00:07<00:12, 140.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▌                           | 779/2629 [00:06<00:14, 127.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▋                          | 854/2629 [00:06<00:12, 138.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 35%|█████████████▍                         | 909/2629 [00:07<00:12, 142.04it/s]\u001b[A\n",
      "\n",
      " 32%|████████████▎                          | 831/2629 [00:07<00:13, 136.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|██████████████▏                        | 955/2629 [00:07<00:08, 186.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|██████████████▏                        | 958/2629 [00:07<00:12, 138.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▊                           | 797/2629 [00:07<00:13, 138.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 35%|█████████████▋                         | 924/2629 [00:07<00:11, 143.99it/s]\u001b[A\n",
      "\n",
      " 32%|████████████▌                          | 848/2629 [00:07<00:12, 140.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████▍                           | 769/2629 [00:07<00:16, 110.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████▏                         | 887/2629 [00:07<00:11, 147.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|██████████████▍                        | 974/2629 [00:07<00:10, 162.60it/s]\u001b[A\u001b[A\u001b[A\n",
      " 36%|█████████████▉                         | 939/2629 [00:07<00:12, 139.02it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████████▍                        | 972/2629 [00:07<00:14, 115.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 33%|████████████▊                          | 863/2629 [00:07<00:13, 132.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|██████████████▍                       | 1000/2629 [00:07<00:08, 186.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████▍                         | 903/2629 [00:07<00:12, 143.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▉                            | 781/2629 [00:07<00:18, 98.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▎                          | 831/2629 [00:07<00:12, 140.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 37%|██████████████▌                        | 985/2629 [00:07<00:15, 107.37it/s]\u001b[A\n",
      "\n",
      " 33%|█████████████                          | 879/2629 [00:07<00:12, 136.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|██████████████▉                       | 1034/2629 [00:07<00:07, 225.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▋                         | 926/2629 [00:07<00:10, 165.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|███████████▊                           | 800/2629 [00:07<00:15, 119.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▌                          | 851/2629 [00:07<00:11, 154.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 38%|██████████████▊                        | 998/2629 [00:07<00:14, 112.60it/s]\u001b[A\n",
      "\n",
      " 34%|█████████████▎                         | 895/2629 [00:07<00:12, 142.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|██████████████                         | 951/2629 [00:07<00:08, 188.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|███████████████▎                      | 1058/2629 [00:07<00:07, 219.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|████████████▏                          | 822/2629 [00:07<00:12, 143.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▉                          | 869/2629 [00:07<00:10, 160.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 38%|██████████████▌                       | 1006/2629 [00:07<00:08, 189.27it/s]\u001b[A\n",
      "\n",
      " 38%|██████████████▋                       | 1012/2629 [00:07<00:13, 117.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████████▍                        | 975/2629 [00:07<00:08, 201.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████████▍                          | 842/2629 [00:07<00:11, 156.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████▍                         | 902/2629 [00:07<00:08, 206.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 39%|██████████████▊                       | 1028/2629 [00:07<00:08, 197.43it/s]\u001b[A\n",
      "\n",
      "\n",
      " 41%|███████████████▌                      | 1081/2629 [00:07<00:07, 206.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 39%|██████████████▊                       | 1025/2629 [00:07<00:13, 119.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▊                        | 997/2629 [00:07<00:07, 204.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▊                         | 929/2629 [00:07<00:07, 223.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▋                          | 859/2629 [00:07<00:11, 152.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|███████████████▉                      | 1103/2629 [00:07<00:07, 198.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 40%|███████████████▏                      | 1048/2629 [00:07<00:10, 148.90it/s]\u001b[A\u001b[A\n",
      " 40%|███████████████▏                      | 1048/2629 [00:07<00:09, 171.02it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▋                       | 1019/2629 [00:07<00:07, 205.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|██████████████                         | 952/2629 [00:07<00:07, 212.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|████████████▉                          | 876/2629 [00:07<00:11, 148.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 38%|██████████████▋                        | 994/2629 [00:07<00:07, 218.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|███████████████▍                      | 1070/2629 [00:07<00:09, 166.87it/s]\u001b[A\u001b[A\u001b[A\n",
      " 41%|███████████████▍                      | 1069/2629 [00:07<00:08, 181.12it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▏                      | 1048/2629 [00:07<00:06, 228.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████████▎                         | 896/2629 [00:07<00:10, 161.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|████████████████▌                     | 1145/2629 [00:08<00:07, 192.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 39%|██████████████▋                       | 1017/2629 [00:08<00:07, 217.04it/s]\u001b[A\u001b[A\n",
      " 42%|███████████████▊                      | 1097/2629 [00:08<00:07, 192.36it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████████▍                        | 974/2629 [00:07<00:08, 184.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▌                      | 1079/2629 [00:07<00:06, 251.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▌                         | 914/2629 [00:08<00:10, 161.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 43%|████████████████▏                     | 1118/2629 [00:08<00:07, 196.79it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████████████████                      | 1111/2629 [00:08<00:05, 270.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|████████████████▊                     | 1165/2629 [00:08<00:08, 171.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 40%|███████████████                       | 1039/2629 [00:08<00:08, 189.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▍                     | 1138/2629 [00:08<00:08, 186.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 43%|████████████████▎                     | 1127/2629 [00:08<00:08, 170.57it/s]\u001b[A\n",
      "\n",
      "\n",
      " 45%|█████████████████                     | 1183/2629 [00:08<00:08, 170.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▋                       | 1012/2629 [00:08<00:09, 166.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▍                     | 1139/2629 [00:08<00:06, 221.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████████▊                         | 931/2629 [00:08<00:13, 122.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 40%|███████████████▎                      | 1059/2629 [00:08<00:09, 162.59it/s]\u001b[A\u001b[A\n",
      " 44%|████████████████▋                     | 1157/2629 [00:08<00:09, 160.93it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▉                       | 1030/2629 [00:08<00:09, 164.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|█████████████████▎                    | 1201/2629 [00:08<00:09, 157.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|██████████████                         | 949/2629 [00:08<00:12, 135.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 41%|███████████████▌                      | 1077/2629 [00:08<00:09, 166.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|████████████████▉                     | 1175/2629 [00:08<00:08, 164.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▏                      | 1047/2629 [00:08<00:09, 163.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|█████████████████▌                    | 1218/2629 [00:08<00:09, 148.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████▏                    | 1186/2629 [00:08<00:07, 205.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|███████████████▊                      | 1095/2629 [00:08<00:10, 151.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████▏                    | 1192/2629 [00:08<00:09, 145.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▌                    | 1212/2629 [00:08<00:06, 219.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|█████████████████▊                    | 1234/2629 [00:08<00:10, 133.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████                     | 1177/2629 [00:08<00:11, 125.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▋                      | 1083/2629 [00:08<00:09, 161.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 42%|████████████████                      | 1111/2629 [00:08<00:11, 132.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▉                    | 1239/2629 [00:08<00:06, 231.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|█████████████████▍                    | 1208/2629 [00:08<00:11, 125.98it/s]\u001b[A\u001b[A\u001b[A\n",
      " 45%|█████████████████▏                    | 1193/2629 [00:08<00:11, 127.65it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▋                        | 992/2629 [00:08<00:14, 113.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 43%|████████████████▎                     | 1126/2629 [00:08<00:11, 127.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|██████████████████▎                   | 1265/2629 [00:08<00:09, 140.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|███████████████▉                      | 1100/2629 [00:08<00:11, 138.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████████▌                       | 1004/2629 [00:08<00:14, 112.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▎                   | 1263/2629 [00:08<00:07, 188.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 46%|█████████████████▋                    | 1222/2629 [00:09<00:12, 112.36it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▋                       | 1020/2629 [00:09<00:13, 123.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|██████████████████▌                   | 1280/2629 [00:09<00:11, 119.60it/s]\u001b[A\u001b[A\u001b[A\n",
      " 46%|█████████████████▋                    | 1220/2629 [00:09<00:11, 119.99it/s]\u001b[A\n",
      "\n",
      " 43%|████████████████▍                     | 1140/2629 [00:09<00:14, 104.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▊                    | 1234/2629 [00:09<00:13, 103.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████████▉                       | 1033/2629 [00:09<00:14, 109.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 44%|████████████████▋                     | 1152/2629 [00:09<00:14, 101.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▌                   | 1284/2629 [00:09<00:09, 140.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 47%|█████████████████▊                    | 1233/2629 [00:09<00:12, 111.28it/s]\u001b[A\n",
      "\n",
      "\n",
      " 47%|██████████████████▍                    | 1245/2629 [00:09<00:13, 98.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 44%|█████████████████▎                     | 1163/2629 [00:09<00:14, 98.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▋                      | 1128/2629 [00:09<00:17, 87.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 47%|█████████████████▉                    | 1245/2629 [00:09<00:13, 100.84it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▋                    | 1256/2629 [00:09<00:14, 97.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▊                   | 1301/2629 [00:09<00:10, 124.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 45%|████████████████▉                     | 1176/2629 [00:09<00:14, 102.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|███████████████████▎                   | 1305/2629 [00:09<00:15, 85.01it/s]\u001b[A\u001b[A\u001b[A\n",
      " 48%|██████████████████▏                   | 1256/2629 [00:09<00:13, 103.00it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▊                    | 1266/2629 [00:09<00:14, 93.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████████▋                       | 1056/2629 [00:09<00:18, 86.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|███████████████████▌                   | 1319/2629 [00:09<00:13, 96.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 45%|█████████████████▏                    | 1190/2629 [00:09<00:13, 108.60it/s]\u001b[A\u001b[A\n",
      " 48%|██████████████████▎                   | 1268/2629 [00:09<00:12, 107.32it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▉                    | 1276/2629 [00:09<00:14, 93.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|█████████████████                      | 1149/2629 [00:09<00:17, 83.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|███████████████████▏                  | 1331/2629 [00:09<00:12, 100.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 46%|█████████████████▌                    | 1215/2629 [00:09<00:09, 143.82it/s]\u001b[A\u001b[A\n",
      " 49%|██████████████████▍                   | 1279/2629 [00:09<00:13, 100.97it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▏                  | 1330/2629 [00:09<00:11, 113.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████████▊                       | 1066/2629 [00:09<00:20, 76.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|███████████████████                    | 1286/2629 [00:09<00:15, 88.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 49%|██████████████████▋                   | 1290/2629 [00:09<00:13, 100.49it/s]\u001b[A\n",
      "\n",
      "\n",
      " 51%|███████████████████▉                   | 1343/2629 [00:09<00:13, 94.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▍                  | 1347/2629 [00:09<00:10, 124.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|████████████████                       | 1083/2629 [00:09<00:15, 97.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████████▊                   | 1305/2629 [00:09<00:11, 113.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 47%|█████████████████▊                    | 1231/2629 [00:09<00:11, 119.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 42%|███████████████▊                      | 1097/2629 [00:09<00:14, 106.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|███████████████████▎                   | 1301/2629 [00:10<00:14, 89.93it/s]\u001b[A\n",
      "\n",
      "\n",
      " 52%|███████████████████▋                  | 1361/2629 [00:09<00:10, 116.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 50%|███████████████████                   | 1317/2629 [00:10<00:12, 103.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████▋                     | 1192/2629 [00:10<00:15, 94.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 42%|████████████████                      | 1109/2629 [00:10<00:14, 104.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 50%|███████████████████▍                   | 1311/2629 [00:10<00:15, 83.19it/s]\u001b[A\n",
      "\n",
      "\n",
      " 51%|███████████████████▋                   | 1328/2629 [00:10<00:13, 93.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▊                     | 1203/2629 [00:10<00:15, 91.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 48%|██████████████████▋                    | 1258/2629 [00:10<00:14, 97.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▋                      | 1121/2629 [00:10<00:15, 96.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|████████████████████▍                  | 1374/2629 [00:10<00:13, 91.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|████████████████████▍                  | 1377/2629 [00:10<00:14, 88.79it/s]\u001b[A\u001b[A\u001b[A\n",
      " 51%|███████████████████▊                   | 1338/2629 [00:10<00:14, 89.92it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|██████████████████                     | 1215/2629 [00:10<00:14, 97.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 48%|██████████████████▎                   | 1270/2629 [00:10<00:13, 101.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████████▍                     | 1139/2629 [00:10<00:12, 117.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████                  | 1388/2629 [00:10<00:12, 100.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|████████████████████                  | 1392/2629 [00:10<00:12, 102.59it/s]\u001b[A\u001b[A\u001b[A\n",
      " 51%|███████████████████▌                  | 1352/2629 [00:10<00:12, 101.77it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▊                    | 1230/2629 [00:10<00:12, 109.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▌                   | 1282/2629 [00:10<00:12, 103.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▍                 | 1418/2629 [00:10<00:08, 144.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|████████████████████▎                 | 1408/2629 [00:10<00:10, 116.38it/s]\u001b[A\u001b[A\u001b[A\n",
      " 52%|███████████████████▋                  | 1364/2629 [00:10<00:11, 106.14it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████                    | 1250/2629 [00:10<00:10, 132.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|████████████████████▉                 | 1445/2629 [00:10<00:06, 173.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 49%|██████████████████▋                   | 1296/2629 [00:10<00:11, 111.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|█████████████████                     | 1181/2629 [00:10<00:09, 150.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|████████████████████▌                 | 1422/2629 [00:10<00:10, 120.66it/s]\u001b[A\u001b[A\u001b[A\n",
      " 53%|███████████████████▉                  | 1382/2629 [00:10<00:09, 125.78it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▎                   | 1269/2629 [00:10<00:09, 147.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▎                | 1474/2629 [00:10<00:05, 202.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 50%|██████████████████▉                   | 1312/2629 [00:10<00:10, 121.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|█████████████████▍                    | 1204/2629 [00:10<00:08, 171.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|████████████████████▉                 | 1447/2629 [00:10<00:07, 155.13it/s]\u001b[A\u001b[A\u001b[A\n",
      " 54%|████████████████████▍                 | 1410/2629 [00:10<00:07, 167.28it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▌                   | 1285/2629 [00:10<00:09, 142.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 51%|███████████████████▎                  | 1332/2629 [00:10<00:09, 141.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▋                    | 1226/2629 [00:10<00:07, 183.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|█████████████████████▎                | 1475/2629 [00:10<00:06, 186.05it/s]\u001b[A\u001b[A\u001b[A\n",
      " 55%|████████████████████▊                 | 1436/2629 [00:10<00:06, 191.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████████▉                   | 1307/2629 [00:10<00:08, 163.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████                | 1528/2629 [00:10<00:04, 226.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 52%|███████████████████▌                  | 1354/2629 [00:10<00:07, 162.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████████▉                    | 1245/2629 [00:10<00:07, 174.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████████████████████▋                | 1503/2629 [00:10<00:05, 210.66it/s]\u001b[A\u001b[A\u001b[A\n",
      " 55%|█████████████████████                 | 1456/2629 [00:10<00:06, 189.01it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|███████████████████▎                  | 1333/2629 [00:10<00:06, 188.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▌               | 1558/2629 [00:10<00:04, 246.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 52%|███████████████████▉                  | 1376/2629 [00:10<00:07, 175.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|██████████████████▎                   | 1267/2629 [00:10<00:07, 185.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|██████████████████████▏               | 1531/2629 [00:10<00:04, 227.56it/s]\u001b[A\u001b[A\u001b[A\n",
      " 56%|█████████████████████▍                | 1484/2629 [00:11<00:05, 214.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▋                  | 1361/2629 [00:10<00:05, 213.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|████████████████████▏                 | 1398/2629 [00:11<00:06, 188.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▉               | 1584/2629 [00:10<00:04, 234.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|██████████████████▌                   | 1286/2629 [00:11<00:07, 178.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|██████████████████████▍               | 1555/2629 [00:11<00:04, 228.94it/s]\u001b[A\u001b[A\u001b[A\n",
      " 57%|█████████████████████▌                | 1491/2629 [00:11<00:05, 201.92it/s]\u001b[A\n",
      "\n",
      " 57%|█████████████████████▊                | 1506/2629 [00:11<00:05, 190.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▎              | 1609/2629 [00:11<00:04, 226.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████████▉                   | 1311/2629 [00:11<00:06, 196.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|███████████████████▉                  | 1383/2629 [00:11<00:06, 180.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 58%|█████████████████████▊                | 1512/2629 [00:11<00:06, 175.23it/s]\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████████████████████▊               | 1579/2629 [00:11<00:05, 190.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 55%|████████████████████▊                 | 1442/2629 [00:11<00:06, 188.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▌              | 1634/2629 [00:11<00:04, 232.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████                | 1526/2629 [00:11<00:06, 166.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▎                 | 1403/2629 [00:11<00:07, 162.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 58%|██████████████████████▏               | 1535/2629 [00:11<00:05, 189.10it/s]\u001b[A\n",
      "\n",
      "\n",
      " 61%|███████████████████████▏              | 1600/2629 [00:11<00:05, 192.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████████████████████▏                | 1462/2629 [00:11<00:06, 187.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▏             | 1672/2629 [00:11<00:03, 269.70it/s]\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▎               | 1544/2629 [00:11<00:06, 164.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▌                 | 1422/2629 [00:11<00:07, 164.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|███████████████████████▍              | 1625/2629 [00:11<00:04, 206.99it/s]\u001b[A\u001b[A\u001b[A\n",
      " 59%|██████████████████████▍               | 1555/2629 [00:11<00:05, 183.51it/s]\u001b[A\n",
      "\n",
      " 56%|█████████████████████▍                | 1481/2629 [00:11<00:06, 178.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|███████████████████▉                  | 1378/2629 [00:11<00:06, 203.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▌               | 1565/2629 [00:11<00:06, 171.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|████████████████████▊                 | 1441/2629 [00:11<00:07, 164.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|███████████████████████▊              | 1647/2629 [00:11<00:04, 204.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████████████████████▋                | 1502/2629 [00:11<00:06, 185.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|████████████████████▏                 | 1399/2629 [00:11<00:06, 192.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 60%|██████████████████████▊               | 1574/2629 [00:11<00:06, 162.13it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▉               | 1583/2629 [00:11<00:06, 162.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████                 | 1460/2629 [00:11<00:06, 170.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|████████████████████████              | 1669/2629 [00:11<00:04, 200.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 54%|████████████████████▌                 | 1419/2629 [00:11<00:06, 188.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|███████████████████████▏              | 1601/2629 [00:11<00:06, 162.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████▎                | 1478/2629 [00:11<00:06, 172.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▎            | 1750/2629 [00:11<00:04, 197.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|████████████████████▊                 | 1440/2629 [00:11<00:06, 194.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 61%|██████████████████████▉               | 1591/2629 [00:11<00:08, 119.78it/s]\u001b[A\n",
      "\n",
      "\n",
      " 64%|████████████████████████▍             | 1690/2629 [00:11<00:05, 160.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▌                | 1496/2629 [00:11<00:07, 152.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▍              | 1618/2629 [00:12<00:07, 137.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|██████████████████████▏               | 1539/2629 [00:11<00:08, 127.24it/s]\u001b[A\u001b[A\n",
      " 61%|███████████████████████▏              | 1605/2629 [00:12<00:08, 118.64it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████                 | 1460/2629 [00:11<00:07, 162.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▌              | 1633/2629 [00:12<00:07, 126.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|████████████████████████▋             | 1708/2629 [00:12<00:07, 126.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████▍              | 1647/2629 [00:12<00:10, 95.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|███████████████████████                | 1554/2629 [00:12<00:13, 79.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▌             | 1723/2629 [00:12<00:09, 97.61it/s]\u001b[A\u001b[A\u001b[A\n",
      " 62%|████████████████████████               | 1619/2629 [00:12<00:13, 73.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▌                | 1525/2629 [00:12<00:12, 91.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|████████████████████████▌              | 1658/2629 [00:12<00:09, 97.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 60%|███████████████████████▏               | 1567/2629 [00:12<00:12, 86.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|█████████████████████████             | 1738/2629 [00:12<00:08, 106.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▏             | 1671/2629 [00:12<00:09, 101.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|██████████████████████▏                | 1492/2629 [00:12<00:12, 94.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 60%|███████████████████████▍               | 1583/2629 [00:12<00:10, 99.69it/s]\u001b[A\u001b[A\n",
      " 62%|████████████████████████▏              | 1630/2629 [00:12<00:14, 69.17it/s]\u001b[A\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▎            | 1753/2629 [00:12<00:07, 114.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|██████████████████████▊                | 1536/2629 [00:12<00:13, 79.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████████████████████▊                | 1511/2629 [00:12<00:10, 108.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 64%|████████████████████████▉              | 1683/2629 [00:12<00:09, 99.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▋            | 1774/2629 [00:12<00:06, 134.36it/s]\u001b[A\u001b[A\u001b[A\n",
      " 62%|████████████████████████▎              | 1639/2629 [00:12<00:15, 64.98it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|███████████████████████████            | 1823/2629 [00:12<00:09, 84.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▌             | 1696/2629 [00:12<00:08, 106.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▉            | 1793/2629 [00:12<00:05, 146.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|██████████████████████▉                | 1546/2629 [00:12<00:15, 70.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 62%|███████████████████████▎              | 1617/2629 [00:12<00:08, 114.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▋             | 1710/2629 [00:12<00:08, 113.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      " 62%|███████████████████████▋              | 1635/2629 [00:12<00:07, 126.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|███████████████████████                | 1555/2629 [00:12<00:15, 68.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▏           | 1835/2629 [00:12<00:10, 78.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|████████████████████████▉             | 1723/2629 [00:13<00:07, 115.76it/s]\u001b[A\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▋           | 1846/2629 [00:13<00:04, 195.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▋               | 1571/2629 [00:13<00:07, 140.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|███████████████████████▎               | 1570/2629 [00:13<00:12, 84.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▎           | 1845/2629 [00:12<00:09, 80.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|█████████████████████████             | 1735/2629 [00:13<00:08, 109.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|██████████████████████████▉           | 1867/2629 [00:13<00:03, 190.65it/s]\u001b[A\u001b[A\u001b[A\n",
      " 63%|████████████████████████▌              | 1654/2629 [00:13<00:19, 51.14it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|███████████████████████▍               | 1583/2629 [00:13<00:11, 93.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████████████████████▉               | 1587/2629 [00:13<00:08, 129.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▎            | 1747/2629 [00:13<00:08, 107.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▎          | 1888/2629 [00:13<00:03, 189.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 63%|████████████████████████              | 1663/2629 [00:13<00:09, 102.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▏              | 1604/2629 [00:13<00:07, 135.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████▊           | 1872/2629 [00:13<00:07, 98.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 63%|████████████████████████▋              | 1660/2629 [00:13<00:20, 47.49it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▋               | 1594/2629 [00:13<00:12, 84.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▍            | 1760/2629 [00:13<00:07, 112.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 64%|████████████████████████▎             | 1683/2629 [00:13<00:07, 122.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▏          | 1884/2629 [00:13<00:07, 100.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 67%|█████████████████████████▋            | 1773/2629 [00:13<00:07, 116.40it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|███████████████████████▍              | 1619/2629 [00:13<00:08, 120.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 65%|████████████████████████▌             | 1700/2629 [00:13<00:07, 129.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▊               | 1604/2629 [00:13<00:12, 79.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▊          | 1928/2629 [00:13<00:04, 148.42it/s]\u001b[A\u001b[A\u001b[A\n",
      " 64%|████████████████████████▊              | 1676/2629 [00:13<00:17, 54.82it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████████████▏          | 1896/2629 [00:13<00:08, 90.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▊            | 1785/2629 [00:13<00:08, 102.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|███████████████████████▉               | 1613/2629 [00:13<00:13, 77.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 65%|████████████████████████▊             | 1714/2629 [00:13<00:07, 122.69it/s]\u001b[A\u001b[A\n",
      " 64%|█████████████████████████              | 1687/2629 [00:13<00:14, 66.86it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████████████▎          | 1908/2629 [00:13<00:07, 95.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|███████████████████████▊              | 1647/2629 [00:13<00:08, 119.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|████████████████████████████          | 1945/2629 [00:13<00:04, 139.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▋            | 1796/2629 [00:13<00:08, 98.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|████████████████████████▉             | 1727/2629 [00:13<00:08, 104.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|█████████████████████████▏             | 1695/2629 [00:13<00:14, 65.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|███████████████████████▉              | 1660/2629 [00:13<00:07, 121.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▎         | 1961/2629 [00:13<00:04, 142.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▊            | 1807/2629 [00:13<00:08, 91.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 66%|█████████████████████████▏            | 1740/2629 [00:13<00:08, 109.38it/s]\u001b[A\u001b[A\n",
      " 65%|█████████████████████████▎             | 1703/2629 [00:13<00:13, 67.75it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████▎             | 1680/2629 [00:13<00:06, 142.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████          | 1940/2629 [00:13<00:05, 117.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▌         | 1979/2629 [00:13<00:04, 150.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▉            | 1817/2629 [00:14<00:09, 86.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 65%|█████████████████████████▍             | 1714/2629 [00:14<00:11, 76.81it/s]\u001b[A\n",
      "\n",
      " 67%|█████████████████████████▎            | 1752/2629 [00:14<00:08, 107.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▌             | 1700/2629 [00:13<00:06, 152.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▏         | 1953/2629 [00:13<00:05, 115.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|████████████████████████████▊         | 1995/2629 [00:14<00:04, 146.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▏           | 1830/2629 [00:14<00:08, 96.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 67%|█████████████████████████▌            | 1770/2629 [00:14<00:06, 124.84it/s]\u001b[A\u001b[A\n",
      " 66%|█████████████████████████▋             | 1729/2629 [00:14<00:09, 93.60it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▍         | 1968/2629 [00:14<00:05, 121.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████         | 2011/2629 [00:14<00:04, 147.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▊             | 1716/2629 [00:14<00:06, 138.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▋           | 1843/2629 [00:14<00:07, 103.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 68%|█████████████████████████▊            | 1786/2629 [00:14<00:06, 133.39it/s]\u001b[A\u001b[A\n",
      " 66%|█████████████████████████▊             | 1739/2629 [00:14<00:09, 94.83it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▋         | 1981/2629 [00:14<00:05, 121.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████             | 1736/2629 [00:14<00:05, 154.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▎        | 2027/2629 [00:14<00:04, 140.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|██████████████████████████▉           | 1861/2629 [00:14<00:06, 121.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 67%|█████████████████████████▎            | 1754/2629 [00:14<00:08, 107.80it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████▊         | 1994/2629 [00:14<00:05, 118.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|█████████████████████████▎            | 1754/2629 [00:14<00:05, 160.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▌        | 2043/2629 [00:14<00:04, 143.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████           | 1874/2629 [00:14<00:06, 120.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 67%|█████████████████████████▌            | 1768/2629 [00:14<00:07, 114.35it/s]\u001b[A\n",
      "\n",
      " 68%|██████████████████████████▋            | 1800/2629 [00:14<00:08, 99.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████▉         | 2006/2629 [00:14<00:05, 116.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 68%|█████████████████████████▋            | 1781/2629 [00:14<00:04, 186.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▊        | 2063/2629 [00:14<00:03, 155.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|████████████████████████▋             | 1706/2629 [00:14<00:09, 101.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████████████████████████▏           | 1813/2629 [00:14<00:07, 105.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████        | 2079/2629 [00:14<00:03, 146.53it/s]\u001b[A\u001b[A\u001b[A\n",
      " 72%|███████████████████████████▉           | 1887/2629 [00:14<00:08, 92.10it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▉         | 2018/2629 [00:14<00:06, 93.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|█████████████████████████▍             | 1717/2629 [00:14<00:11, 80.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████            | 1801/2629 [00:14<00:06, 125.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|████████████████████████████▏          | 1898/2629 [00:14<00:07, 94.16it/s]\u001b[A\u001b[A\n",
      " 68%|██████████████████████████▌            | 1791/2629 [00:14<00:09, 86.35it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|██████████████████████████████         | 2029/2629 [00:14<00:07, 85.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▎       | 2094/2629 [00:14<00:04, 111.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▋          | 1912/2629 [00:14<00:06, 103.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▎           | 1817/2629 [00:14<00:06, 126.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 69%|██████████████████████████▋            | 1801/2629 [00:14<00:09, 85.40it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|█████████████████████████▊             | 1743/2629 [00:14<00:09, 98.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████████████████████████▏           | 1836/2629 [00:14<00:10, 77.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▍       | 2107/2629 [00:14<00:04, 110.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▊          | 1926/2629 [00:15<00:06, 112.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▍           | 1833/2629 [00:14<00:05, 132.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 69%|██████████████████████████▊            | 1810/2629 [00:15<00:09, 81.99it/s]\u001b[A\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▋       | 2120/2629 [00:15<00:04, 114.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████████████             | 1754/2629 [00:14<00:09, 95.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 74%|████████████████████████████          | 1939/2629 [00:15<00:06, 111.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▍        | 2048/2629 [00:15<00:07, 73.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████▋           | 1848/2629 [00:15<00:06, 125.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 69%|██████████████████████████▉            | 1820/2629 [00:15<00:09, 84.71it/s]\u001b[A\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▊       | 2133/2629 [00:15<00:04, 115.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████████████████████████▌           | 1857/2629 [00:15<00:09, 85.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▏         | 1951/2629 [00:15<00:06, 105.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████▌        | 2061/2629 [00:15<00:06, 84.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|██████████████████████████▉           | 1862/2629 [00:15<00:05, 128.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 70%|███████████████████████████▏           | 1830/2629 [00:15<00:09, 87.08it/s]\u001b[A\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████       | 2147/2629 [00:15<00:04, 120.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████████████████████████▋           | 1870/2629 [00:15<00:08, 94.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▎            | 1775/2629 [00:15<00:09, 94.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▎         | 1963/2629 [00:15<00:06, 107.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▎          | 1887/2629 [00:15<00:04, 157.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 70%|██████████████████████████▋           | 1845/2629 [00:15<00:07, 100.57it/s]\u001b[A\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▏      | 2162/2629 [00:15<00:03, 125.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████████████████████████▏          | 1883/2629 [00:15<00:07, 102.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▍            | 1785/2629 [00:15<00:09, 93.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▉        | 2083/2629 [00:15<00:05, 94.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████▌          | 1904/2629 [00:15<00:04, 157.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 71%|██████████████████████████▉           | 1861/2629 [00:15<00:06, 116.29it/s]\u001b[A\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▍      | 2175/2629 [00:15<00:03, 126.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████████████████████████▍          | 1894/2629 [00:15<00:07, 103.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████████████▋            | 1796/2629 [00:15<00:08, 92.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████▋         | 1988/2629 [00:15<00:06, 101.20it/s]\n",
      "\n",
      "\n",
      " 80%|███████████████████████████████        | 2093/2629 [00:15<00:06, 86.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████████████████████████▌          | 1905/2629 [00:15<00:06, 104.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▊          | 1921/2629 [00:15<00:05, 132.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 71%|███████████████████████████▊           | 1873/2629 [00:15<00:08, 89.45it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████▉         | 1999/2629 [00:15<00:06, 102.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 73%|████████████████████████████▍          | 1916/2629 [00:15<00:07, 98.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▊            | 1806/2629 [00:15<00:11, 73.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████████████████████████▉          | 1936/2629 [00:15<00:05, 121.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▊      | 2204/2629 [00:15<00:04, 106.08it/s]\u001b[A\u001b[A\u001b[A\n",
      " 77%|█████████████████████████████▏        | 2015/2629 [00:15<00:05, 117.60it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████▍       | 2118/2629 [00:15<00:05, 97.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████████████████████████▉          | 1929/2629 [00:15<00:06, 105.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████████████████████████▉            | 1818/2629 [00:15<00:09, 83.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▏         | 1952/2629 [00:15<00:05, 129.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▍        | 2037/2629 [00:15<00:04, 144.25it/s]\u001b[A\u001b[A\u001b[A\n",
      " 72%|███████████████████████████▍          | 1901/2629 [00:15<00:06, 106.99it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▊       | 2134/2629 [00:15<00:04, 111.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 74%|████████████████████████████          | 1940/2629 [00:15<00:06, 103.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████████████▏           | 1832/2629 [00:15<00:08, 96.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▎     | 2232/2629 [00:15<00:03, 119.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▍         | 1968/2629 [00:15<00:04, 135.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 73%|███████████████████████████▋          | 1914/2629 [00:16<00:06, 112.35it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████       | 2152/2629 [00:15<00:03, 129.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 74%|████████████████████████████▏         | 1952/2629 [00:16<00:06, 107.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▋        | 2052/2629 [00:16<00:04, 126.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▌     | 2253/2629 [00:16<00:02, 141.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 74%|███████████████████████████▉          | 1934/2629 [00:16<00:05, 130.95it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▍      | 2177/2629 [00:16<00:02, 157.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 75%|████████████████████████████▎         | 1963/2629 [00:16<00:06, 105.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████████████           | 1876/2629 [00:16<00:05, 141.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████▊        | 2066/2629 [00:16<00:04, 118.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▊     | 2268/2629 [00:16<00:02, 141.61it/s]\u001b[A\u001b[A\u001b[A\n",
      " 74%|████████████████████████████▏         | 1948/2629 [00:16<00:05, 132.90it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▋      | 2195/2629 [00:16<00:02, 163.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 75%|████████████████████████████▌         | 1978/2629 [00:16<00:05, 118.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████        | 2083/2629 [00:16<00:04, 131.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▎        | 2025/2629 [00:16<00:03, 161.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████     | 2286/2629 [00:16<00:02, 148.47it/s]\u001b[A\u001b[A\u001b[A\n",
      " 75%|████████████████████████████▎         | 1963/2629 [00:16<00:04, 136.03it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████      | 2218/2629 [00:16<00:02, 181.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 76%|████████████████████████████▊         | 1992/2629 [00:16<00:05, 121.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▌          | 1910/2629 [00:16<00:05, 143.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▌        | 2042/2629 [00:16<00:03, 161.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 75%|████████████████████████████▋         | 1981/2629 [00:16<00:04, 147.73it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▍     | 2241/2629 [00:16<00:01, 194.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▎    | 2302/2629 [00:16<00:02, 135.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████▌       | 2118/2629 [00:16<00:03, 150.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████▉          | 1929/2629 [00:16<00:04, 153.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▊        | 2061/2629 [00:16<00:03, 166.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 76%|████████████████████████████▉         | 2003/2629 [00:16<00:03, 167.02it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▊     | 2269/2629 [00:16<00:01, 216.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▍    | 2316/2629 [00:16<00:02, 136.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████▉       | 2140/2629 [00:16<00:02, 168.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████████████▏         | 1954/2629 [00:16<00:03, 178.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████        | 2082/2629 [00:16<00:03, 178.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 77%|█████████████████████████████▏        | 2020/2629 [00:16<00:03, 167.04it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████     | 2291/2629 [00:16<00:01, 209.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▋    | 2330/2629 [00:16<00:02, 132.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 78%|█████████████████████████████▋        | 2051/2629 [00:16<00:03, 166.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▏      | 2158/2629 [00:16<00:02, 160.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 77%|█████████████████████████████▍        | 2037/2629 [00:16<00:03, 166.90it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▌    | 2320/2629 [00:16<00:01, 229.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████▌         | 1973/2629 [00:16<00:04, 158.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 2346/2629 [00:16<00:02, 139.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 79%|██████████████████████████████        | 2083/2629 [00:16<00:02, 203.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▍      | 2175/2629 [00:16<00:02, 155.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 78%|█████████████████████████████▊        | 2061/2629 [00:16<00:03, 178.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 2348/2629 [00:16<00:01, 240.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|████████████████████████████▊         | 1990/2629 [00:16<00:04, 157.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▏   | 2367/2629 [00:16<00:01, 156.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████▍       | 2104/2629 [00:16<00:02, 190.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▋      | 2191/2629 [00:17<00:02, 146.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▍   | 2385/2629 [00:16<00:01, 161.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|█████████████████████████████         | 2007/2629 [00:16<00:04, 148.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████▋       | 2124/2629 [00:17<00:02, 192.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▎   | 2373/2629 [00:16<00:01, 203.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 79%|██████████████████████████████        | 2079/2629 [00:17<00:03, 139.37it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▉      | 2208/2629 [00:17<00:02, 151.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▋   | 2402/2629 [00:17<00:01, 161.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████▎        | 2026/2629 [00:17<00:03, 159.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 82%|██████████████████████████████▉       | 2144/2629 [00:17<00:02, 177.40it/s]\u001b[A\u001b[A\n",
      " 80%|██████████████████████████████▎       | 2101/2629 [00:17<00:03, 157.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▌   | 2395/2629 [00:17<00:01, 190.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▏     | 2226/2629 [00:17<00:02, 159.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████   | 2422/2629 [00:17<00:01, 171.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████▋        | 2050/2629 [00:17<00:03, 177.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▉   | 2415/2629 [00:17<00:01, 187.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 81%|██████████████████████████████▋       | 2119/2629 [00:17<00:03, 152.47it/s]\u001b[A\n",
      "\n",
      " 82%|███████████████████████████████▏      | 2162/2629 [00:17<00:02, 161.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▍     | 2243/2629 [00:17<00:02, 140.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████▊      | 2204/2629 [00:17<00:02, 157.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████▉        | 2069/2629 [00:17<00:03, 162.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 81%|██████████████████████████████▊       | 2136/2629 [00:17<00:03, 156.03it/s]\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▋     | 2260/2629 [00:17<00:02, 146.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 83%|███████████████████████████████▍      | 2179/2629 [00:17<00:03, 144.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████▏       | 2086/2629 [00:17<00:03, 155.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▏  | 2435/2629 [00:17<00:01, 152.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████      | 2221/2629 [00:17<00:02, 142.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 82%|███████████████████████████████       | 2153/2629 [00:17<00:03, 142.94it/s]\u001b[A\n",
      "\n",
      " 84%|███████████████████████████████▊      | 2197/2629 [00:17<00:02, 151.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|████████████████████████████████▉     | 2276/2629 [00:17<00:02, 137.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████▍       | 2103/2629 [00:17<00:03, 158.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▎     | 2237/2629 [00:17<00:02, 139.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▍  | 2452/2629 [00:17<00:01, 142.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 82%|███████████████████████████████▎      | 2168/2629 [00:17<00:03, 139.99it/s]\u001b[A\n",
      "\n",
      " 84%|████████████████████████████████      | 2214/2629 [00:17<00:02, 156.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████▏    | 2293/2629 [00:17<00:02, 145.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▌     | 2252/2629 [00:17<00:02, 138.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▋       | 2120/2629 [00:17<00:03, 138.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 83%|███████████████████████████████▋      | 2190/2629 [00:17<00:02, 159.24it/s]\u001b[A\n",
      "\n",
      " 85%|████████████████████████████████▎     | 2235/2629 [00:17<00:02, 169.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 2468/2629 [00:17<00:01, 132.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▎    | 2309/2629 [00:17<00:02, 149.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▊     | 2268/2629 [00:17<00:02, 143.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▋    | 2332/2629 [00:17<00:01, 170.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▍ | 2522/2629 [00:17<00:00, 144.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▉  | 2482/2629 [00:17<00:01, 128.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████▊       | 2135/2629 [00:17<00:04, 120.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████     | 2287/2629 [00:17<00:02, 152.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 85%|████████████████████████████████▎     | 2237/2629 [00:17<00:02, 191.77it/s]\u001b[A\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 2352/2629 [00:18<00:01, 175.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████  | 2496/2629 [00:17<00:01, 126.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▋ | 2537/2629 [00:17<00:00, 136.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████       | 2149/2629 [00:17<00:03, 122.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▎    | 2303/2629 [00:17<00:02, 145.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 87%|█████████████████████████████████▏    | 2298/2629 [00:18<00:01, 187.95it/s]\u001b[A\u001b[A\n",
      " 90%|██████████████████████████████████▎   | 2370/2629 [00:18<00:01, 170.27it/s]\u001b[A\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▉ | 2559/2629 [00:18<00:00, 156.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████▏      | 2162/2629 [00:18<00:03, 120.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▌    | 2318/2629 [00:18<00:02, 145.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 88%|█████████████████████████████████▌    | 2319/2629 [00:18<00:01, 192.91it/s]\u001b[A\u001b[A\n",
      " 91%|██████████████████████████████████▌   | 2390/2629 [00:18<00:01, 177.25it/s]\u001b[A\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▎| 2580/2629 [00:18<00:00, 171.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████████████▌      | 2182/2629 [00:18<00:03, 140.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▋    | 2333/2629 [00:18<00:02, 141.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 2347/2629 [00:18<00:01, 210.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▊   | 2408/2629 [00:18<00:01, 171.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▌| 2598/2629 [00:18<00:00, 164.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 2349/2629 [00:18<00:01, 144.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 90%|██████████████████████████████████▎   | 2371/2629 [00:18<00:01, 215.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▏  | 2432/2629 [00:18<00:01, 188.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|█████████████████████████████████████▌ | 2532/2629 [00:18<00:00, 99.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▊| 2615/2629 [00:18<00:00, 164.75it/s]\u001b[A\u001b[A\u001b[A\n",
      " 87%|█████████████████████████████████▏    | 2295/2629 [00:18<00:02, 125.94it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▏   | 2364/2629 [00:18<00:01, 143.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 91%|██████████████████████████████████▋   | 2399/2629 [00:18<00:00, 232.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 2629/2629 [00:18<00:00, 142.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▊ | 2543/2629 [00:18<00:00, 100.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▍   | 2379/2629 [00:18<00:01, 144.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 88%|█████████████████████████████████▍    | 2310/2629 [00:18<00:02, 119.77it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████▏     | 2226/2629 [00:18<00:03, 130.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 2473/2629 [00:18<00:00, 189.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▉ | 2554/2629 [00:18<00:00, 100.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▋   | 2396/2629 [00:18<00:01, 150.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 89%|█████████████████████████████████▋    | 2328/2629 [00:18<00:02, 131.63it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████  | 2496/2629 [00:18<00:00, 197.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▎| 2578/2629 [00:18<00:00, 135.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 93%|███████████████████████████████████▎  | 2446/2629 [00:18<00:00, 190.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▊   | 2412/2629 [00:18<00:01, 148.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████▋     | 2265/2629 [00:18<00:02, 156.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 96%|████████████████████████████████████▍ | 2522/2629 [00:18<00:00, 213.97it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▍| 2592/2629 [00:18<00:00, 132.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 2467/2629 [00:18<00:00, 194.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|█████████████████████████████████     | 2287/2629 [00:18<00:01, 174.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▊ | 2544/2629 [00:18<00:00, 210.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 90%|██████████████████████████████████    | 2358/2629 [00:18<00:02, 121.95it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████▋| 2607/2629 [00:18<00:00, 136.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 95%|███████████████████████████████████▉  | 2488/2629 [00:18<00:00, 197.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|█████████████████████████████████▍    | 2310/2629 [00:18<00:01, 184.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▍  | 2451/2629 [00:18<00:01, 158.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 98%|█████████████████████████████████████ | 2566/2629 [00:19<00:00, 198.07it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████▉| 2621/2629 [00:18<00:00, 127.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 95%|████████████████████████████████████▎ | 2509/2629 [00:19<00:00, 191.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 2629/2629 [00:19<00:00, 138.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▋  | 2467/2629 [00:19<00:01, 147.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 99%|█████████████████████████████████████▍| 2590/2629 [00:19<00:00, 208.45it/s]\u001b[A\n",
      "\n",
      " 97%|████████████████████████████████████▋ | 2540/2629 [00:19<00:00, 223.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████▉    | 2348/2629 [00:19<00:01, 169.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▉  | 2482/2629 [00:19<00:01, 141.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 99%|█████████████████████████████████████▊| 2614/2629 [00:19<00:00, 214.88it/s]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████| 2629/2629 [00:19<00:00, 135.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████▎ | 2510/2629 [00:19<00:00, 177.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 90%|██████████████████████████████████▏   | 2366/2629 [00:19<00:01, 159.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 92%|███████████████████████████████████   | 2428/2629 [00:19<00:01, 152.82it/s]\u001b[A\n",
      "\n",
      " 99%|█████████████████████████████████████▍| 2590/2629 [00:19<00:00, 222.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▋ | 2534/2629 [00:19<00:00, 193.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████▌   | 2391/2629 [00:19<00:01, 183.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 93%|███████████████████████████████████▎  | 2447/2629 [00:19<00:01, 162.73it/s]\u001b[A\n",
      "\n",
      " 99%|█████████████████████████████████████▊| 2613/2629 [00:19<00:00, 214.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████▉ | 2556/2629 [00:19<00:00, 197.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████▊   | 2410/2629 [00:19<00:01, 182.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████| 2629/2629 [00:19<00:00, 134.00it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▍| 2589/2629 [00:19<00:00, 233.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████   | 2430/2629 [00:19<00:01, 187.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 95%|███████████████████████████████████▉  | 2488/2629 [00:19<00:00, 180.58it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████▊| 2619/2629 [00:19<00:00, 250.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████▍  | 2451/2629 [00:19<00:00, 192.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████| 2629/2629 [00:19<00:00, 133.06it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████▊  | 2477/2629 [00:19<00:00, 212.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 97%|████████████████████████████████████▋ | 2537/2629 [00:19<00:00, 210.42it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████▋ | 2535/2629 [00:19<00:00, 318.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 98%|█████████████████████████████████████▏| 2571/2629 [00:20<00:00, 246.27it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████▎| 2580/2629 [00:20<00:00, 353.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████| 2629/2629 [00:20<00:00, 130.17it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████| 2629/2629 [00:20<00:00, 130.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_abolish.tsv\n",
      "Processing Started...\n",
      "Data Size:  281\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 78%|████████████████████████████████▌         | 31/40 [00:00<00:00, 307.68it/s]\n",
      " 72%|██████████████████████████████▍           | 29/40 [00:00<00:00, 286.50it/s]\u001b[A\n",
      "\n",
      " 55%|███████████████████████                   | 22/40 [00:00<00:00, 217.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 297.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|█████████████████████████████▍            | 28/40 [00:00<00:00, 278.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 260.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 238.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 221.56it/s]\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 223.12it/s]\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 230.21it/s]\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 202.56it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_abolish.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_abolish.tsv\n",
      "Processing Started...\n",
      "Data Size:  283\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 42%|█████████████████▊                        | 17/40 [00:00<00:00, 164.92it/s]\n",
      " 48%|███████████████████▉                      | 19/40 [00:00<00:00, 182.80it/s]\u001b[A\n",
      "\n",
      " 52%|██████████████████████                    | 21/40 [00:00<00:00, 196.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|██████████████▋                           | 14/40 [00:00<00:00, 134.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|███████████████▊                          | 15/40 [00:00<00:00, 149.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|██████████████▋                           | 14/40 [00:00<00:00, 136.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 198.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 174.15it/s]\n",
      "\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▍           | 29/40 [00:00<00:00, 142.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|████████████████████████████████▌         | 31/40 [00:00<00:00, 150.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|█████████████████████████████▍            | 28/40 [00:00<00:00, 133.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 199.19it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 145.54it/s]\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 165.85it/s]\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 142.66it/s]\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 151.42it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_abolish.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_alter.tsv\n",
      "Processing Started...\n",
      "Data Size:  254\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 58%|████████████████████████▌                 | 21/36 [00:00<00:00, 209.89it/s]\n",
      " 67%|████████████████████████████              | 24/36 [00:00<00:00, 233.02it/s]\u001b[A\n",
      "\n",
      " 67%|████████████████████████████              | 24/36 [00:00<00:00, 238.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|█████████████████████████▋                | 22/36 [00:00<00:00, 218.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 219.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 222.04it/s]\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 182.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 214.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 210.34it/s]\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 216.72it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 145.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_alter.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_alter.tsv\n",
      "Processing Started...\n",
      "Data Size:  255\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 47%|███████████████████▊                      | 17/36 [00:00<00:00, 168.08it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 61%|█████████████████████████▋                | 22/36 [00:00<00:00, 206.61it/s]\u001b[A\n",
      "\n",
      " 50%|█████████████████████                     | 18/36 [00:00<00:00, 170.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/36 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 42%|█████████████████▌                        | 15/36 [00:00<00:00, 133.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 183.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 180.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████████▉                               | 10/36 [00:00<00:00, 93.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 149.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 175.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 173.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 144.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 158.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_alter.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_begin.tsv\n",
      "Processing Started...\n",
      "Data Size:  683\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 16%|██████▉                                   | 16/97 [00:00<00:00, 145.26it/s]\n",
      "\n",
      "  8%|███▋                                        | 8/97 [00:00<00:01, 76.36it/s]\u001b[A\u001b[A\n",
      " 11%|████▉                                      | 11/97 [00:00<00:00, 97.02it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▉                                      | 11/97 [00:00<00:00, 96.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|█████▋                                    | 13/97 [00:00<00:00, 109.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▊                                     | 11/97 [00:00<00:00, 101.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 16%|███████                                    | 16/97 [00:00<00:01, 77.97it/s]\u001b[A\u001b[A\n",
      " 32%|█████████████▍                            | 31/97 [00:00<00:00, 105.68it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▉                                | 23/97 [00:00<00:00, 105.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|██▎                                         | 5/97 [00:00<00:02, 44.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|█████████▌                                | 22/97 [00:00<00:00, 103.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██████████▋                                | 24/97 [00:00<00:00, 91.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 25%|██████████▋                                | 24/97 [00:00<00:00, 76.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|███████████████▏                          | 35/97 [00:00<00:00, 111.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▌                                   | 17/97 [00:00<00:00, 85.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 34%|██████████████▋                            | 33/97 [00:00<00:00, 90.43it/s]\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▌                           | 35/97 [00:00<00:00, 97.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|██████████████▋                            | 33/97 [00:00<00:00, 95.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 36%|███████████████▌                           | 35/97 [00:00<00:00, 87.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████████▍                              | 28/97 [00:00<00:00, 95.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 60%|█████████████████████████                 | 58/97 [00:00<00:00, 111.41it/s]\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████████████████████▋                    | 50/97 [00:00<00:00, 116.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|███████████████████                        | 43/97 [00:00<00:00, 94.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 46%|███████████████████▉                       | 45/97 [00:00<00:00, 91.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|█████████████████████████▉                | 60/97 [00:00<00:00, 116.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|████████████████▊                          | 38/97 [00:00<00:00, 92.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 72%|██████████████████████████████▎           | 70/97 [00:00<00:00, 112.70it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|████████████████████████▏                 | 56/97 [00:00<00:00, 106.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|██████████████████████████▊               | 62/97 [00:00<00:00, 111.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████████████████████████                 | 58/97 [00:00<00:00, 103.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████████████████████████████▏          | 72/97 [00:00<00:00, 115.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████████████████████████                 | 58/97 [00:00<00:00, 129.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 70%|█████████████████████████████▍            | 68/97 [00:00<00:00, 107.58it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|██████████████████████████████▋           | 71/97 [00:00<00:00, 118.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|███████████████████████████████████▌      | 82/97 [00:00<00:00, 106.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 73%|██████████████████████████████▋           | 71/97 [00:00<00:00, 112.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████████    | 88/97 [00:00<00:00, 128.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████████████████████████████▌          | 73/97 [00:00<00:00, 133.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 126.02it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 123.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 112.47it/s]\n",
      "\n",
      "\n",
      " 87%|████████████████████████████████████▎     | 84/97 [00:00<00:00, 112.29it/s]\u001b[A\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 94/97 [00:00<00:00, 116.56it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 114.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 106.52it/s]\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 106.88it/s]\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 118.99it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_begin.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_begin.tsv\n",
      "Processing Started...\n",
      "Data Size:  684\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 12%|█████▏                                    | 12/97 [00:00<00:00, 119.69it/s]\n",
      " 10%|████▍                                      | 10/97 [00:00<00:00, 99.27it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/97 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      " 11%|████▊                                     | 11/97 [00:00<00:00, 103.38it/s]\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████▋                                    | 13/97 [00:00<00:00, 128.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▊                                     | 11/97 [00:00<00:00, 109.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██████████▊                               | 25/97 [00:00<00:00, 125.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|██████                                    | 14/97 [00:00<00:00, 136.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 22%|█████████                                 | 21/97 [00:00<00:00, 100.66it/s]\u001b[A\n",
      "\n",
      " 25%|██████████▍                               | 24/97 [00:00<00:00, 109.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|███████████▎                              | 26/97 [00:00<00:00, 114.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|████████████████▉                         | 39/97 [00:00<00:00, 129.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|████████████▉                             | 30/97 [00:00<00:00, 134.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 35%|██████████████▋                           | 34/97 [00:00<00:00, 107.61it/s]\u001b[A\n",
      "\n",
      " 36%|███████████████▏                          | 35/97 [00:00<00:00, 105.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████████                              | 28/97 [00:00<00:00, 114.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|██████████████████████▉                   | 53/97 [00:00<00:00, 131.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|█████████████████▎                        | 40/97 [00:00<00:00, 117.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|███████████████████                       | 44/97 [00:00<00:00, 130.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 52%|█████████████████████▋                    | 50/97 [00:00<00:00, 126.55it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|█████████████████▎                        | 40/97 [00:00<00:00, 111.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|█████████████████████████████             | 67/97 [00:00<00:00, 133.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|███████████████████████▊                  | 55/97 [00:00<00:00, 127.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|█████████████████████████▉                | 60/97 [00:00<00:00, 139.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|███████████████████████████▋              | 64/97 [00:00<00:00, 130.12it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|██████████████████████▉                   | 53/97 [00:00<00:00, 117.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|████████████████████████▋                 | 57/97 [00:00<00:00, 101.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|█████████████████████████████▉            | 69/97 [00:00<00:00, 125.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|█████████████████████████████▍            | 68/97 [00:00<00:00, 125.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████████       | 81/97 [00:00<00:00, 124.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 80%|█████████████████████████████████▊        | 78/97 [00:00<00:00, 130.61it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|█████████████████████████████▉            | 69/97 [00:00<00:00, 131.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████████████████████████████▏          | 72/97 [00:00<00:00, 116.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|████████████████████████████████████▎     | 84/97 [00:00<00:00, 131.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████████       | 81/97 [00:00<00:00, 121.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████████▋ | 94/97 [00:00<00:00, 116.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████████████████████████████████▎     | 84/97 [00:00<00:00, 135.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 123.46it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 130.40it/s]\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 138.11it/s]\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 121.14it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 128.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 121.98it/s]\n",
      "100%|██████████████████████████████████████████| 97/97 [00:00<00:00, 109.67it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_begin.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_block.tsv\n",
      "Processing Started...\n",
      "Data Size:  362\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 18%|███████▊                                    | 9/51 [00:00<00:00, 77.66it/s]\u001b[A\n",
      "\n",
      " 18%|███████▊                                    | 9/51 [00:00<00:00, 88.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▉                                     | 8/51 [00:00<00:00, 79.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▍                                  | 10/51 [00:00<00:00, 95.48it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|██████                                      | 7/51 [00:00<00:00, 66.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 33%|██████████████▎                            | 17/51 [00:00<00:00, 68.78it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▍                                        | 4/51 [00:00<00:01, 36.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 35%|███████████████▏                           | 18/51 [00:00<00:00, 65.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|█████████████▍                             | 16/51 [00:00<00:00, 58.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|████████████████▊                          | 20/51 [00:00<00:00, 71.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|█████████▎                                 | 11/51 [00:00<00:00, 52.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|███████████▊                               | 14/51 [00:00<00:00, 46.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 47%|████████████████████▏                      | 24/51 [00:00<00:00, 55.87it/s]\u001b[A\n",
      "\n",
      " 49%|█████████████████████                      | 25/51 [00:00<00:00, 62.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|███████████████████▍                       | 23/51 [00:00<00:00, 62.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|███████████████████████▌                   | 28/51 [00:00<00:00, 69.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|█████████████████▋                         | 21/51 [00:00<00:00, 68.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|██████████████████████████▉                | 32/51 [00:00<00:00, 62.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 78%|█████████████████████████████████▋         | 40/51 [00:00<00:00, 90.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|██████████████████████████████▎            | 36/51 [00:00<00:00, 85.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 57%|████████████████████████▍                  | 29/51 [00:00<00:00, 49.19it/s]\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 51/51 [00:00<00:00, 93.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|█████████████████████████▎                 | 30/51 [00:00<00:00, 76.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 51/51 [00:00<00:00, 93.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 51/51 [00:00<00:00, 91.82it/s]\n",
      "\n",
      "100%|███████████████████████████████████████████| 51/51 [00:00<00:00, 78.67it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████████▋         | 40/51 [00:00<00:00, 83.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 51/51 [00:00<00:00, 69.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 51/51 [00:00<00:00, 74.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 51/51 [00:00<00:00, 75.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_block.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_block.tsv\n",
      "Processing Started...\n",
      "Data Size:  363\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 27%|███████████▌                              | 14/51 [00:00<00:00, 138.15it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 25%|██████████▋                               | 13/51 [00:00<00:00, 127.68it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████████▎                             | 15/51 [00:00<00:00, 148.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 18%|███████▊                                    | 9/51 [00:00<00:00, 80.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|███████████████▋                          | 19/51 [00:00<00:00, 174.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▊                                    | 9/51 [00:00<00:00, 84.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 55%|███████████████████████                   | 28/51 [00:00<00:00, 113.74it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|████████▍                                  | 10/51 [00:00<00:00, 87.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|████████████████████████▋                 | 30/51 [00:00<00:00, 131.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 35%|███████████████▏                           | 18/51 [00:00<00:00, 74.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████                        | 22/51 [00:00<00:00, 103.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 82%|██████████████████████████████████▌       | 42/51 [00:00<00:00, 129.90it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|████████████████████████████████▉         | 40/51 [00:00<00:00, 103.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 131.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████████▏     | 44/51 [00:00<00:00, 129.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████████████████████████▎                 | 30/51 [00:00<00:00, 91.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|████████████████████████████▊             | 35/51 [00:00<00:00, 113.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 138.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 126.17it/s]\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 104.75it/s]\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████████▍      | 43/51 [00:00<00:00, 101.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 123.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 110.63it/s]\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 101.57it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_block.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_catalyse.tsv\n",
      "Processing Started...\n",
      "Data Size:  195\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 44%|██████████████████▋                       | 12/27 [00:00<00:00, 118.12it/s]\n",
      " 33%|██████████████▋                             | 9/27 [00:00<00:00, 88.09it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 30%|█████████████                               | 8/27 [00:00<00:00, 76.99it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      " 30%|█████████████                               | 8/27 [00:00<00:00, 77.47it/s]\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|████████▏                                   | 5/27 [00:00<00:00, 49.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|███████████▍                                | 7/27 [00:00<00:00, 60.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|███████████▍                                | 7/27 [00:00<00:00, 60.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|██████████████████████████████████████▏    | 24/27 [00:00<00:00, 83.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████████████████████████▍                 | 16/27 [00:00<00:00, 65.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|███████████████████                        | 12/27 [00:00<00:00, 58.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 67%|████████████████████████████▋              | 18/27 [00:00<00:00, 62.27it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 80.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|██████████████████████▎                    | 14/27 [00:00<00:00, 59.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|██████████████████████████████████████▏    | 24/27 [00:00<00:00, 67.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|██████████████████████████████▎            | 19/27 [00:00<00:00, 61.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 25/27 [00:00<00:00, 63.32it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 67.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 71.27it/s]\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 64.35it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 67.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 65.76it/s]\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 67.47it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_catalyse.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_catalyse.tsv\n",
      "Processing Started...\n",
      "Data Size:  196\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 32%|██████████████▏                             | 9/28 [00:00<00:00, 89.33it/s]\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 32%|██████████████▏                             | 9/28 [00:00<00:00, 80.77it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 36%|███████████████▎                           | 10/28 [00:00<00:00, 92.62it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▋               | 18/28 [00:00<00:00, 86.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 32%|██████████████▏                             | 9/28 [00:00<00:00, 89.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:00<00:00, 85.79it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|█████████▍                                  | 6/28 [00:00<00:00, 58.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:00<00:00, 92.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 96.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████▍                        | 12/28 [00:00<00:00, 52.28it/s]\u001b[A\u001b[A\u001b[A\n",
      " 96%|█████████████████████████████████████████▍ | 27/28 [00:00<00:00, 84.19it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 84.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 86.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|███████████████████▉                       | 13/28 [00:00<00:00, 60.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 82.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 75.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 71.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 68.90it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_catalyse.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_confer.tsv\n",
      "Processing Started...\n",
      "Data Size:  317\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 22%|█████████▌                                 | 10/45 [00:00<00:00, 88.83it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 24%|██████████▌                                | 11/45 [00:00<00:00, 96.19it/s]\u001b[A\n",
      "\n",
      " 20%|████████▊                                   | 9/45 [00:00<00:00, 84.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|███████▊                                    | 8/45 [00:00<00:00, 79.61it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▊                                     | 7/45 [00:00<00:00, 64.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|███████████████████                        | 20/45 [00:00<00:00, 91.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 47%|████████████████████                       | 21/45 [00:00<00:00, 87.38it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▊                                     | 7/45 [00:00<00:00, 67.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 40%|█████████████████▏                         | 18/45 [00:00<00:00, 80.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 36%|███████████████▎                           | 16/45 [00:00<00:00, 74.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|█████████████▍                             | 14/45 [00:00<00:00, 70.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|████████████████████████████▋              | 30/45 [00:00<00:00, 92.86it/s]\u001b[A\u001b[A\u001b[A\n",
      " 67%|████████████████████████████▋              | 30/45 [00:00<00:00, 78.64it/s]\u001b[A\n",
      "\n",
      " 60%|█████████████████████████▊                 | 27/45 [00:00<00:00, 72.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 53%|██████████████████████▉                    | 24/45 [00:00<00:00, 69.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|█████████████▍                             | 14/45 [00:00<00:00, 52.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|████████████████████████▊                  | 26/45 [00:00<00:00, 64.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|██████████████████████████████████████▏    | 40/45 [00:00<00:00, 79.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 84%|████████████████████████████████████▎      | 38/45 [00:00<00:00, 77.44it/s]\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████| 45/45 [00:00<00:00, 83.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|███████████████████                        | 20/45 [00:00<00:00, 49.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 45/45 [00:00<00:00, 82.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▋               | 29/45 [00:00<00:00, 58.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████| 45/45 [00:00<00:00, 78.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████████▌           | 33/45 [00:00<00:00, 50.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|████████████████████████▊                  | 26/45 [00:00<00:00, 50.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████▎      | 38/45 [00:00<00:00, 57.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 36/45 [00:00<00:00, 59.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|██████████████████████████████████████▏    | 40/45 [00:00<00:00, 54.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████████▌           | 33/45 [00:00<00:00, 54.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 45/45 [00:00<00:00, 62.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 45/45 [00:00<00:00, 61.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 45/45 [00:00<00:00, 60.54it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 45/45 [00:00<00:00, 62.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_confer.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_confer.tsv\n",
      "Processing Started...\n",
      "Data Size:  319\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 24%|██████████▎                               | 11/45 [00:00<00:00, 101.65it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 22%|█████████▌                                 | 10/45 [00:00<00:00, 99.43it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 18%|███████▊                                    | 8/45 [00:00<00:00, 73.61it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████▊                                      | 6/45 [00:00<00:00, 55.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▊                                    | 8/45 [00:00<00:00, 77.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▊                                    | 8/45 [00:00<00:00, 74.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|████████▊                                   | 9/45 [00:00<00:00, 81.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 36%|███████████████▎                           | 16/45 [00:00<00:00, 70.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|█████████████████████                      | 22/45 [00:00<00:00, 71.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|██████████████████▋                       | 20/45 [00:00<00:00, 100.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 44%|███████████████████                        | 20/45 [00:00<00:00, 71.47it/s]\u001b[A\n",
      "\n",
      "\n",
      " 27%|███████████▍                               | 12/45 [00:00<00:00, 45.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|███████████████████                        | 20/45 [00:00<00:00, 97.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████████████████████████████▋          | 34/45 [00:00<00:00, 114.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|█████████████████████████████████▌        | 36/45 [00:00<00:00, 123.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|████████████████████████████▋              | 30/45 [00:00<00:00, 66.70it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|██████████████████████▉                    | 24/45 [00:00<00:00, 58.56it/s]\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 45/45 [00:00<00:00, 121.45it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████| 45/45 [00:00<00:00, 113.60it/s]\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████████████████▎       | 37/45 [00:00<00:00, 66.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 36/45 [00:00<00:00, 76.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|████████████████████████████▋              | 30/45 [00:00<00:00, 70.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 45/45 [00:00<00:00, 71.66it/s]\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 45/45 [00:00<00:00, 74.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 91%|███████████████████████████████████████▏   | 41/45 [00:00<00:00, 87.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████▎      | 38/45 [00:00<00:00, 71.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 45/45 [00:00<00:00, 69.36it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 45/45 [00:00<00:00, 69.20it/s]\n",
      "100%|███████████████████████████████████████████| 45/45 [00:00<00:00, 70.45it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_confer.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_decrease.tsv\n",
      "Processing Started...\n",
      "Data Size:  195\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 48%|████████████████████▏                     | 13/27 [00:00<00:00, 120.73it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 41%|█████████████████▌                         | 11/27 [00:00<00:00, 93.37it/s]\u001b[A\n",
      "\n",
      " 22%|█████████▊                                  | 6/27 [00:00<00:00, 59.37it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 56%|███████████████████████▎                  | 15/27 [00:00<00:00, 147.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|████████████████████▏                     | 13/27 [00:00<00:00, 128.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|██████████████████████████▍               | 17/27 [00:00<00:00, 147.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|███████████████▉                           | 10/27 [00:00<00:00, 93.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 78%|█████████████████████████████████▍         | 21/27 [00:00<00:00, 89.41it/s]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 100.09it/s]\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 143.71it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 95.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 104.79it/s]\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 78.84it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████████████████████████████▊           | 20/27 [00:00<00:00, 80.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 86.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 56.66it/s]\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_decrease.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_decrease.tsv\n",
      "Processing Started...\n",
      "Data Size:  197\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 21%|█████████▍                                  | 6/28 [00:00<00:00, 59.35it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 25%|███████████                                 | 7/28 [00:00<00:00, 66.19it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|███████▊                                    | 5/28 [00:00<00:00, 42.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|███████████                                 | 7/28 [00:00<00:00, 67.78it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 43%|██████████████████▍                        | 12/28 [00:00<00:00, 54.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 50%|█████████████████████▌                     | 14/28 [00:00<00:00, 56.78it/s]\u001b[A\n",
      "\n",
      "\n",
      " 39%|████████████████▉                          | 11/28 [00:00<00:00, 51.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|████████████████████████▌                  | 16/28 [00:00<00:00, 80.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:00<00:00, 64.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 43%|██████████████████▍                        | 12/28 [00:00<00:00, 47.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███████████████▎                           | 10/28 [00:00<00:00, 90.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 75%|████████████████████████████████▎          | 21/28 [00:00<00:00, 60.83it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████████▉   | 26/28 [00:00<00:00, 88.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|██████████████████████████                 | 17/28 [00:00<00:00, 53.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 86.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 62.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 102.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 63.14it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 61.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 62.00it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_decrease.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_delete.tsv\n",
      "Processing Started...\n",
      "Data Size:  411\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████████▌                              | 16/58 [00:00<00:00, 150.66it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/58 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 26%|██████████▊                               | 15/58 [00:00<00:00, 149.24it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 22%|█████████▍                                | 13/58 [00:00<00:00, 116.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|███████▉                                  | 11/58 [00:00<00:00, 108.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|█████████▍                                | 13/58 [00:00<00:00, 124.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▉                                  | 11/58 [00:00<00:00, 105.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 55%|███████████████████████▏                  | 32/58 [00:00<00:00, 160.13it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|███████████████████████▏                  | 32/58 [00:00<00:00, 133.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████████████████████                     | 29/58 [00:00<00:00, 134.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████████████████▋                         | 23/58 [00:00<00:00, 110.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███████████████▉                          | 22/58 [00:00<00:00, 105.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|██████████████████▊                       | 26/58 [00:00<00:00, 111.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████████▎        | 46/58 [00:00<00:00, 123.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████████████████████████████▏          | 43/58 [00:00<00:00, 132.41it/s]\u001b[A\u001b[A\n",
      " 84%|███████████████████████████████████▍      | 49/58 [00:00<00:00, 135.81it/s]\u001b[A\n",
      "\n",
      "\n",
      " 67%|████████████████████████████▏             | 39/58 [00:00<00:00, 127.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|████████████████████████▌                 | 34/58 [00:00<00:00, 111.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|████████████████████████████▏             | 39/58 [00:00<00:00, 118.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▉                 | 35/58 [00:00<00:00, 98.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 58/58 [00:00<00:00, 125.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 58/58 [00:00<00:00, 110.71it/s]\n",
      "100%|██████████████████████████████████████████| 58/58 [00:00<00:00, 121.21it/s]\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 58/58 [00:00<00:00, 112.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████████████████████████████████▉     | 51/58 [00:00<00:00, 106.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████████▎        | 46/58 [00:00<00:00, 100.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 58/58 [00:00<00:00, 111.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 58/58 [00:00<00:00, 105.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 58/58 [00:00<00:00, 105.90it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_delete.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_delete.tsv\n",
      "Processing Started...\n",
      "Data Size:  413\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 15%|██████▋                                     | 9/59 [00:00<00:00, 88.55it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 17%|███████▎                                   | 10/59 [00:00<00:00, 88.31it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 19%|███████▊                                  | 11/59 [00:00<00:00, 109.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|█████▉                                      | 8/59 [00:00<00:00, 79.26it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 32%|█████████████▊                             | 19/59 [00:00<00:00, 91.08it/s]/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 32%|█████████████▊                             | 19/59 [00:00<00:00, 81.68it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|██████▋                                     | 9/59 [00:00<00:00, 79.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 41%|█████████████████                         | 24/59 [00:00<00:00, 110.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▉                                      | 8/59 [00:00<00:00, 77.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|███████████▋                               | 16/59 [00:00<00:00, 77.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|█████████████████████▏                     | 29/59 [00:00<00:00, 73.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|█████████████▊                             | 19/59 [00:00<00:00, 95.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 47%|████████████████████▍                      | 28/59 [00:00<00:00, 71.77it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|████████▋                                  | 12/59 [00:00<00:00, 55.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|███████████████████▋                       | 27/59 [00:00<00:00, 88.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|████████████████████████████▍              | 39/59 [00:00<00:00, 79.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|██████████████████████████▉                | 37/59 [00:00<00:00, 76.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████████▏       | 48/59 [00:00<00:00, 107.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███████████████▎                           | 21/59 [00:00<00:00, 68.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|██████████████████████████▏                | 36/59 [00:00<00:00, 84.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 59/59 [00:00<00:00, 109.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|█████████████████████████████▉            | 42/59 [00:00<00:00, 104.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 48/59 [00:00<00:00, 76.54it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 59/59 [00:00<00:00, 119.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████████████████▌         | 46/59 [00:00<00:00, 88.68it/s]\u001b[A\u001b[A\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 53/59 [00:00<00:00, 74.10it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|████████████████████████████████████████▊  | 56/59 [00:00<00:00, 75.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 59/59 [00:00<00:00, 78.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 59/59 [00:00<00:00, 101.69it/s]\n",
      "100%|███████████████████████████████████████████| 59/59 [00:00<00:00, 88.10it/s]\n",
      "100%|███████████████████████████████████████████| 59/59 [00:00<00:00, 77.35it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 59/59 [00:00<00:00, 84.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_delete.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_develop.tsv\n",
      "Processing Started...\n",
      "Data Size:  62\n",
      "number of threads:  7\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 234.83it/s]\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 225.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 204.31it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 152.36it/s]\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 166.51it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 94.73it/s]\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 110.39it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_develop.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_develop.tsv\n",
      "Processing Started...\n",
      "Data Size:  62\n",
      "number of threads:  7\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 144.89it/s]\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 201.71it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████████| 8/8 [00:00<00:00, 200.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 83.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 81.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 97.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:00<00:00, 65.29it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_develop.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_disrupt.tsv\n",
      "Processing Started...\n",
      "Data Size:  119\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 76%|████████████████████████████████          | 13/17 [00:00<00:00, 125.99it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 53%|███████████████████████▎                    | 9/17 [00:00<00:00, 87.90it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 127.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|███████████████████████▎                    | 9/17 [00:00<00:00, 86.82it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 53%|███████████████████████▎                    | 9/17 [00:00<00:00, 86.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████| 17/17 [00:00<00:00, 94.01it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 17/17 [00:00<00:00, 90.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 17/17 [00:00<00:00, 90.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 17/17 [00:00<00:00, 99.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 17/17 [00:00<00:00, 73.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 17/17 [00:00<00:00, 91.19it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_disrupt.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_disrupt.tsv\n",
      "Processing Started...\n",
      "Data Size:  120\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 53%|███████████████████████▎                    | 9/17 [00:00<00:00, 86.47it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 71%|█████████████████████████████▋            | 12/17 [00:00<00:00, 118.38it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 126.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████| 17/17 [00:00<00:00, 96.55it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|███████████████████████████▏              | 11/17 [00:00<00:00, 100.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 82%|███████████████████████████████████▍       | 14/17 [00:00<00:00, 66.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 17/17 [00:00<00:00, 74.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 108.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 17/17 [00:00<00:00, 80.35it/s]\n",
      "100%|██████████████████████████████████████████| 17/17 [00:00<00:00, 105.37it/s]\n",
      "100%|███████████████████████████████████████████| 17/17 [00:00<00:00, 70.67it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_disrupt.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_eliminate.tsv\n",
      "Processing Started...\n",
      "Data Size:  191\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 63%|██████████████████████████▍               | 17/27 [00:00<00:00, 168.23it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 56%|███████████████████████▎                  | 15/27 [00:00<00:00, 149.62it/s]\u001b[A\n",
      "\n",
      " 63%|██████████████████████████▍               | 17/27 [00:00<00:00, 168.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|██████████████████▋                       | 12/27 [00:00<00:00, 119.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 162.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|███████████████████████████████████▊      | 23/27 [00:00<00:00, 226.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 183.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|█████████████                               | 8/27 [00:00<00:00, 79.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 141.37it/s]\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 94.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 98.26it/s]\n",
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 99.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 106.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_eliminate.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_eliminate.tsv\n",
      "Processing Started...\n",
      "Data Size:  192\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 63%|██████████████████████████▍               | 17/27 [00:00<00:00, 167.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 168.00it/s]\u001b[A\n",
      "\n",
      "\n",
      " 56%|███████████████████████▎                  | 15/27 [00:00<00:00, 131.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 48%|████████████████████▏                     | 13/27 [00:00<00:00, 115.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|████████████████████▏                     | 13/27 [00:00<00:00, 123.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 121.28it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 136.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|████████████████████▏                     | 13/27 [00:00<00:00, 120.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 110.72it/s]\n",
      " 30%|█████████████                               | 8/27 [00:00<00:00, 79.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 126.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████████▍ | 26/27 [00:00<00:00, 123.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 132.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 27/27 [00:00<00:00, 120.73it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_eliminate.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_encode.tsv\n",
      "Processing Started...\n",
      "Data Size:  196\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 43%|██████████████████                        | 12/28 [00:00<00:00, 116.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 57%|████████████████████████                  | 16/28 [00:00<00:00, 139.93it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 29%|████████████▌                               | 8/28 [00:00<00:00, 66.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 138.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|██████████████▏                             | 9/28 [00:00<00:00, 87.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 124.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████                        | 12/28 [00:00<00:00, 115.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:00<00:00, 95.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 123.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 104.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████████   | 26/28 [00:00<00:00, 128.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 123.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 120.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 121.30it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_encode.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_encode.tsv\n",
      "Processing Started...\n",
      "Data Size:  197\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 46%|███████████████████▌                      | 13/28 [00:00<00:00, 125.55it/s]\n",
      " 50%|█████████████████████                     | 14/28 [00:00<00:00, 135.65it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 165.02it/s]\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 165.44it/s]\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 194.34it/s]\n",
      "\n",
      "\n",
      "\n",
      " 61%|█████████████████████████▍                | 17/28 [00:00<00:00, 155.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|████████████████████████                  | 16/28 [00:00<00:00, 157.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 39%|████████████████▉                          | 11/28 [00:00<00:00, 96.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 139.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 130.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 126.43it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 131.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_encode.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_express.tsv\n",
      "Processing Started...\n",
      "Data Size:  395\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██████████▌                               | 14/56 [00:00<00:00, 136.29it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 23%|█████████▊                                | 13/56 [00:00<00:00, 119.11it/s]\u001b[A\n",
      "\n",
      " 18%|███████▋                                   | 10/56 [00:00<00:00, 97.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 16%|███████                                     | 9/56 [00:00<00:00, 86.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████████████████████                     | 28/56 [00:00<00:00, 128.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|█████████                                 | 12/56 [00:00<00:00, 118.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 45%|██████████████████▊                       | 25/56 [00:00<00:00, 112.45it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|█████████▊                                | 13/56 [00:00<00:00, 124.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|█████████████▊                             | 18/56 [00:00<00:00, 86.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|██████████████████▊                       | 25/56 [00:00<00:00, 116.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████                        | 24/56 [00:00<00:00, 110.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 70%|█████████████████████████████▎            | 39/56 [00:00<00:00, 119.62it/s]\u001b[A\n",
      "\n",
      " 68%|████████████████████████████▌             | 38/56 [00:00<00:00, 123.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|███████████████████████▎                  | 31/56 [00:00<00:00, 102.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 96%|████████████████████████████████████████▌ | 54/56 [00:00<00:00, 120.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 64%|███████████████████████████               | 36/56 [00:00<00:00, 110.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 120.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 95%|███████████████████████████████████████▊  | 53/56 [00:00<00:00, 126.36it/s]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 125.75it/s]\n",
      " 96%|████████████████████████████████████████▌ | 54/56 [00:00<00:00, 133.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 122.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|██████████████████████████████            | 40/56 [00:00<00:00, 108.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 115.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 114.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 113.72it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 113.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_express.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_express.tsv\n",
      "Processing Started...\n",
      "Data Size:  397\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 36%|███████████████                           | 20/56 [00:00<00:00, 193.33it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 30%|████████████▋                             | 17/56 [00:00<00:00, 167.20it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 23%|█████████▊                                | 13/56 [00:00<00:00, 126.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|████████████                              | 16/56 [00:00<00:00, 159.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|████████████▋                             | 17/56 [00:00<00:00, 164.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|█████████▊                                | 13/56 [00:00<00:00, 120.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|████████▎                                 | 11/56 [00:00<00:00, 106.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 71%|██████████████████████████████            | 40/56 [00:00<00:00, 170.20it/s]\u001b[A\n",
      "\n",
      " 55%|███████████████████████▎                  | 31/56 [00:00<00:00, 154.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|████████████████████████▊                 | 33/56 [00:00<00:00, 165.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|█████████████████████████▍                | 34/56 [00:00<00:00, 159.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|██████████████████▊                       | 25/56 [00:00<00:00, 117.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|███████████████████▏                       | 25/56 [00:00<00:00, 91.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 148.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|██████████████████████████████████████▎   | 51/56 [00:00<00:00, 126.64it/s]\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████████▌    | 50/56 [00:00<00:00, 130.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████████▌    | 50/56 [00:00<00:00, 137.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 134.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 137.62it/s]\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 144.97it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 132.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 130.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 56/56 [00:00<00:00, 120.77it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_express.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_generate.tsv\n",
      "Processing Started...\n",
      "Data Size:  370\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/52 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████████                              | 15/52 [00:00<00:00, 140.39it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 19%|████████▎                                  | 10/52 [00:00<00:00, 97.61it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 27%|███████████▎                              | 14/52 [00:00<00:00, 125.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|████████████▉                             | 16/52 [00:00<00:00, 155.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██████████▌                               | 13/52 [00:00<00:00, 116.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█████▉                                      | 7/52 [00:00<00:00, 63.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|████████████████████████▏                 | 30/52 [00:00<00:00, 140.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 44%|██████████████████▌                       | 23/52 [00:00<00:00, 107.90it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 67%|████████████████████████████▎             | 35/52 [00:00<00:00, 172.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████████████████████▊                    | 27/52 [00:00<00:00, 116.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████▏                     | 25/52 [00:00<00:00, 109.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|███████████████████▍                      | 24/52 [00:00<00:00, 122.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|███████████████████████████▍              | 34/52 [00:00<00:00, 169.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 52/52 [00:00<00:00, 169.36it/s]\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 52/52 [00:00<00:00, 140.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|████████████████████████████████▎         | 40/52 [00:00<00:00, 123.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████████▏| 51/52 [00:00<00:00, 154.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 52/52 [00:00<00:00, 158.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 52/52 [00:00<00:00, 125.95it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████| 52/52 [00:00<00:00, 129.75it/s]\n",
      "100%|██████████████████████████████████████████| 52/52 [00:00<00:00, 109.03it/s]\n",
      "100%|██████████████████████████████████████████| 52/52 [00:00<00:00, 124.70it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_generate.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_generate.tsv\n",
      "Processing Started...\n",
      "Data Size:  371\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 19%|████████                                   | 10/53 [00:00<00:00, 87.03it/s]\n",
      " 21%|████████▉                                  | 11/53 [00:00<00:00, 99.36it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 13%|█████▊                                      | 7/53 [00:00<00:00, 68.92it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▉                                  | 11/53 [00:00<00:00, 88.49it/s]\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 38%|████████████████▏                          | 20/53 [00:00<00:00, 92.52it/s]\n",
      "\n",
      "\n",
      "\n",
      " 19%|████████                                   | 10/53 [00:00<00:00, 91.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 26%|███████████▎                               | 14/53 [00:00<00:00, 63.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|████▏                                       | 5/53 [00:00<00:01, 46.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 40%|█████████████████                          | 21/53 [00:00<00:00, 78.78it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|███████▍                                    | 9/53 [00:00<00:00, 77.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|████████████████████████▎                  | 30/53 [00:00<00:00, 93.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 42%|█████████████████▍                        | 22/53 [00:00<00:00, 102.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██████████▌                                | 13/53 [00:00<00:00, 64.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 43%|██████████████████▋                        | 23/53 [00:00<00:00, 68.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|██████████████▌                            | 18/53 [00:00<00:00, 80.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████████████████████████▊                | 33/53 [00:00<00:00, 96.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████████████████▍          | 40/53 [00:00<00:00, 86.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████████████████████▎                      | 25/53 [00:00<00:00, 85.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████████████████████▉                     | 27/53 [00:00<00:00, 84.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████████████████████████▊                | 33/53 [00:00<00:00, 73.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|██████████████████████████████████████    | 48/53 [00:00<00:00, 115.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 106.23it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 52/53 [00:00<00:00, 97.02it/s]\n",
      "\n",
      "\n",
      "\n",
      " 98%|█████████████████████████████████████████▏| 52/53 [00:00<00:00, 129.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 120.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 53/53 [00:00<00:00, 92.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████████▌      | 45/53 [00:00<00:00, 88.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 53/53 [00:00<00:00, 89.19it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 104.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 53/53 [00:00<00:00, 82.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 53/53 [00:00<00:00, 96.19it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_generate.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_inhibit.tsv\n",
      "Processing Started...\n",
      "Data Size:  339\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 33%|██████████████                            | 16/48 [00:00<00:00, 154.83it/s]/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 27%|███████████▍                              | 13/48 [00:00<00:00, 117.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|███▋                                        | 4/48 [00:00<00:01, 39.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 29%|████████████▎                             | 14/48 [00:00<00:00, 123.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████████████████████████████▌          | 36/48 [00:00<00:00, 179.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|█████████▊                                 | 11/48 [00:00<00:00, 87.45it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|████████▎                                   | 9/48 [00:00<00:00, 84.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 179.15it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|███████████▋                               | 13/48 [00:00<00:00, 68.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████████████████████████▍                | 29/48 [00:00<00:00, 132.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|███████████████████▎                      | 22/48 [00:00<00:00, 113.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▍                | 29/48 [00:00<00:00, 136.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|██████████████████▍                       | 21/48 [00:00<00:00, 103.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 133.24it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████████████████████▌                     | 24/48 [00:00<00:00, 85.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████████████████████████████████▋    | 43/48 [00:00<00:00, 120.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████████▌   | 44/48 [00:00<00:00, 139.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 133.16it/s]\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 121.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████████████████████████████▎           | 35/48 [00:00<00:00, 93.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 111.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 113.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 48/48 [00:00<00:00, 95.67it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_inhibit.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_inhibit.tsv\n",
      "Processing Started...\n",
      "Data Size:  341\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████████▎                             | 14/48 [00:00<00:00, 130.42it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 40%|████████████████▋                         | 19/48 [00:00<00:00, 174.00it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 31%|█████████████▏                            | 15/48 [00:00<00:00, 142.38it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      " 38%|███████████████▊                          | 18/48 [00:00<00:00, 164.01it/s]\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█████▌                                      | 6/48 [00:00<00:00, 51.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▉                                  | 10/48 [00:00<00:00, 59.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████████████████████████                  | 28/48 [00:00<00:00, 82.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██████████▊                                | 12/48 [00:00<00:00, 55.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 40%|█████████████████                          | 19/48 [00:00<00:00, 72.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████████████████████████▉                | 30/48 [00:00<00:00, 90.97it/s]\u001b[A\u001b[A\n",
      " 77%|████████████████████████████████▍         | 37/48 [00:00<00:00, 103.93it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██████████▊                                | 12/48 [00:00<00:00, 54.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████████▉        | 39/48 [00:00<00:00, 87.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 48/48 [00:00<00:00, 108.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████████▋      | 41/48 [00:00<00:00, 90.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|███████████████████▋                       | 22/48 [00:00<00:00, 71.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 48/48 [00:00<00:00, 99.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 48/48 [00:00<00:00, 97.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 48/48 [00:00<00:00, 98.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████████▉        | 39/48 [00:00<00:00, 83.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|████████████████████████████▋              | 32/48 [00:00<00:00, 78.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 48/48 [00:00<00:00, 82.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 48/48 [00:00<00:00, 84.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 48/48 [00:00<00:00, 82.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_inhibit.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_initiate.tsv\n",
      "Processing Started...\n",
      "Data Size:  197\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 50%|█████████████████████                     | 14/28 [00:00<00:00, 139.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 43%|██████████████████                        | 12/28 [00:00<00:00, 116.26it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 43%|██████████████████                        | 12/28 [00:00<00:00, 115.32it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████████████████████                     | 14/28 [00:00<00:00, 128.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 149.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|███████████████████▌                      | 13/28 [00:00<00:00, 126.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|█████████▍                                  | 6/28 [00:00<00:00, 59.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 131.30it/s]\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 112.34it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 93%|███████████████████████████████████████   | 26/28 [00:00<00:00, 104.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 105.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 96.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|███████████████████▉                       | 13/28 [00:00<00:00, 59.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 102.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 74.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_initiate.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_initiate.tsv\n",
      "Processing Started...\n",
      "Data Size:  197\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 39%|████████████████▌                         | 11/28 [00:00<00:00, 107.89it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 50%|█████████████████████                     | 14/28 [00:00<00:00, 129.09it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 39%|████████████████▉                          | 11/28 [00:00<00:00, 96.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▎                           | 10/28 [00:00<00:00, 92.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 25%|███████████                                 | 7/28 [00:00<00:00, 66.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████████         | 22/28 [00:00<00:00, 106.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|███████████                                 | 7/28 [00:00<00:00, 67.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 108.50it/s]\n",
      " 75%|████████████████████████████████▎          | 21/28 [00:00<00:00, 98.21it/s]\u001b[A\u001b[A\n",
      " 96%|████████████████████████████████████████▌ | 27/28 [00:00<00:00, 100.18it/s]\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 104.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|██████████████████████████                 | 17/28 [00:00<00:00, 78.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 28/28 [00:00<00:00, 102.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 93.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 89.07it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 82.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 28/28 [00:00<00:00, 85.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_initiate.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_lead.tsv\n",
      "Processing Started...\n",
      "Data Size:  492\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 21%|█████████                                 | 15/70 [00:00<00:00, 149.01it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 19%|███████▊                                  | 13/70 [00:00<00:00, 126.28it/s]\u001b[A\n",
      "\n",
      " 21%|█████████                                 | 15/70 [00:00<00:00, 149.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|██████▌                                   | 11/70 [00:00<00:00, 106.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|████▍                                       | 7/70 [00:00<00:00, 63.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|████████▍                                 | 14/70 [00:00<00:00, 132.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████                        | 30/70 [00:00<00:00, 133.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 37%|███████████████▉                           | 26/70 [00:00<00:00, 99.21it/s]\u001b[A\n",
      "\n",
      " 43%|██████████████████                        | 30/70 [00:00<00:00, 109.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|███████████▋                               | 19/70 [00:00<00:00, 90.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|█████████████▌                             | 22/70 [00:00<00:00, 86.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████                        | 30/70 [00:00<00:00, 128.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████████████████▊                         | 28/70 [00:00<00:00, 109.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 53%|██████████████████████▏                   | 37/70 [00:00<00:00, 102.28it/s]\u001b[A\n",
      "\n",
      " 60%|█████████████████████████▏                | 42/70 [00:00<00:00, 108.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|███████████████████████████                | 44/70 [00:00<00:00, 99.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|███████████████████████████▌              | 46/70 [00:00<00:00, 137.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|████████████████████████                  | 40/70 [00:00<00:00, 109.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 70%|█████████████████████████████▍            | 49/70 [00:00<00:00, 107.17it/s]\u001b[A\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████████         | 55/70 [00:00<00:00, 100.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 77%|█████████████████████████████████▏         | 54/70 [00:00<00:00, 99.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 63%|███████████████████████████                | 44/70 [00:00<00:00, 99.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████████████████████████████▏          | 52/70 [00:00<00:00, 110.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 86%|████████████████████████████████████      | 60/70 [00:00<00:00, 106.39it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████████      | 60/70 [00:00<00:00, 114.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 94%|███████████████████████████████████████▌  | 66/70 [00:00<00:00, 101.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 112.47it/s]\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 107.76it/s]\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 109.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 116.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 111.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 110.43it/s]\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 106.86it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_lead.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_lead.tsv\n",
      "Processing Started...\n",
      "Data Size:  493\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 20%|████████▍                                 | 14/70 [00:00<00:00, 139.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 13%|█████▋                                      | 9/70 [00:00<00:00, 88.09it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 24%|██████████▏                               | 17/70 [00:00<00:00, 161.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▍                                 | 14/70 [00:00<00:00, 125.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|█████████████████▍                        | 29/70 [00:00<00:00, 139.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 30%|████████████▌                             | 21/70 [00:00<00:00, 106.68it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▊                                    | 11/70 [00:00<00:00, 94.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▊                                    | 11/70 [00:00<00:00, 91.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|███████████████████▏                      | 32/70 [00:00<00:00, 159.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|████████████████▏                         | 27/70 [00:00<00:00, 106.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|█████████████████████████▊                | 43/70 [00:00<00:00, 139.80it/s]\u001b[A\u001b[A\n",
      " 49%|████████████████████▍                     | 34/70 [00:00<00:00, 112.14it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|█████████████▏                            | 22/70 [00:00<00:00, 101.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|█████████████▏                            | 22/70 [00:00<00:00, 100.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|██████████████████████▊                   | 38/70 [00:00<00:00, 105.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 69%|████████████████████████████▊             | 48/70 [00:00<00:00, 141.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 69%|████████████████████████████▊             | 48/70 [00:00<00:00, 118.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|██████████████████████▏                   | 37/70 [00:00<00:00, 119.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 81%|██████████████████████████████████▏       | 57/70 [00:00<00:00, 122.51it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|███████████████████▊                      | 33/70 [00:00<00:00, 101.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|███████████████████████████████▏          | 52/70 [00:00<00:00, 116.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████████████████████████████████▊    | 63/70 [00:00<00:00, 143.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████████████████████████████▏          | 52/70 [00:00<00:00, 129.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 81%|██████████████████████████████████▏       | 57/70 [00:00<00:00, 109.13it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|███████████████████████████                | 44/70 [00:00<00:00, 98.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 122.74it/s]\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 141.37it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 112.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|█████████████████████████████████         | 55/70 [00:00<00:00, 102.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 97%|████████████████████████████████████████▊ | 68/70 [00:00<00:00, 102.32it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 107.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 103.86it/s]\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 115.67it/s]\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 109.32it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_lead.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_lose.tsv\n",
      "Processing Started...\n",
      "Data Size:  286\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|██████████████▋                           | 14/40 [00:00<00:00, 126.86it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 38%|███████████████▊                          | 15/40 [00:00<00:00, 136.46it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 35%|██████████████▋                           | 14/40 [00:00<00:00, 125.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|████████████▌                             | 12/40 [00:00<00:00, 116.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▍           | 29/40 [00:00<00:00, 137.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|█████████████▋                            | 13/40 [00:00<00:00, 121.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|████████████▌                             | 12/40 [00:00<00:00, 116.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 72%|██████████████████████████████▍           | 29/40 [00:00<00:00, 133.37it/s]\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▍           | 29/40 [00:00<00:00, 144.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 131.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 141.27it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|█████████████████████████████▍            | 28/40 [00:00<00:00, 137.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 135.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 108.37it/s]\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 119.44it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 40/40 [00:00<00:00, 105.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_lose.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_lose.tsv\n",
      "Processing Started...\n",
      "Data Size:  287\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|██████████████▎                           | 14/41 [00:00<00:00, 133.48it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 32%|█████████████▎                            | 13/41 [00:00<00:00, 115.11it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 37%|███████████████▎                          | 15/41 [00:00<00:00, 138.57it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      " 24%|██████████▍                                | 10/41 [00:00<00:00, 97.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|███████████▎                              | 11/41 [00:00<00:00, 109.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|████████▌                                   | 8/41 [00:00<00:00, 75.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 68%|████████████████████████████▋             | 28/41 [00:00<00:00, 133.74it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|███████████████▎                          | 15/41 [00:00<00:00, 146.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 71%|█████████████████████████████▋            | 29/41 [00:00<00:00, 137.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▎             | 28/41 [00:00<00:00, 97.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 142.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|█████████████████▊                         | 17/41 [00:00<00:00, 83.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|█████████████████████████████████▊        | 33/41 [00:00<00:00, 164.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 107.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 116.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 151.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 128.39it/s]\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 112.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 104.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_lose.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_modify.tsv\n",
      "Processing Started...\n",
      "Data Size:  166\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 57%|███████████████████████▋                  | 13/23 [00:00<00:00, 124.93it/s]\n",
      " 65%|███████████████████████████▍              | 15/23 [00:00<00:00, 136.39it/s]\u001b[A\n",
      "\n",
      " 78%|████████████████████████████████▊         | 18/23 [00:00<00:00, 177.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████████████████████▉                    | 12/23 [00:00<00:00, 118.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 177.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|█████████████████████████████▏            | 16/23 [00:00<00:00, 155.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|█████████████████▏                          | 9/23 [00:00<00:00, 89.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 112.17it/s]\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 107.88it/s]\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 129.22it/s]\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 143.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|███████████▍                                | 6/23 [00:00<00:00, 55.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 93.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 97.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_modify.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_modify.tsv\n",
      "Processing Started...\n",
      "Data Size:  167\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████████████████████▉                    | 12/23 [00:00<00:00, 115.46it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 65%|███████████████████████████▍              | 15/23 [00:00<00:00, 148.64it/s]\u001b[A\n",
      "\n",
      " 70%|█████████████████████████████▏            | 16/23 [00:00<00:00, 156.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 157.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 136.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 111.95it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|████████████████████                      | 11/23 [00:00<00:00, 106.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 116.46it/s]\n",
      " 39%|█████████████████▏                          | 9/23 [00:00<00:00, 87.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 100.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 113.15it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 103.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_modify.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_mutate.tsv\n",
      "Processing Started...\n",
      "Data Size:  244\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▊                                    | 6/34 [00:00<00:00, 54.30it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 12%|█████▏                                      | 4/34 [00:00<00:00, 39.31it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/34 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██████████▎                                 | 8/34 [00:00<00:00, 79.75it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 35%|███████████████▏                           | 12/34 [00:00<00:00, 56.85it/s]/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 26%|███████████▋                                | 9/34 [00:00<00:00, 82.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▊                                    | 6/34 [00:00<00:00, 52.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▊                                    | 6/34 [00:00<00:00, 58.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 35%|███████████████▏                           | 12/34 [00:00<00:00, 57.95it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|█████████                                   | 7/34 [00:00<00:00, 67.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████████████████████▌                     | 17/34 [00:00<00:00, 85.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|██████████████████████▊                    | 18/34 [00:00<00:00, 85.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|███████████████████████████▊               | 22/34 [00:00<00:00, 68.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███████████████▏                           | 12/34 [00:00<00:00, 56.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 59%|█████████████████████████▎                 | 20/34 [00:00<00:00, 64.61it/s]\u001b[A\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████████▏        | 27/34 [00:00<00:00, 91.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 34/34 [00:00<00:00, 78.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████████▏        | 27/34 [00:00<00:00, 75.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 88%|█████████████████████████████████████▉     | 30/34 [00:00<00:00, 77.64it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 34/34 [00:00<00:00, 87.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 34/34 [00:00<00:00, 84.35it/s]\n",
      "100%|███████████████████████████████████████████| 34/34 [00:00<00:00, 69.33it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████████████████████████▌                | 21/34 [00:00<00:00, 60.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 74%|███████████████████████████████▌           | 25/34 [00:00<00:00, 58.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 34/34 [00:00<00:00, 71.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████████████████████████████████▋      | 29/34 [00:00<00:00, 65.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 34/34 [00:00<00:00, 62.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 34/34 [00:00<00:00, 67.04it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_mutate.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_mutate.tsv\n",
      "Processing Started...\n",
      "Data Size:  245\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 20%|████████▊                                   | 7/35 [00:00<00:00, 68.74it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 74%|███████████████████████████████▏          | 26/35 [00:00<00:00, 117.53it/s]\n",
      "\n",
      "\n",
      " 14%|██████▎                                     | 5/35 [00:00<00:00, 49.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 29%|████████████▎                              | 10/35 [00:00<00:00, 98.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 35/35 [00:00<00:00, 128.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|██████████                                  | 8/35 [00:00<00:00, 76.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 40%|█████████████████▏                         | 14/35 [00:00<00:00, 61.78it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|█████                                       | 4/35 [00:00<00:00, 31.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|████████████▎                              | 10/35 [00:00<00:00, 45.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 57%|████████████████████████▌                  | 20/35 [00:00<00:00, 93.24it/s]\u001b[A\u001b[A\n",
      " 60%|█████████████████████████▊                 | 21/35 [00:00<00:00, 60.54it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|██████████                                  | 8/35 [00:00<00:00, 33.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|████████████████████████▌                  | 20/35 [00:00<00:00, 72.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████████████████████████████████▊      | 30/35 [00:00<00:00, 93.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|███████████████████▋                       | 16/35 [00:00<00:00, 55.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|████████████████████▉                      | 17/35 [00:00<00:00, 53.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 35/35 [00:00<00:00, 91.24it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|██████████████▋                            | 12/35 [00:00<00:00, 34.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 28/35 [00:00<00:00, 64.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|███████████████████████████                | 22/35 [00:00<00:00, 55.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 35/35 [00:00<00:00, 69.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 35/35 [00:00<00:00, 72.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 91%|███████████████████████████████████████▎   | 32/35 [00:00<00:00, 69.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 35/35 [00:00<00:00, 62.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 35/35 [00:00<00:00, 66.85it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 35/35 [00:00<00:00, 59.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_mutate.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_proliferate.tsv\n",
      "Processing Started...\n",
      "Data Size:  162\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|█████████████████████████▌                | 14/23 [00:00<00:00, 139.99it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 65%|███████████████████████████▍              | 15/23 [00:00<00:00, 132.44it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 52%|█████████████████████▉                    | 12/23 [00:00<00:00, 114.38it/s]\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 140.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|███████████████████████▋                  | 13/23 [00:00<00:00, 124.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 123.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███████████████▎                            | 8/23 [00:00<00:00, 70.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 115.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 89.71it/s]\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 123.40it/s]\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 96.27it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 97.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_proliferate.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_proliferate.tsv\n",
      "Processing Started...\n",
      "Data Size:  163\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 26%|███████████▍                                | 6/23 [00:00<00:00, 59.36it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 48%|████████████████████▌                      | 11/23 [00:00<00:00, 94.45it/s]\n",
      "\n",
      " 48%|████████████████████▌                      | 11/23 [00:00<00:00, 95.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████████████████████                      | 11/23 [00:00<00:00, 106.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 132.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|█████████████▍                              | 7/23 [00:00<00:00, 63.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 157.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████████████████████████████████▌     | 20/23 [00:00<00:00, 102.15it/s]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 100.19it/s]\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 101.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 112.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 128.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 89.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_proliferate.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_recognize.tsv\n",
      "Processing Started...\n",
      "Data Size:  291\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|██████████████▎                           | 14/41 [00:00<00:00, 138.22it/s]/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 37%|███████████████▎                          | 15/41 [00:00<00:00, 140.56it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 29%|████████████▎                             | 12/41 [00:00<00:00, 119.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|████████████▎                             | 12/41 [00:00<00:00, 107.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|███████████▎                              | 11/41 [00:00<00:00, 107.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|███████████████▎                          | 15/41 [00:00<00:00, 143.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|████████████████████████████▋             | 28/41 [00:00<00:00, 129.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 73%|██████████████████████████████▋           | 30/41 [00:00<00:00, 143.06it/s]\u001b[A\n",
      "\n",
      " 59%|████████████████████████▌                 | 24/41 [00:00<00:00, 119.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|████████████████████████▌                 | 24/41 [00:00<00:00, 112.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 148.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 132.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 71%|█████████████████████████████▋            | 29/41 [00:00<00:00, 127.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 168.84it/s]\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 143.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 119.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 113.18it/s]\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 129.11it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_recognize.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_recognize.tsv\n",
      "Processing Started...\n",
      "Data Size:  293\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 24%|██████████▍                                | 10/41 [00:00<00:00, 86.89it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 27%|███████████▌                               | 11/41 [00:00<00:00, 92.20it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      " 20%|████████▌                                   | 8/41 [00:00<00:00, 76.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 27%|███████████▌                               | 11/41 [00:00<00:00, 84.63it/s]\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 51%|██████████████████████                     | 21/41 [00:00<00:00, 97.95it/s]\n",
      "\n",
      "\n",
      "\n",
      " 34%|██████████████▎                           | 14/41 [00:00<00:00, 138.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|█████████▋                                  | 9/41 [00:00<00:00, 88.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 66%|███████████████████████████▋              | 27/41 [00:00<00:00, 125.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|█████████████▎                            | 13/41 [00:00<00:00, 125.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|█████████████████▊                         | 17/41 [00:00<00:00, 78.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 61%|█████████████████████████▌                | 25/41 [00:00<00:00, 103.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 135.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 162.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 109.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|██████████████████████████▋               | 26/41 [00:00<00:00, 114.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|█████████████████████████████▎             | 28/41 [00:00<00:00, 90.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████████████████████████████████▉     | 36/41 [00:00<00:00, 102.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 102.28it/s]\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 123.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 41/41 [00:00<00:00, 119.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 41/41 [00:00<00:00, 88.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_recognize.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_result.tsv\n",
      "Processing Started...\n",
      "Data Size:  496\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 21%|█████████                                 | 15/70 [00:00<00:00, 141.40it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 21%|█████████                                 | 15/70 [00:00<00:00, 145.78it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 20%|████████▍                                 | 14/70 [00:00<00:00, 139.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|█████████▌                                | 16/70 [00:00<00:00, 147.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|██████▌                                   | 11/70 [00:00<00:00, 108.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████                        | 30/70 [00:00<00:00, 124.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▊                                    | 11/70 [00:00<00:00, 85.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 43%|██████████████████                        | 30/70 [00:00<00:00, 118.71it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|██████████████████▌                       | 31/70 [00:00<00:00, 132.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|█████████████▌                             | 22/70 [00:00<00:00, 97.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|████████████▌                             | 21/70 [00:00<00:00, 103.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 40%|█████████████████▏                         | 28/70 [00:00<00:00, 98.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|█████████████████████████▊                | 43/70 [00:00<00:00, 123.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 61%|█████████████████████████▊                | 43/70 [00:00<00:00, 113.60it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|███████████████████████████               | 45/70 [00:00<00:00, 130.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|██████████████████████▊                   | 38/70 [00:00<00:00, 130.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|█████████████████████▌                    | 36/70 [00:00<00:00, 110.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 59%|████████████████████████▌                 | 41/70 [00:00<00:00, 105.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|█████████████████████████████████▌        | 56/70 [00:00<00:00, 117.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 79%|█████████████████████████████████         | 55/70 [00:00<00:00, 111.86it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████████▏    | 62/70 [00:00<00:00, 141.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 76%|███████████████████████████████▊          | 53/70 [00:00<00:00, 137.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████████████████████████████▏          | 52/70 [00:00<00:00, 105.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 140.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████████▊ | 68/70 [00:00<00:00, 112.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 116.96it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 135.99it/s]\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 113.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████████      | 60/70 [00:00<00:00, 104.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 110.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 121.09it/s]\n",
      "100%|██████████████████████████████████████████| 70/70 [00:00<00:00, 107.37it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_result.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_result.tsv\n",
      "Processing Started...\n",
      "Data Size:  497\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|████████▊                                 | 15/71 [00:00<00:00, 141.04it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/71 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 18%|███████▋                                  | 13/71 [00:00<00:00, 129.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|██████▌                                   | 11/71 [00:00<00:00, 105.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|███████                                   | 12/71 [00:00<00:00, 115.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██████████                                | 17/71 [00:00<00:00, 168.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 31%|█████████████                             | 22/71 [00:00<00:00, 106.86it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|█████████▍                                | 16/71 [00:00<00:00, 140.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 37%|███████████████▍                          | 26/71 [00:00<00:00, 118.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|██████████████▊                           | 25/71 [00:00<00:00, 122.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|██████████████▊                           | 25/71 [00:00<00:00, 122.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 63%|██████████████████████████▌               | 45/71 [00:00<00:00, 129.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|████████████████████                      | 34/71 [00:00<00:00, 134.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 54%|██████████████████████▍                   | 38/71 [00:00<00:00, 106.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 61%|█████████████████████████▍                | 43/71 [00:00<00:00, 140.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|██████████████████████▍                   | 38/71 [00:00<00:00, 114.48it/s]\u001b[A\u001b[A\u001b[A\n",
      " 62%|██████████████████████████                | 44/71 [00:00<00:00, 103.82it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|███████████████████████████▏              | 46/71 [00:00<00:00, 139.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|██████████████████████████████████▉       | 59/71 [00:00<00:00, 116.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████████████████████████████▎          | 53/71 [00:00<00:00, 116.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▏           | 51/71 [00:00<00:00, 119.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 77%|████████████████████████████████▌         | 55/71 [00:00<00:00, 101.41it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████████████████      | 61/71 [00:00<00:00, 135.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 71/71 [00:00<00:00, 122.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 71/71 [00:00<00:00, 147.89it/s]\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 71/71 [00:00<00:00, 145.70it/s]\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 71/71 [00:00<00:00, 138.69it/s]\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████████████████████████████████▊    | 64/71 [00:00<00:00, 110.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 71/71 [00:00<00:00, 110.77it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████| 71/71 [00:00<00:00, 118.19it/s]\n",
      "100%|███████████████████████████████████████████| 71/71 [00:00<00:00, 99.11it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_result.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_skip.tsv\n",
      "Processing Started...\n",
      "Data Size:  79\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 119.12it/s]\n",
      "\n",
      " 64%|████████████████████████████                | 7/11 [00:00<00:00, 68.84it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 118.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 117.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 141.52it/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████| 11/11 [00:00<00:00, 65.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 11/11 [00:00<00:00, 97.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 11/11 [00:00<00:00, 93.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_skip.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_skip.tsv\n",
      "Processing Started...\n",
      "Data Size:  80\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 106.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 120.37it/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 111.92it/s]\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 122.78it/s]\n",
      "100%|██████████████████████████████████████████| 11/11 [00:00<00:00, 123.15it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 11/11 [00:00<00:00, 97.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 11/11 [00:00<00:00, 92.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_skip.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_splice.tsv\n",
      "Processing Started...\n",
      "Data Size:  390\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 29%|████████████▏                             | 16/55 [00:00<00:00, 154.21it/s]/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      " 33%|█████████████▋                            | 18/55 [00:00<00:00, 170.27it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 20%|████████▌                                  | 11/55 [00:00<00:00, 98.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▉                                | 13/55 [00:00<00:00, 128.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|████████▍                                 | 11/55 [00:00<00:00, 101.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▏                | 33/55 [00:00<00:00, 153.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|███████▏                                    | 9/55 [00:00<00:00, 88.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 38%|████████████████▍                          | 21/55 [00:00<00:00, 98.93it/s]\u001b[A\u001b[A\n",
      " 65%|███████████████████████████▍              | 36/55 [00:00<00:00, 154.43it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|███████████████████▊                      | 26/55 [00:00<00:00, 124.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████████████████▊                         | 22/55 [00:00<00:00, 103.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|████████████████▊                         | 22/55 [00:00<00:00, 110.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███████████████▋                           | 20/55 [00:00<00:00, 80.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████████████████████████▏                | 33/55 [00:00<00:00, 104.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 71%|█████████████████████████████▊            | 39/55 [00:00<00:00, 125.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|█████████████████████████████████████▍    | 49/55 [00:00<00:00, 122.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 139.38it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 126.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|██████████████████████▋                    | 29/55 [00:00<00:00, 83.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 80%|█████████████████████████████████▌        | 44/55 [00:00<00:00, 101.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 95%|███████████████████████████████████████▋  | 52/55 [00:00<00:00, 127.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 125.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 106.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 112.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 116.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 106.04it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_splice.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_splice.tsv\n",
      "Processing Started...\n",
      "Data Size:  390\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 13%|█████▌                                      | 7/55 [00:00<00:00, 57.37it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  9%|████                                        | 5/55 [00:00<00:01, 48.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|████▊                                       | 6/55 [00:00<00:00, 58.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 15%|██████▍                                     | 8/55 [00:00<00:00, 77.49it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 29%|████████████▌                              | 16/55 [00:00<00:00, 74.19it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|████                                        | 5/55 [00:00<00:01, 45.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 36%|███████████████▎                          | 20/55 [00:00<00:00, 102.05it/s]\u001b[A\n",
      "\n",
      " 24%|██████████▏                                | 13/55 [00:00<00:00, 64.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|████                                        | 5/55 [00:00<00:01, 49.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████████████████▊                         | 22/55 [00:00<00:00, 104.42it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 31%|█████████████▎                             | 17/55 [00:00<00:00, 83.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████████████████████▎                      | 26/55 [00:00<00:00, 84.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 56%|███████████████████████▋                  | 31/55 [00:00<00:00, 101.61it/s]\u001b[A\n",
      "\n",
      " 38%|████████████████▍                          | 21/55 [00:00<00:00, 68.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████████████████████████▏                | 33/55 [00:00<00:00, 102.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|████████████████████▎                      | 26/55 [00:00<00:00, 85.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|████████▌                                  | 11/55 [00:00<00:00, 50.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 76%|████████████████████████████████          | 42/55 [00:00<00:00, 102.97it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|█████████████████████████████▋             | 38/55 [00:00<00:00, 92.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 53%|██████████████████████▋                    | 29/55 [00:00<00:00, 70.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███████████████▋                           | 20/55 [00:00<00:00, 64.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|███████████████████████████▎               | 35/55 [00:00<00:00, 79.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 96%|████████████████████████████████████████▍ | 53/55 [00:00<00:00, 104.82it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 55/55 [00:00<00:00, 102.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 87%|█████████████████████████████████████▌     | 48/55 [00:00<00:00, 89.28it/s]\n",
      "\n",
      "\n",
      " 80%|██████████████████████████████████▍        | 44/55 [00:00<00:00, 79.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 67%|████████████████████████████▉              | 37/55 [00:00<00:00, 70.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|█████████████████████                      | 27/55 [00:00<00:00, 60.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 55/55 [00:00<00:00, 82.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|█████████████████████████████▋             | 38/55 [00:00<00:00, 67.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 98%|██████████████████████████████████████████▏| 54/55 [00:00<00:00, 81.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████| 55/55 [00:00<00:00, 84.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|████████████████████████████▉              | 37/55 [00:00<00:00, 70.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 55/55 [00:00<00:00, 73.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 55/55 [00:00<00:00, 73.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 55/55 [00:00<00:00, 72.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 55/55 [00:00<00:00, 77.08it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_splice.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_transcribe.tsv\n",
      "Processing Started...\n",
      "Data Size:  799\n",
      "number of threads:  7\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 14%|█████▊                                   | 16/114 [00:00<00:00, 153.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      " 17%|██████▊                                  | 19/114 [00:00<00:00, 185.64it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      " 11%|████▋                                    | 13/114 [00:00<00:00, 124.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|█████▍                                   | 15/114 [00:00<00:00, 140.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████████▌                             | 32/114 [00:00<00:00, 149.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▊                                   | 16/114 [00:00<00:00, 159.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|████▋                                    | 13/114 [00:00<00:00, 125.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 33%|█████████████▋                           | 38/114 [00:00<00:00, 161.12it/s]\u001b[A\n",
      "\n",
      "\n",
      " 26%|██████████▊                              | 30/114 [00:00<00:00, 137.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 30%|████████████▏                            | 34/114 [00:00<00:00, 168.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 23%|█████████▎                               | 26/114 [00:00<00:00, 102.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|█████████▋                               | 27/114 [00:00<00:00, 129.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|████████████████▉                        | 47/114 [00:00<00:00, 132.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 47%|███████████████████▍                     | 54/114 [00:00<00:00, 182.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 32%|█████████████▎                           | 37/114 [00:00<00:00, 105.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 48%|███████████████████▊                     | 55/114 [00:00<00:00, 143.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|██████████████▍                          | 40/114 [00:00<00:00, 129.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████████████████████▉                   | 61/114 [00:00<00:00, 134.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████████████████████████▌              | 74/114 [00:00<00:00, 187.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 48%|███████████████████▊                     | 55/114 [00:00<00:00, 130.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|███████████████████████▋                 | 66/114 [00:00<00:00, 160.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|████████████████████▊                    | 58/114 [00:00<00:00, 148.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|███████████████████████████▋             | 77/114 [00:00<00:00, 136.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 82%|█████████████████████████████████▊       | 94/114 [00:00<00:00, 190.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 63%|█████████████████████████▉               | 72/114 [00:00<00:00, 142.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|██████████████████████████████▏          | 84/114 [00:00<00:00, 165.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████████████████████████▉              | 75/114 [00:00<00:00, 155.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 188.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|████████████████████████████████         | 89/114 [00:00<00:00, 148.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|███████████████████████████████████▍    | 101/114 [00:00<00:00, 160.28it/s]\u001b[A\u001b[A\u001b[A\n",
      " 73%|█████████████████████████████▊           | 83/114 [00:00<00:00, 110.50it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|██████████████████████████████████▏      | 95/114 [00:00<00:00, 170.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 176.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 96%|██████████████████████████████████████▏ | 109/114 [00:00<00:00, 146.86it/s]\n",
      "\n",
      " 96%|██████████████████████████████████████▏ | 109/114 [00:00<00:00, 161.26it/s]\u001b[A\u001b[A\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 141.78it/s]\u001b[A\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 149.89it/s]\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 161.12it/s]\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 142.35it/s]\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 128.70it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_transcribe.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_transcribe.tsv\n",
      "Processing Started...\n",
      "Data Size:  801\n",
      "number of threads:  7\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/114 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 14%|█████▊                                   | 16/114 [00:00<00:00, 157.45it/s]\n",
      " 15%|██████                                   | 17/114 [00:00<00:00, 165.34it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▏                                 | 20/114 [00:00<00:00, 199.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 14%|█████▊                                   | 16/114 [00:00<00:00, 137.02it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|██████                                   | 17/114 [00:00<00:00, 168.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████████▊                             | 33/114 [00:00<00:00, 162.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 30%|████████████▏                            | 34/114 [00:00<00:00, 164.80it/s]\u001b[A\n",
      "\n",
      "\n",
      " 35%|██████████████▍                          | 40/114 [00:00<00:00, 198.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▍                                  | 18/114 [00:00<00:00, 173.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 32%|████████████▉                            | 36/114 [00:00<00:00, 168.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|█████████████▋                           | 38/114 [00:00<00:00, 190.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|███████████████████▊                     | 55/114 [00:00<00:00, 182.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 49%|████████████████████▏                    | 56/114 [00:00<00:00, 188.09it/s]\u001b[A\n",
      "\n",
      "\n",
      " 55%|██████████████████████▋                  | 63/114 [00:00<00:00, 207.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|██████████████                           | 39/114 [00:00<00:00, 190.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████████████████████▏                   | 59/114 [00:00<00:00, 193.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 57%|███████████████████████▍                 | 65/114 [00:00<00:00, 225.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████████████████████████▌              | 74/114 [00:00<00:00, 180.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 68%|████████████████████████████             | 78/114 [00:00<00:00, 198.77it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|███████████████████████▋                 | 66/114 [00:00<00:00, 224.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 72%|█████████████████████████████▍           | 82/114 [00:00<00:00, 204.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████████████████████████████▋         | 88/114 [00:00<00:00, 220.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|█████████████████████████████████▍       | 93/114 [00:00<00:00, 161.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|███████████████████████████▎             | 76/114 [00:00<00:00, 143.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 86%|███████████████████████████████████▏     | 98/114 [00:00<00:00, 160.32it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|████████████████████████████████         | 89/114 [00:00<00:00, 182.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████████████████████████████████▏   | 103/114 [00:00<00:00, 171.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 192.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 180.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 170.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 163.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 168.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 193.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████| 114/114 [00:00<00:00, 162.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_transcribe.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_transform.tsv\n",
      "Processing Started...\n",
      "Data Size:  375\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 53%|██████████████████████▏                   | 28/53 [00:00<00:00, 276.88it/s]\n",
      " 36%|███████████████                           | 19/53 [00:00<00:00, 188.93it/s]\u001b[A\n",
      "\n",
      " 49%|████████████████████▌                     | 26/53 [00:00<00:00, 258.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████████████████▋                         | 21/53 [00:00<00:00, 207.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|███████████████▊                          | 20/53 [00:00<00:00, 199.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████▏                       | 23/53 [00:00<00:00, 221.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 270.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 81%|██████████████████████████████████        | 43/53 [00:00<00:00, 215.92it/s]\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 251.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████████████████        | 43/53 [00:00<00:00, 209.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 211.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 212.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 236.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 213.21it/s]\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 217.15it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_transform.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_transform.tsv\n",
      "Processing Started...\n",
      "Data Size:  377\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/53 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 42%|█████████████████▍                        | 22/53 [00:00<00:00, 210.57it/s]\n",
      " 47%|███████████████████▊                      | 25/53 [00:00<00:00, 232.36it/s]\u001b[A\n",
      "\n",
      " 43%|██████████████████▏                       | 23/53 [00:00<00:00, 216.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|█████████████████▍                        | 22/53 [00:00<00:00, 211.47it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████▏                       | 23/53 [00:00<00:00, 221.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████▏                       | 23/53 [00:00<00:00, 229.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|██████████████████████████████████▊       | 44/53 [00:00<00:00, 209.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 91%|██████████████████████████████████████    | 48/53 [00:00<00:00, 235.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 229.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 204.84it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 224.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 196.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 207.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 229.68it/s]\n",
      "100%|██████████████████████████████████████████| 53/53 [00:00<00:00, 222.59it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_transform.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_translate.tsv\n",
      "Processing Started...\n",
      "Data Size:  678\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 22%|█████████▏                                | 21/96 [00:00<00:00, 206.89it/s]\n",
      " 19%|███████▉                                  | 18/96 [00:00<00:00, 177.18it/s]\u001b[A\n",
      "\n",
      " 22%|█████████▏                                | 21/96 [00:00<00:00, 201.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 22%|█████████▏                                | 21/96 [00:00<00:00, 205.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|███████▉                                  | 18/96 [00:00<00:00, 162.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▍                                  | 17/96 [00:00<00:00, 167.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|███████▍                                  | 17/96 [00:00<00:00, 169.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 38%|███████████████▊                          | 36/96 [00:00<00:00, 163.77it/s]\u001b[A\n",
      "\n",
      " 44%|██████████████████▍                       | 42/96 [00:00<00:00, 191.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███████████████▎                          | 35/96 [00:00<00:00, 164.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███████████████▊                          | 36/96 [00:00<00:00, 177.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 44%|██████████████████▍                       | 42/96 [00:00<00:00, 164.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███████████████▎                          | 35/96 [00:00<00:00, 170.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 64%|██████████████████████████▋               | 61/96 [00:00<00:00, 173.12it/s]\u001b[A\n",
      "\n",
      "\n",
      " 54%|██████████████████████▊                   | 52/96 [00:00<00:00, 165.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|████████████████████████▉                 | 57/96 [00:00<00:00, 189.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 65%|███████████████████████████▏              | 62/96 [00:00<00:00, 169.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████████████████████████▎               | 60/96 [00:00<00:00, 205.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|██████████████████████████▋               | 61/96 [00:00<00:00, 170.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 84%|███████████████████████████████████▍      | 81/96 [00:00<00:00, 180.84it/s]\u001b[A\n",
      "\n",
      "\n",
      " 72%|██████████████████████████████▏           | 69/96 [00:00<00:00, 158.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|█████████████████████████████████▋        | 77/96 [00:00<00:00, 190.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████████████████████████████████▊     | 84/96 [00:00<00:00, 184.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████████████████▍      | 81/96 [00:00<00:00, 203.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 187.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 177.16it/s]\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 176.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 190.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 179.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 168.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 167.43it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_translate.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_translate.tsv\n",
      "Processing Started...\n",
      "Data Size:  678\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/96 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 25%|██████████▌                               | 24/96 [00:00<00:00, 232.43it/s]\n",
      " 20%|████████▎                                 | 19/96 [00:00<00:00, 180.74it/s]\u001b[A\n",
      "\n",
      " 15%|██████▏                                   | 14/96 [00:00<00:00, 134.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|███████▉                                  | 18/96 [00:00<00:00, 174.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|███████▉                                  | 18/96 [00:00<00:00, 179.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|██████▌                                   | 15/96 [00:00<00:00, 146.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█████▋                                    | 13/96 [00:00<00:00, 119.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 41%|█████████████████                         | 39/96 [00:00<00:00, 191.19it/s]\u001b[A\n",
      "\n",
      " 50%|█████████████████████                     | 48/96 [00:00<00:00, 200.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███████████████▊                          | 36/96 [00:00<00:00, 169.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 38%|███████████████▊                          | 36/96 [00:00<00:00, 151.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███████████████▎                          | 35/96 [00:00<00:00, 175.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███████████████▎                          | 35/96 [00:00<00:00, 174.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 67%|████████████████████████████              | 64/96 [00:00<00:00, 217.65it/s]\u001b[A\n",
      "\n",
      " 72%|██████████████████████████████▏           | 69/96 [00:00<00:00, 199.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|██████████████████████████▋               | 61/96 [00:00<00:00, 203.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 56%|███████████████████████▋                  | 54/96 [00:00<00:00, 161.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 59%|████████████████████████▉                 | 57/96 [00:00<00:00, 193.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|███████████████████████▋                  | 54/96 [00:00<00:00, 179.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      " 90%|█████████████████████████████████████▋    | 86/96 [00:00<00:00, 218.46it/s]\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 206.44it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████████████████████████████▌          | 72/96 [00:00<00:00, 162.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      " 94%|███████████████████████████████████████▍  | 90/96 [00:00<00:00, 167.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 80%|█████████████████████████████████▋        | 77/96 [00:00<00:00, 179.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 173.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 174.66it/s]\n",
      "\n",
      "\n",
      " 92%|██████████████████████████████████████▌   | 88/96 [00:00<00:00, 147.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 93%|██████████████████████████████████████▉   | 89/96 [00:00<00:00, 154.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 172.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 156.16it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 148.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████████| 96/96 [00:00<00:00, 151.89it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_translate.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testa_truncate.tsv\n",
      "Processing Started...\n",
      "Data Size:  161\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 48%|████████████████████                      | 11/23 [00:00<00:00, 109.81it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      " 48%|████████████████████▌                      | 11/23 [00:00<00:00, 90.34it/s]\u001b[A\n",
      "\n",
      " 35%|███████████████▎                            | 8/23 [00:00<00:00, 78.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|█████████████████▏                          | 9/23 [00:00<00:00, 86.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 112.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|██████████████████▋                        | 10/23 [00:00<00:00, 93.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 109.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 119.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 108.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 93.12it/s]\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 119.12it/s]\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 112.13it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testa_truncate.json\n",
      "Loading raw data for task conllsrl from ../data/coNLL_tsv/ner_coNLL_testb_truncate.tsv\n",
      "Processing Started...\n",
      "Data Size:  161\n",
      "number of threads:  7\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\n",
      "\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 30%|█████████████▍                              | 7/23 [00:00<00:00, 69.31it/s]\n",
      " 30%|█████████████▍                              | 7/23 [00:00<00:00, 61.04it/s]\u001b[A\n",
      "\n",
      " 30%|█████████████▍                              | 7/23 [00:00<00:00, 66.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/23 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 70%|█████████████████████████████▉             | 16/23 [00:00<00:00, 77.44it/s]\n",
      "\n",
      " 87%|████████████████████████████████████▌     | 20/23 [00:00<00:00, 102.56it/s]\u001b[A\u001b[A\n",
      " 70%|█████████████████████████████▉             | 16/23 [00:00<00:00, 71.20it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████████████████████▉                    | 12/23 [00:00<00:00, 115.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████| 23/23 [00:00<00:00, 102.30it/s]\n",
      " 30%|█████████████▍                              | 7/23 [00:00<00:00, 66.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████████████████████▉                    | 12/23 [00:00<00:00, 117.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 77.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 73.34it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 94.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 93.04it/s]\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 93.96it/s]\n",
      "100%|███████████████████████████████████████████| 23/23 [00:00<00:00, 78.89it/s]\n",
      "Data Processing done for conllsrl. File saved at ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_testb_truncate.json\n"
     ]
    }
   ],
   "source": [
    "#!python ../data_preparation.py --task_file 'tasks_file_SRL.yml' --data_dir 'coNLL_data' --max_seq_len 50\n",
    "!python ../data_preparation.py --task_file tasks_file_SRL.yml --data_dir ../data/coNLL_tsv --max_seq_len 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../train.py \\\n",
    "    --data_dir 'content/data/bert-base-uncased_prepared_data' \\\n",
    "    --task_file 'tasks_file_SRL.yml' \\\n",
    "    --out_dir 'conll_ner_pos_bert_base' \\\n",
    "    --epochs 10 \\\n",
    "    --train_batch_size 32 \\\n",
    "    --eval_batch_size 32 \\\n",
    "    --grad_accumulation_steps 1 \\\n",
    "    --log_per_updates 50 \\\n",
    "    --max_seq_len 50 \\\n",
    "    --eval_while_train \\\n",
    "    --test_while_train \\\n",
    "    --silent\n",
    "    \n",
    "# python ../train.py --data_dir ../data/coNLL_tsv/bert-base-uncased_prepared_data --task_file tasks_file_SRL.yml --out_dir ../output/ --epochs 10 --train_batch_size 32 --eval_batch_size 32 --grad_accumulation_steps 32 --log_per_updates 50 --max_seq_len 50 --eval_while_train --test_while_train --silent\n",
    "\n",
    "# python ./train.py --data_dir ./data/coNLL_tsv/bert-base-uncased_prepared_data --task_file tasks_file_SRL.yml --out_dir ../output/ --epochs 10 --train_batch_size 32 --eval_batch_size 32 --grad_accumulation_steps 32 --log_per_updates 50 --max_seq_len 50 --eval_while_train --test_while_train --silent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../train.py --data_dir ../data/coNLL_tsv/dmis-lab/biobert-base-cased-v1.2_prepared_data --task_file tasks_file_SRL.yml --out_dir ../output/ --epochs 10 --train_batch_size 32 --eval_batch_size 32 --grad_accumulation_steps 32 --log_per_updates 50 --max_seq_len 50 --eval_while_train --test_while_train --silent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "from infer_pipeline import inferPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pipe = inferPipeline(modelPath='../output/multi_task_model_9_204.pt', maxSeqLen=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to get word representations from biobert model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'embedding.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# run embedding from original biobert (cha duoi terminal)\n",
    "!python embedding.py --data_dir ./data/coNLL_tsv/bert-base-uncased_prepared_data --transform_file word_represent/embedding.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run to get predictions from biobert model to calculate the f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
